{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H8DYFXNlQsAr"
      ],
      "authorship_tag": "ABX9TyORPkouumtBsp4FpzSNRn7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/HBFS_Hybrid_Best_First_Search_with_AlphaZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOvTHRa5oAC_",
        "outputId": "d0db8f2c-820b-4cc9-bd85-4c470aa67ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Netwrok"
      ],
      "metadata": {
        "id": "8WdEqjlyALbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SharedNetwork(nn.Module):\n",
        "    def __init__(self, input_size=16, hidden_size=128, num_actions=4):\n",
        "        super(SharedNetwork, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # 共享层\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size)\n",
        "        )\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, num_actions)\n",
        "        )\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            state: 游戏状态的one-hot编码 (batch_size, 16)\n",
        "        返回:\n",
        "            policy_logits: 动作概率的对数 (batch_size, 4)\n",
        "            value: 状态价值估计 (batch_size, 1)\n",
        "        \"\"\"\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "        # one-hot code\n",
        "        if len(state.shape) == 1:\n",
        "            state = F.one_hot(torch.tensor(state.argmax()), num_classes=16).float()\n",
        "\n",
        "        shared_features = self.shared_layers(state)\n",
        "\n",
        "        policy_logits = self.policy_head(shared_features)\n",
        "        value = self.value_head(shared_features)\n",
        "\n",
        "        return policy_logits, value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        short cut for BFS\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            _, value = self.forward(state)\n",
        "            return value.item()\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load(self, filepath):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=self.device))\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "\n",
        "# 测试代码\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建网络实例\n",
        "    net = SharedNetwork()\n",
        "\n",
        "    # 测试单个状态\n",
        "    state = torch.zeros(16)\n",
        "    state[0] = 1  # 假设在起始位置\n",
        "    policy, value = net(state)\n",
        "\n",
        "    print(\"Policy logits:\", policy)\n",
        "    print(\"Value:\", value)\n",
        "\n",
        "    # 测试批量状态\n",
        "    batch_states = torch.zeros(4, 16)  # 4个状态的batch\n",
        "    batch_states[0][0] = 1\n",
        "    batch_states[1][1] = 1\n",
        "    batch_states[2][2] = 1\n",
        "    batch_states[3][3] = 1\n",
        "\n",
        "    batch_policy, batch_value = net(batch_states)\n",
        "    print(\"\\nBatch policy:\", batch_policy.shape)\n",
        "    print(\"Batch value:\", batch_value.shape)"
      ],
      "metadata": {
        "id": "QylYEVtDuQnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7581784-aaa5-4155-a60e-22b2283d9cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy logits: tensor([1.4125, 0.2342, 0.5599, 0.1235], grad_fn=<ViewBackward0>)\n",
            "Value: tensor([0.6734], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Batch policy: torch.Size([4, 4])\n",
            "Batch value: torch.Size([4, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-6c8ded87f700>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = F.one_hot(torch.tensor(state.argmax()), num_classes=16).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.MCTS"
      ],
      "metadata": {
        "id": "AYZN0-oT_7Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "  prior: float      # P(s,a)\n",
        "  action_taken: Optional[int]\n",
        "  state: Optional[int]\n",
        "  visit_count: int = 0\n",
        "  value_sum: float = 0\n",
        "  parent: Optional['Node'] = None\n",
        "  children: Dict[int, 'Node'] = field(default_factory=dict)\n",
        "  done: bool = False"
      ],
      "metadata": {
        "id": "zhZcAdUp00Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import gymnasium as gym\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    prior: float  # P(s,a)\n",
        "    action_taken: Optional[int]  # 到达此节点采取的动作\n",
        "    visit_count: int = 0  # N(s,a)\n",
        "    value_sum: float = 0  # Q=value_sum/visit_count\n",
        "    parent: Optional['Node'] = None\n",
        "    children: Dict[int, 'Node'] = field(default_factory=dict)\n",
        "    done = False\n",
        "    has_children = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.children is None:\n",
        "            self.children = {}\n",
        "\n",
        "    @property\n",
        "    def Q_value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, num_simulations: int = 100, epoch_of_training=0):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "        # UCB参数，随着训练进行逐渐减小\n",
        "        self.c_puct = max(1.0 * (1 - epoch_of_training/100), 0.1)\n",
        "        # Dirichlet噪声参数，随训练进行逐渐减小\n",
        "        self.epsilon = max(0.25 * (1 - epoch_of_training/100), 0.01)\n",
        "        self.env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "    def predict_next_state(self, current_state, action):\n",
        "\n",
        "      # 克隆环境，以避免影响原始环境\n",
        "      env_copy = gym.make('FrozenLake-v1', render_mode=None)\n",
        "      env_copy.reset(seed=42)\n",
        "\n",
        "      # 设置环境状态\n",
        "      env_copy.unwrapped.s = current_state\n",
        "\n",
        "      # 执行动作并获取下一个状态\n",
        "      next_state, reward, terminated, truncated, info = env_copy.step(action)\n",
        "\n",
        "      return next_state\n",
        "    def search(self, root_state) -> np.ndarray:\n",
        "        \"\"\"执行MCTS搜索\"\"\"\n",
        "        root = Node(prior=1.0, action_taken=None)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            state = root_state\n",
        "            search_path = [node]\n",
        "            done = False\n",
        "\n",
        "            # 1. Selection\n",
        "            while node.has_children and not done:\n",
        "              action, node = self.select_child(node)\n",
        "              # 执行动作\n",
        "              self.env.reset()\n",
        "              self.env.unwrapped.s = state\n",
        "              state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "              done = terminated or truncated\n",
        "              node.done = done\n",
        "              search_path.append(node)\n",
        "\n",
        "\n",
        "\n",
        "            # 2. Expansion and Evaluation\n",
        "            value = 0\n",
        "            if not done:\n",
        "                policy, value = self.evaluate_state(state)\n",
        "                self.expand_node(node, policy)\n",
        "\n",
        "            # 3. Backup\n",
        "            self.backup(search_path, value)\n",
        "\n",
        "        # 返回根节点的动作概率分布\n",
        "        return root\n",
        "\n",
        "    def select_child(self, node: Node) -> Tuple[int, Node]:\n",
        "        \"\"\"使用UCB公式选择最佳子节点\"\"\"\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        sqrt_total_count = math.sqrt(node.visit_count)\n",
        "\n",
        "        for action, child in node.children.items():\n",
        "            # UCB公式\n",
        "            Q = child.Q_value\n",
        "            U = self.c_puct * child.prior * sqrt_total_count / (1 + child.visit_count)\n",
        "            ucb_score = Q + U\n",
        "\n",
        "            if ucb_score > best_score:\n",
        "                best_score = ucb_score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def evaluate_state(self, state) -> Tuple[np.ndarray, float]:\n",
        "        \"\"\"使用神经网络评估状态\"\"\"\n",
        "        # 转换状态为one-hot编码\n",
        "        state_onehot = np.zeros(16)\n",
        "        state_onehot[state] = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy, value = self.model(state_onehot)\n",
        "            policy = F.softmax(policy, dim=0)\n",
        "            #return policy.cpu().numpy(), value.item()\n",
        "        #+++++++++++++++++#\n",
        "        new_policy = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for a in range(4):\n",
        "            state= self.predict_next_state(state,a)\n",
        "            state_onehot = np.zeros(16)\n",
        "            state_onehot[state] = 1\n",
        "            _, value_p = self.model(state_onehot)\n",
        "            new_policy.append(value_p.item())\n",
        "\n",
        "        return np.array(new_policy)/np.sum(new_policy), value.item()\n",
        "    def expand_node(self, node: Node, policy: np.ndarray):\n",
        "        \"\"\"扩展节点，添加所有可能的子节点\"\"\"\n",
        "        # 添加Dirichlet噪声\n",
        "        noise = np.random.dirichlet([0.3] * len(policy))\n",
        "        policy = (1 - self.epsilon) * policy + self.epsilon * noise\n",
        "\n",
        "\n",
        "        # 创建子节点\n",
        "        for action, prob in enumerate(policy):\n",
        "            child = Node(\n",
        "                prior=prob,\n",
        "                action_taken=action,\n",
        "                parent=node\n",
        "            )\n",
        "            node.children[action] = child\n",
        "\n",
        "        node.has_children = True\n",
        "\n",
        "    def backup(self, search_path: List[Node], value: float):\n",
        "        \"\"\"反向传播更新节点统计信息\"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.visit_count += 1\n",
        "            node.value_sum += value\n",
        "\n",
        "    def get_action_probs(self, root: Node, temperature=0.1) -> np.ndarray:\n",
        "        \"\"\"获取动作概率分布\"\"\"\n",
        "        counts = np.array([child.visit_count for child in root.children.values()])\n",
        "        probs = counts / np.sum(counts)\n",
        "\n",
        "        return probs"
      ],
      "metadata": {
        "id": "qTdYK3jI_orM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_play_mcts(replay_buffer, model, epoch_of_training=0):\n",
        "   env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "   model.eval()\n",
        "\n",
        "   observation,_ = env.reset()\n",
        "   done = False\n",
        "   episode_reward = 0\n",
        "\n",
        "   trajectory = []\n",
        "   while not done:\n",
        "       print('observation',observation)\n",
        "       mcts = MCTS(model)\n",
        "       root = mcts.search(observation)\n",
        "       action_probs = mcts.get_action_probs(root, temperature=1)\n",
        "       action = np.argmax(action_probs)\n",
        "       print('action',action)\n",
        "       #print('action_probs',action_probs)\n",
        "       next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "       print('next_observation',next_observation)\n",
        "\n",
        "       episode_reward += reward\n",
        "       done = terminated or truncated\n",
        "\n",
        "       trajectory.append({\n",
        "           'state': observation,\n",
        "           'action': action,\n",
        "           'action_probs': action_probs\n",
        "       })\n",
        "       observation = next_observation\n",
        "\n",
        "   # Store episode with decaying rewards\n",
        "   for idx, t in enumerate(trajectory):\n",
        "       decay_reward = episode_reward * (0.95 ** (len(trajectory) - idx - 1))\n",
        "       replay_buffer.store(\n",
        "           t['state'],\n",
        "           t['action'],\n",
        "           t['action_probs'],\n",
        "           decay_reward\n",
        "       )\n",
        "\n",
        "   print('Episode reward:', episode_reward)\n",
        "   return episode_reward"
      ],
      "metadata": {
        "id": "tgvyc8j4fGHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 初始化组件\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SharedNetwork().to(device)\n",
        "replay_buffer = ReplayBuffer(batch_size=32, minimum_size=100)\n",
        "reward = self_play_mcts(replay_buffer, model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYUtyPAOek2s",
        "outputId": "f3d653b9-e85a-4de2-9ead-62b29cc4da4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-6c8ded87f700>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = F.one_hot(torch.tensor(state.argmax()), num_classes=16).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action 2\n",
            "next_observation 1\n",
            "observation 1\n",
            "action 1\n",
            "next_observation 5\n",
            "Episode reward: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1 test test\n"
      ],
      "metadata": {
        "id": "CRldFC-CCaCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "def test_mcts():\n",
        "    # 1. 初始化环境和网络\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    shared_net = SharedNetwork()\n",
        "    mcts = MCTS(shared_net, num_simulations=100)\n",
        "\n",
        "    # 2. 运行一个完整的episode\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    path = []\n",
        "\n",
        "    print(\"Starting MCTS test...\")\n",
        "    print(f\"Initial state: {state}\")\n",
        "\n",
        "    while not done:\n",
        "        # 将状态转换为one-hot编码\n",
        "        state_onehot = np.zeros(16)\n",
        "        state_onehot[state] = 1\n",
        "\n",
        "        # 执行MCTS搜索\n",
        "        root = mcts.search(state)\n",
        "        action_probs = mcts.get_action_probs(root, temperature=0.1)\n",
        "\n",
        "        # 选择动作\n",
        "        action = np.argmax(action_probs)\n",
        "\n",
        "        # 打印当前状态和选择的动作\n",
        "        action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
        "        print(f\"\\nCurrent state: {state}\")\n",
        "        print(f\"Chosen action: {action_names[action]}\")\n",
        "        print(f\"Action probabilities: {action_probs}\")\n",
        "\n",
        "        # 查看根节点的统计信息\n",
        "        print(\"\\nRoot node statistics:\")\n",
        "        for a, child in root.children.items():\n",
        "            print(f\"Action {action_names[a]}: Visits = {child.visit_count}, \"\n",
        "                  f\"Q-value = {child.Q_value:.3f}\")\n",
        "\n",
        "        # 执行动作\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "        # 记录路径\n",
        "        path.append((state, action))\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"\\nEpisode finished!\")\n",
        "            print(f\"Final state: {state}\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            if reward == 1:\n",
        "                print(\"Successfully reached the goal!\")\n",
        "            else:\n",
        "                print(\"Failed - either fell in a hole or exceeded steps\")\n",
        "\n",
        "    # 打印完整路径\n",
        "    print(\"\\nComplete path:\")\n",
        "    for step, (s, a) in enumerate(path):\n",
        "        print(f\"Step {step}: State {s} -> Action {action_names[a]}\")\n",
        "\n",
        "    return total_reward, path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 设置随机种子以保证可重复性\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # 运行测试\n",
        "    reward, path = test_mcts()\n",
        "\n",
        "    # 可以多次运行测试来查看稳定性\n",
        "    print(\"\\nRunning multiple episodes to check stability...\")\n",
        "    rewards = []\n",
        "    for i in range(5):\n",
        "        reward, _ = test_mcts()\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(f\"\\nResults over 5 episodes:\")\n",
        "    print(f\"Average reward: {np.mean(rewards):.2f}\")\n",
        "    print(f\"Rewards: {rewards}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KwKtQXIICiY6",
        "outputId": "949037de-4941-4f9e-d7a6-737df34be4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.41369954e-03 9.95171090e-01 1.51123394e-06 2.41369954e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 23, Q-value = 0.264\n",
            "Action DOWN: Visits = 42, Q-value = 0.157\n",
            "Action RIGHT: Visits = 11, Q-value = 0.203\n",
            "Action UP: Visits = 23, Q-value = 0.265\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.11014813e-02 1.08412903e-05 5.31066349e-09 9.88887672e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.220\n",
            "Action DOWN: Visits = 15, Q-value = 0.084\n",
            "Action RIGHT: Visits = 7, Q-value = 0.000\n",
            "Action UP: Visits = 47, Q-value = 0.234\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.58328683e-01 2.41534761e-01 9.09395568e-05 4.56163571e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.257\n",
            "Action DOWN: Visits = 33, Q-value = 0.184\n",
            "Action RIGHT: Visits = 15, Q-value = 0.243\n",
            "Action UP: Visits = 14, Q-value = 0.235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f48b08736cb>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = F.one_hot(torch.tensor(state.argmax()), num_classes=16).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.84405121e-06 1.33625446e-10 4.96030844e-13 9.99995156e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.149\n",
            "Action DOWN: Visits = 7, Q-value = 0.065\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 68, Q-value = 0.214\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [3.24747216e-06 9.99286984e-01 6.25660478e-04 8.41079339e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 13, Q-value = 0.176\n",
            "Action DOWN: Visits = 46, Q-value = 0.164\n",
            "Action RIGHT: Visits = 22, Q-value = 0.229\n",
            "Action UP: Visits = 18, Q-value = 0.226\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.12189999e-04 5.43817925e-07 4.13637504e-12 9.99187266e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.126\n",
            "Action DOWN: Visits = 13, Q-value = 0.106\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 55, Q-value = 0.231\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.42932902e-03 9.59090179e-01 9.82108343e-08 3.84803939e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.265\n",
            "Action DOWN: Visits = 40, Q-value = 0.207\n",
            "Action RIGHT: Visits = 8, Q-value = 0.160\n",
            "Action UP: Visits = 29, Q-value = 0.192\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.43547412e-05 1.40183019e-08 1.50520371e-09 9.99985630e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.152\n",
            "Action DOWN: Visits = 10, Q-value = -0.016\n",
            "Action RIGHT: Visits = 8, Q-value = 0.000\n",
            "Action UP: Visits = 61, Q-value = 0.232\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.25610106e-01 5.73669425e-01 5.60224047e-04 1.60245235e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.239\n",
            "Action DOWN: Visits = 34, Q-value = 0.182\n",
            "Action RIGHT: Visits = 17, Q-value = 0.195\n",
            "Action UP: Visits = 15, Q-value = 0.231\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.84433905e-07 3.67682530e-07 6.68590680e-13 9.99999448e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 14, Q-value = 0.127\n",
            "Action DOWN: Visits = 15, Q-value = 0.089\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 66, Q-value = 0.248\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.34360887e-04 3.56861478e-01 2.56188776e-04 6.42747972e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.231\n",
            "Action DOWN: Visits = 33, Q-value = 0.173\n",
            "Action RIGHT: Visits = 16, Q-value = 0.209\n",
            "Action UP: Visits = 35, Q-value = 0.220\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [3.31512760e-03 9.55706505e-01 5.74893844e-05 4.09208780e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.239\n",
            "Action DOWN: Visits = 37, Q-value = 0.207\n",
            "Action RIGHT: Visits = 14, Q-value = 0.178\n",
            "Action UP: Visits = 27, Q-value = 0.241\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.00341576e-05 2.10073368e-09 1.21138890e-07 9.99979843e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.161\n",
            "Action DOWN: Visits = 8, Q-value = -0.020\n",
            "Action RIGHT: Visits = 12, Q-value = 0.000\n",
            "Action UP: Visits = 59, Q-value = 0.228\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [6.83300192e-05 9.64595036e-01 3.50977492e-02 2.38884607e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.269\n",
            "Action DOWN: Visits = 39, Q-value = 0.189\n",
            "Action RIGHT: Visits = 28, Q-value = 0.233\n",
            "Action UP: Visits = 17, Q-value = 0.250\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.07702787e-05 4.49123287e-11 7.25359183e-12 9.99969230e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 23, Q-value = 0.147\n",
            "Action DOWN: Visits = 6, Q-value = 0.046\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 65, Q-value = 0.214\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.08910755e-04 9.99572039e-01 1.69278403e-05 2.12243915e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.205\n",
            "Action DOWN: Visits = 48, Q-value = 0.177\n",
            "Action RIGHT: Visits = 16, Q-value = 0.211\n",
            "Action UP: Visits = 13, Q-value = 0.216\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.76025364e-03 3.43379002e-12 3.19796616e-11 9.93239746e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 34, Q-value = 0.195\n",
            "Action DOWN: Visits = 4, Q-value = -0.041\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 56, Q-value = 0.226\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.02426604e-04 1.45585164e-01 8.54117110e-01 1.95298995e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.243\n",
            "Action DOWN: Visits = 31, Q-value = 0.161\n",
            "Action RIGHT: Visits = 37, Q-value = 0.218\n",
            "Action UP: Visits = 16, Q-value = 0.241\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.70441707e-02 1.74532308e-09 1.03059582e-04 9.82852768e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.248\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 18, Q-value = 0.171\n",
            "Action UP: Visits = 45, Q-value = 0.199\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.46059503e-01 5.86054807e-06 1.73587539e-01 4.80347098e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.228\n",
            "Action DOWN: Visits = 10, Q-value = 0.000\n",
            "Action RIGHT: Visits = 28, Q-value = 0.214\n",
            "Action UP: Visits = 31, Q-value = 0.187\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [3.52633282e-02 2.49798496e-06 9.13302879e-01 5.14312951e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.233\n",
            "Action DOWN: Visits = 10, Q-value = 0.000\n",
            "Action RIGHT: Visits = 36, Q-value = 0.210\n",
            "Action UP: Visits = 27, Q-value = 0.198\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.97513634e-01 1.49901163e-10 4.23433683e-02 6.01429973e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 38, Q-value = 0.231\n",
            "Action DOWN: Visits = 4, Q-value = -0.099\n",
            "Action RIGHT: Visits = 28, Q-value = 0.180\n",
            "Action UP: Visits = 29, Q-value = 0.166\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.22134139e-02 1.58782971e-07 9.37597567e-03 9.28410452e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.230\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 24, Q-value = 0.208\n",
            "Action UP: Visits = 38, Q-value = 0.195\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.32438777e-03 3.66603127e-07 6.11489652e-04 9.97063756e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = 0.227\n",
            "Action DOWN: Visits = 10, Q-value = 0.000\n",
            "Action RIGHT: Visits = 21, Q-value = 0.197\n",
            "Action UP: Visits = 44, Q-value = 0.219\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [3.83511571e-02 5.51202268e-09 9.55869063e-01 5.77977470e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.247\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 40, Q-value = 0.195\n",
            "Action UP: Visits = 24, Q-value = 0.206\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.44346014e-03 3.56530271e-08 2.95768332e-06 9.98553547e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.205\n",
            "Action DOWN: Visits = 9, Q-value = 0.078\n",
            "Action RIGHT: Visits = 14, Q-value = 0.180\n",
            "Action UP: Visits = 50, Q-value = 0.205\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.16101386e-03 9.28327598e-10 5.48168163e-05 9.96784168e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.220\n",
            "Action DOWN: Visits = 6, Q-value = -0.066\n",
            "Action RIGHT: Visits = 18, Q-value = 0.180\n",
            "Action UP: Visits = 48, Q-value = 0.219\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.83130140e-01 1.68546717e-12 1.76734043e-06 6.16868093e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 41, Q-value = 0.204\n",
            "Action DOWN: Visits = 3, Q-value = 0.041\n",
            "Action RIGHT: Visits = 12, Q-value = 0.182\n",
            "Action UP: Visits = 43, Q-value = 0.218\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.54025330e-03 1.56963880e-14 2.55677621e-04 9.95204069e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.229\n",
            "Action DOWN: Visits = 2, Q-value = -0.197\n",
            "Action RIGHT: Visits = 21, Q-value = 0.192\n",
            "Action UP: Visits = 48, Q-value = 0.215\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.09665366e-01 8.88111595e-11 8.67296480e-04 6.89467338e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 36, Q-value = 0.207\n",
            "Action DOWN: Visits = 4, Q-value = 0.031\n",
            "Action RIGHT: Visits = 20, Q-value = 0.224\n",
            "Action UP: Visits = 39, Q-value = 0.212\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.39177331e-02 1.20073292e-10 3.39177331e-02 9.32164534e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.189\n",
            "Action DOWN: Visits = 4, Q-value = 0.031\n",
            "Action RIGHT: Visits = 28, Q-value = 0.192\n",
            "Action UP: Visits = 39, Q-value = 0.207\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.79807816e-02 5.14106640e-06 1.89028134e-02 9.53111264e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.211\n",
            "Action DOWN: Visits = 11, Q-value = 0.014\n",
            "Action RIGHT: Visits = 25, Q-value = 0.225\n",
            "Action UP: Visits = 37, Q-value = 0.211\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.95508026e-02 1.69286421e-13 1.35499941e-01 7.94949257e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.217\n",
            "Action DOWN: Visits = 2, Q-value = -0.197\n",
            "Action RIGHT: Visits = 31, Q-value = 0.178\n",
            "Action UP: Visits = 37, Q-value = 0.196\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.88576501e-03 4.77252772e-11 1.34812095e-02 9.83633025e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = 0.205\n",
            "Action DOWN: Visits = 4, Q-value = 0.031\n",
            "Action RIGHT: Visits = 28, Q-value = 0.191\n",
            "Action UP: Visits = 43, Q-value = 0.200\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.05206190e-01 2.62217232e-10 6.89829495e-01 2.04964314e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.228\n",
            "Action DOWN: Visits = 4, Q-value = -0.099\n",
            "Action RIGHT: Visits = 35, Q-value = 0.222\n",
            "Action UP: Visits = 31, Q-value = 0.209\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.97365619e-01 6.64135166e-09 2.24220937e-03 3.92165174e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = 0.215\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 25, Q-value = 0.220\n",
            "Action UP: Visits = 21, Q-value = 0.190\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.47152766e-04 2.43363771e-12 3.19955468e-07 9.99852527e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = 0.196\n",
            "Action DOWN: Visits = 4, Q-value = -0.099\n",
            "Action RIGHT: Visits = 13, Q-value = 0.191\n",
            "Action UP: Visits = 58, Q-value = 0.205\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.05206190e-01 2.62217232e-10 6.89829495e-01 2.04964314e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.222\n",
            "Action DOWN: Visits = 4, Q-value = -0.099\n",
            "Action RIGHT: Visits = 35, Q-value = 0.196\n",
            "Action UP: Visits = 31, Q-value = 0.199\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [5.57221990e-01 2.00079008e-10 4.42778004e-01 5.78737844e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.189\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 43, Q-value = 0.196\n",
            "Action UP: Visits = 7, Q-value = 0.171\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [9.20932099e-05 2.69743473e-12 7.80245551e-01 2.19662356e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = 0.175\n",
            "Action DOWN: Visits = 3, Q-value = 0.041\n",
            "Action RIGHT: Visits = 42, Q-value = 0.211\n",
            "Action UP: Visits = 37, Q-value = 0.215\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99623090e-01 2.38628268e-10 3.76365209e-04 5.44055127e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = 0.200\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 25, Q-value = 0.204\n",
            "Action UP: Visits = 13, Q-value = 0.195\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.93903536e-02 1.98557221e-09 4.19734640e-01 5.60875004e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = 0.200\n",
            "Action DOWN: Visits = 5, Q-value = -0.079\n",
            "Action RIGHT: Visits = 34, Q-value = 0.208\n",
            "Action UP: Visits = 35, Q-value = 0.193\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.35006712e-01 1.86395774e-11 1.10064841e-06 6.49921878e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 47, Q-value = 0.191\n",
            "Action DOWN: Visits = 4, Q-value = 0.031\n",
            "Action RIGHT: Visits = 12, Q-value = 0.171\n",
            "Action UP: Visits = 36, Q-value = 0.204\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.18537254e-01 2.93423296e-07 1.73263522e-02 5.64136100e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.236\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 24, Q-value = 0.201\n",
            "Action UP: Visits = 34, Q-value = 0.227\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [9.74658863e-04 6.64591338e-09 9.98050676e-01 9.74658863e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 23, Q-value = 0.224\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 46, Q-value = 0.210\n",
            "Action UP: Visits = 23, Q-value = 0.193\n",
            "\n",
            "Episode finished!\n",
            "Final state: 2\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action DOWN\n",
            "Step 1: State 4 -> Action UP\n",
            "Step 2: State 0 -> Action LEFT\n",
            "Step 3: State 0 -> Action DOWN\n",
            "Step 4: State 4 -> Action UP\n",
            "Step 5: State 0 -> Action RIGHT\n",
            "Step 6: State 1 -> Action LEFT\n",
            "Step 7: State 0 -> Action DOWN\n",
            "Step 8: State 4 -> Action UP\n",
            "Step 9: State 0 -> Action DOWN\n",
            "Step 10: State 4 -> Action UP\n",
            "Step 11: State 0 -> Action LEFT\n",
            "Step 12: State 0 -> Action DOWN\n",
            "Step 13: State 4 -> Action UP\n",
            "Step 14: State 0 -> Action UP\n",
            "Step 15: State 0 -> Action UP\n",
            "Step 16: State 0 -> Action RIGHT\n",
            "Step 17: State 1 -> Action LEFT\n",
            "Step 18: State 0 -> Action DOWN\n",
            "Step 19: State 4 -> Action UP\n",
            "Step 20: State 0 -> Action DOWN\n",
            "Step 21: State 4 -> Action UP\n",
            "Step 22: State 0 -> Action DOWN\n",
            "Step 23: State 4 -> Action UP\n",
            "Step 24: State 0 -> Action UP\n",
            "Step 25: State 0 -> Action DOWN\n",
            "Step 26: State 4 -> Action UP\n",
            "Step 27: State 0 -> Action DOWN\n",
            "Step 28: State 4 -> Action UP\n",
            "Step 29: State 0 -> Action RIGHT\n",
            "Step 30: State 1 -> Action RIGHT\n",
            "Step 31: State 2 -> Action UP\n",
            "Step 32: State 2 -> Action LEFT\n",
            "Step 33: State 1 -> Action LEFT\n",
            "Step 34: State 0 -> Action RIGHT\n",
            "Step 35: State 1 -> Action UP\n",
            "Step 36: State 1 -> Action LEFT\n",
            "Step 37: State 0 -> Action LEFT\n",
            "Step 38: State 0 -> Action DOWN\n",
            "Step 39: State 4 -> Action UP\n",
            "Step 40: State 0 -> Action DOWN\n",
            "Step 41: State 4 -> Action UP\n",
            "Step 42: State 0 -> Action UP\n",
            "Step 43: State 0 -> Action LEFT\n",
            "Step 44: State 0 -> Action LEFT\n",
            "Step 45: State 0 -> Action LEFT\n",
            "Step 46: State 0 -> Action DOWN\n",
            "Step 47: State 4 -> Action UP\n",
            "Step 48: State 0 -> Action DOWN\n",
            "Step 49: State 4 -> Action UP\n",
            "Step 50: State 0 -> Action LEFT\n",
            "Step 51: State 0 -> Action DOWN\n",
            "Step 52: State 4 -> Action UP\n",
            "Step 53: State 0 -> Action DOWN\n",
            "Step 54: State 4 -> Action UP\n",
            "Step 55: State 0 -> Action DOWN\n",
            "Step 56: State 4 -> Action UP\n",
            "Step 57: State 0 -> Action DOWN\n",
            "Step 58: State 4 -> Action UP\n",
            "Step 59: State 0 -> Action DOWN\n",
            "Step 60: State 4 -> Action UP\n",
            "Step 61: State 0 -> Action DOWN\n",
            "Step 62: State 4 -> Action UP\n",
            "Step 63: State 0 -> Action DOWN\n",
            "Step 64: State 4 -> Action UP\n",
            "Step 65: State 0 -> Action UP\n",
            "Step 66: State 0 -> Action DOWN\n",
            "Step 67: State 4 -> Action UP\n",
            "Step 68: State 0 -> Action DOWN\n",
            "Step 69: State 4 -> Action UP\n",
            "Step 70: State 0 -> Action DOWN\n",
            "Step 71: State 4 -> Action UP\n",
            "Step 72: State 0 -> Action RIGHT\n",
            "Step 73: State 1 -> Action UP\n",
            "Step 74: State 1 -> Action UP\n",
            "Step 75: State 1 -> Action RIGHT\n",
            "Step 76: State 2 -> Action LEFT\n",
            "Step 77: State 1 -> Action UP\n",
            "Step 78: State 1 -> Action UP\n",
            "Step 79: State 1 -> Action RIGHT\n",
            "Step 80: State 2 -> Action UP\n",
            "Step 81: State 2 -> Action UP\n",
            "Step 82: State 2 -> Action UP\n",
            "Step 83: State 2 -> Action UP\n",
            "Step 84: State 2 -> Action UP\n",
            "Step 85: State 2 -> Action UP\n",
            "Step 86: State 2 -> Action UP\n",
            "Step 87: State 2 -> Action UP\n",
            "Step 88: State 2 -> Action UP\n",
            "Step 89: State 2 -> Action RIGHT\n",
            "Step 90: State 3 -> Action LEFT\n",
            "Step 91: State 2 -> Action UP\n",
            "Step 92: State 2 -> Action RIGHT\n",
            "Step 93: State 3 -> Action LEFT\n",
            "Step 94: State 2 -> Action RIGHT\n",
            "Step 95: State 3 -> Action LEFT\n",
            "Step 96: State 2 -> Action UP\n",
            "Step 97: State 2 -> Action LEFT\n",
            "Step 98: State 1 -> Action UP\n",
            "Step 99: State 1 -> Action RIGHT\n",
            "\n",
            "Running multiple episodes to check stability...\n",
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [2.98338627e-11 6.23651015e-15 9.99999999e-01 1.05615847e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.470\n",
            "Action DOWN: Visits = 3, Q-value = 0.084\n",
            "Action RIGHT: Visits = 79, Q-value = 0.736\n",
            "Action UP: Visits = 10, Q-value = 0.541\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [2.27601967e-06 3.85445929e-11 9.99747473e-01 2.50251009e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.573\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 55, Q-value = 0.684\n",
            "Action UP: Visits = 24, Q-value = 0.557\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [3.59050256e-17 9.76521941e-01 2.12015586e-02 2.27650002e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 44, Q-value = 0.794\n",
            "Action RIGHT: Visits = 30, Q-value = 0.699\n",
            "Action UP: Visits = 24, Q-value = 0.686\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 2.10446556e-08 4.29609636e-15 9.99999979e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 14, Q-value = 0.763\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 82, Q-value = 0.832\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.56844527e-06 9.86210887e-06 7.80234485e-10 9.99984569e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = 0.939\n",
            "Action DOWN: Visits = 18, Q-value = 0.702\n",
            "Action RIGHT: Visits = 7, Q-value = 0.387\n",
            "Action UP: Visits = 57, Q-value = 0.732\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.92640135e-13 2.41322380e-06 6.83808006e-03 9.93159507e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.200\n",
            "Action DOWN: Visits = 14, Q-value = 0.660\n",
            "Action RIGHT: Visits = 31, Q-value = 0.669\n",
            "Action UP: Visits = 51, Q-value = 0.703\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.38574922e-11 4.38574922e-01 1.20982969e-05 5.61412979e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 4, Q-value = 0.200\n",
            "Action DOWN: Visits = 40, Q-value = 0.820\n",
            "Action RIGHT: Visits = 14, Q-value = 0.788\n",
            "Action UP: Visits = 41, Q-value = 0.735\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [8.67138961e-19 9.99743156e-01 5.24325771e-11 2.56843757e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 64, Q-value = 0.788\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 28, Q-value = 0.738\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.77576943e-19 1.81838790e-06 1.04857409e-14 9.99998182e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.000\n",
            "Action DOWN: Visits = 20, Q-value = 0.892\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 75, Q-value = 0.818\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.36487448e-19 7.87055403e-08 8.25287406e-12 9.99999921e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 15, Q-value = 0.714\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 77, Q-value = 0.774\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [3.28812011e-18 8.52853675e-08 9.97304878e-01 2.69503625e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 11, Q-value = 0.760\n",
            "Action RIGHT: Visits = 56, Q-value = 0.834\n",
            "Action UP: Visits = 31, Q-value = 0.827\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99498991e-01 1.21945400e-15 5.01008904e-04 7.20075391e-11]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 62, Q-value = 0.862\n",
            "Action DOWN: Visits = 2, Q-value = 0.000\n",
            "Action RIGHT: Visits = 29, Q-value = 0.692\n",
            "Action UP: Visits = 6, Q-value = 0.452\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.37145694e-15 1.38515266e-03 2.03859108e-07 9.98614643e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 29, Q-value = 0.632\n",
            "Action RIGHT: Visits = 12, Q-value = 0.415\n",
            "Action UP: Visits = 56, Q-value = 0.715\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.90561536e-15 8.09535013e-02 4.66818681e-10 9.19046498e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 40, Q-value = 0.735\n",
            "Action RIGHT: Visits = 6, Q-value = 0.301\n",
            "Action UP: Visits = 51, Q-value = 0.740\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.60594460e-18 2.89170960e-02 2.78503857e-10 9.71082904e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 38, Q-value = 0.765\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 54, Q-value = 0.874\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.06676121e-10 2.48197982e-03 2.42380842e-06 9.97515596e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.443\n",
            "Action DOWN: Visits = 28, Q-value = 0.683\n",
            "Action RIGHT: Visits = 14, Q-value = 0.527\n",
            "Action UP: Visits = 51, Q-value = 0.669\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.32371353e-13 3.93522927e-08 3.22542522e-03 9.96774535e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.133\n",
            "Action DOWN: Visits = 10, Q-value = 0.679\n",
            "Action RIGHT: Visits = 31, Q-value = 0.662\n",
            "Action UP: Visits = 55, Q-value = 0.762\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [9.85236222e-15 9.39594481e-01 3.35479130e-08 6.04054856e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 50, Q-value = 0.682\n",
            "Action RIGHT: Visits = 9, Q-value = 0.401\n",
            "Action UP: Visits = 38, Q-value = 0.657\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 9.73881727e-05 4.95984939e-13 9.99902612e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 27, Q-value = 0.737\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 68, Q-value = 0.726\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.88382361e-11 3.88261992e-01 5.07482094e-09 6.11738003e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 4, Q-value = 0.200\n",
            "Action DOWN: Visits = 43, Q-value = 0.841\n",
            "Action RIGHT: Visits = 7, Q-value = 0.452\n",
            "Action UP: Visits = 45, Q-value = 0.754\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.61327502e-14 3.97283790e-02 5.88564427e-04 9.59683057e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 32, Q-value = 0.833\n",
            "Action RIGHT: Visits = 21, Q-value = 0.517\n",
            "Action UP: Visits = 44, Q-value = 0.675\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.54009989e-19 9.40247797e-06 2.14056303e-11 9.99990598e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 22, Q-value = 1.029\n",
            "Action RIGHT: Visits = 6, Q-value = 0.301\n",
            "Action UP: Visits = 70, Q-value = 0.815\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.24555120e-18 1.56065068e-02 3.16948359e-11 9.84393493e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 37, Q-value = 0.830\n",
            "Action RIGHT: Visits = 5, Q-value = 0.361\n",
            "Action UP: Visits = 56, Q-value = 0.830\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.36100940e-17 2.22087916e-03 8.21080934e-02 9.15671027e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 23, Q-value = 0.807\n",
            "Action RIGHT: Visits = 33, Q-value = 0.743\n",
            "Action UP: Visits = 42, Q-value = 0.800\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.22843994e-16 7.42790653e-09 3.38870973e-11 9.99999993e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 12, Q-value = 0.566\n",
            "Action RIGHT: Visits = 7, Q-value = 0.387\n",
            "Action UP: Visits = 78, Q-value = 0.815\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.05330637e-13 9.92271049e-01 4.25019978e-07 7.72852591e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.133\n",
            "Action DOWN: Visits = 52, Q-value = 0.824\n",
            "Action RIGHT: Visits = 12, Q-value = 0.640\n",
            "Action UP: Visits = 32, Q-value = 0.917\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.31322525e-20 5.37047498e-08 5.49936638e-15 9.99999946e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.000\n",
            "Action DOWN: Visits = 15, Q-value = 0.769\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 80, Q-value = 0.767\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.23825620e-12 2.63703503e-01 1.95804969e-04 7.36100692e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.200\n",
            "Action DOWN: Visits = 37, Q-value = 0.916\n",
            "Action RIGHT: Visits = 18, Q-value = 0.624\n",
            "Action UP: Visits = 41, Q-value = 0.733\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.54009989e-19 9.40247797e-06 2.14056303e-11 9.99990598e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 22, Q-value = 0.740\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 70, Q-value = 0.789\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.54961033e-10 5.61748679e-16 2.27258671e-05 9.99977274e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.520\n",
            "Action DOWN: Visits = 2, Q-value = 0.459\n",
            "Action RIGHT: Visits = 23, Q-value = 0.707\n",
            "Action UP: Visits = 67, Q-value = 0.867\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.61467886e-11 9.99766588e-01 1.77535818e-09 2.33410073e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 5, Q-value = 0.424\n",
            "Action DOWN: Visits = 60, Q-value = 0.740\n",
            "Action RIGHT: Visits = 8, Q-value = 0.395\n",
            "Action UP: Visits = 26, Q-value = 0.677\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.39763137e-16 1.50069526e-07 1.43117452e-13 9.99999850e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.000\n",
            "Action DOWN: Visits = 16, Q-value = 0.618\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 77, Q-value = 0.645\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.92660721e-08 2.14855605e-07 2.58995027e-09 9.99999763e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 11, Q-value = 0.505\n",
            "Action DOWN: Visits = 14, Q-value = 0.725\n",
            "Action RIGHT: Visits = 9, Q-value = 0.451\n",
            "Action UP: Visits = 65, Q-value = 0.760\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.73032827e-19 4.51119258e-05 4.61946120e-12 9.99954888e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 25, Q-value = 0.732\n",
            "Action RIGHT: Visits = 5, Q-value = 0.361\n",
            "Action UP: Visits = 68, Q-value = 0.846\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.52913998e-16 1.06352810e-05 6.37611326e-09 9.99989358e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 21, Q-value = 0.724\n",
            "Action RIGHT: Visits = 10, Q-value = 0.497\n",
            "Action UP: Visits = 66, Q-value = 0.828\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.64943632e-09 9.53673405e-07 4.61966519e-12 9.99999045e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 9, Q-value = 0.498\n",
            "Action DOWN: Visits = 17, Q-value = 0.821\n",
            "Action RIGHT: Visits = 5, Q-value = 0.271\n",
            "Action UP: Visits = 68, Q-value = 0.798\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.60471027e-15 6.76322332e-02 6.29874255e-11 9.32367767e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 40, Q-value = 1.012\n",
            "Action RIGHT: Visits = 5, Q-value = 0.361\n",
            "Action UP: Visits = 52, Q-value = 0.719\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.85693575e-15 2.86797199e-10 4.97350221e-12 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.133\n",
            "Action DOWN: Visits = 9, Q-value = 0.576\n",
            "Action RIGHT: Visits = 6, Q-value = 0.301\n",
            "Action UP: Visits = 81, Q-value = 0.697\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.76183334e-18 9.86103543e-06 1.14413120e-04 9.99875726e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 18, Q-value = 0.865\n",
            "Action RIGHT: Visits = 23, Q-value = 0.637\n",
            "Action UP: Visits = 57, Q-value = 0.758\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.44909654e-11 4.19484808e-02 4.50418383e-03 9.53547335e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 4, Q-value = 0.150\n",
            "Action DOWN: Visits = 30, Q-value = 0.713\n",
            "Action RIGHT: Visits = 24, Q-value = 0.564\n",
            "Action UP: Visits = 41, Q-value = 0.654\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.03945020e-15 2.86737259e-10 2.08997635e-04 9.99791002e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 7, Q-value = 0.482\n",
            "Action RIGHT: Visits = 27, Q-value = 0.596\n",
            "Action UP: Visits = 63, Q-value = 0.629\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.07938045e-20 1.43479926e-11 3.07130712e-12 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 7, Q-value = 0.712\n",
            "Action RIGHT: Visits = 6, Q-value = 0.301\n",
            "Action UP: Visits = 85, Q-value = 0.832\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [5.73529283e-15 9.79522319e-01 6.01389041e-09 2.04776751e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 53, Q-value = 0.749\n",
            "Action RIGHT: Visits = 8, Q-value = 0.339\n",
            "Action UP: Visits = 36, Q-value = 0.683\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 4.38418161e-04 7.78530986e-13 9.99561582e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 30, Q-value = 0.836\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 65, Q-value = 0.715\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.63158747e-03 3.18063678e-04 6.53586726e-03 9.90514482e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.871\n",
            "Action DOWN: Visits = 17, Q-value = 0.719\n",
            "Action RIGHT: Visits = 23, Q-value = 0.696\n",
            "Action UP: Visits = 38, Q-value = 0.746\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.48593298e-19 3.39674510e-08 3.36346186e-06 9.99996603e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 12, Q-value = 0.716\n",
            "Action RIGHT: Visits = 19, Q-value = 0.533\n",
            "Action UP: Visits = 67, Q-value = 0.751\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.05707781e-10 7.17258682e-01 2.26178097e-08 2.82741295e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 5, Q-value = 0.160\n",
            "Action DOWN: Visits = 45, Q-value = 0.843\n",
            "Action RIGHT: Visits = 8, Q-value = 0.452\n",
            "Action UP: Visits = 41, Q-value = 0.706\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 1.85661333e-20 1.94680018e-14 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 1, Q-value = 1.365\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 94, Q-value = 0.895\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.59962156e-17 3.89701054e-12 6.92020478e-11 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 6, Q-value = 0.983\n",
            "Action RIGHT: Visits = 8, Q-value = 0.339\n",
            "Action UP: Visits = 83, Q-value = 0.800\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.49136869e-11 9.99999469e-01 4.28330296e-07 1.02399946e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.560\n",
            "Action DOWN: Visits = 65, Q-value = 0.784\n",
            "Action RIGHT: Visits = 15, Q-value = 0.655\n",
            "Action UP: Visits = 13, Q-value = 0.660\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 1.31080560e-02 2.66207993e-11 9.86891944e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 37, Q-value = 0.806\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 57, Q-value = 0.760\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.92658376e-08 7.60608196e-16 1.23895060e-05 9.99987591e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 11, Q-value = 0.505\n",
            "Action DOWN: Visits = 2, Q-value = 0.459\n",
            "Action RIGHT: Visits = 21, Q-value = 0.645\n",
            "Action UP: Visits = 65, Q-value = 0.759\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.86797199e-20 2.93680332e-17 1.73415299e-12 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 2, Q-value = 0.459\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 90, Q-value = 0.742\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.53910759e-14 7.17235048e-06 1.55674369e-01 8.44318459e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 14, Q-value = 1.112\n",
            "Action RIGHT: Visits = 38, Q-value = 0.690\n",
            "Action UP: Visits = 45, Q-value = 0.717\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.42572199e-09 2.86495085e-04 7.60393932e-06 9.99705894e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 8, Q-value = 0.594\n",
            "Action DOWN: Visits = 23, Q-value = 0.657\n",
            "Action RIGHT: Visits = 16, Q-value = 0.649\n",
            "Action UP: Visits = 52, Q-value = 0.721\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [2.38439408e-10 3.94335187e-08 9.98831952e-01 1.16800808e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.539\n",
            "Action DOWN: Visits = 10, Q-value = 0.634\n",
            "Action RIGHT: Visits = 55, Q-value = 0.810\n",
            "Action UP: Visits = 28, Q-value = 0.653\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999993e-01 1.40696696e-11 6.03529145e-09 8.11328048e-10]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 73, Q-value = 0.611\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 11, Q-value = 0.411\n",
            "Action UP: Visits = 9, Q-value = 0.351\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.19964830e-19 6.91777661e-08 1.17153154e-12 9.99999931e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 15, Q-value = 0.664\n",
            "Action RIGHT: Visits = 5, Q-value = 0.271\n",
            "Action UP: Visits = 78, Q-value = 0.810\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.67894476e-15 7.69438875e-02 1.01491092e-08 9.23056102e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 39, Q-value = 0.750\n",
            "Action RIGHT: Visits = 8, Q-value = 0.339\n",
            "Action UP: Visits = 50, Q-value = 0.766\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [9.45209427e-18 9.23056081e-01 3.29574149e-08 7.69438857e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 50, Q-value = 0.732\n",
            "Action RIGHT: Visits = 9, Q-value = 0.401\n",
            "Action UP: Visits = 39, Q-value = 0.705\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.77576943e-19 1.81838790e-06 1.04857409e-14 9.99998182e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.000\n",
            "Action DOWN: Visits = 20, Q-value = 0.612\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 75, Q-value = 0.763\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.32047966e-08 3.27575375e-04 2.26609342e-11 9.99672401e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = 0.536\n",
            "Action DOWN: Visits = 26, Q-value = 0.617\n",
            "Action RIGHT: Visits = 5, Q-value = 0.271\n",
            "Action UP: Visits = 58, Q-value = 0.729\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.99895148e-11 5.76589927e-09 1.04846605e-04 9.99895148e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.560\n",
            "Action DOWN: Visits = 9, Q-value = 0.622\n",
            "Action RIGHT: Visits = 24, Q-value = 0.658\n",
            "Action UP: Visits = 60, Q-value = 0.760\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.69710910e-18 9.90303345e-01 4.69710910e-08 9.69660754e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 54, Q-value = 0.824\n",
            "Action RIGHT: Visits = 10, Q-value = 0.542\n",
            "Action UP: Visits = 34, Q-value = 0.737\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 5.71751272e-10 5.58350851e-13 9.99999999e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 10, Q-value = 0.633\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 84, Q-value = 0.793\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.32686597e-09 2.38271076e-16 6.73056814e-08 9.99999930e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = 0.521\n",
            "Action DOWN: Visits = 2, Q-value = 0.459\n",
            "Action RIGHT: Visits = 14, Q-value = 0.548\n",
            "Action UP: Visits = 73, Q-value = 0.783\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.07144370e-11 9.97859704e-01 2.10381275e-03 3.64833318e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 4, Q-value = 0.480\n",
            "Action DOWN: Visits = 50, Q-value = 0.704\n",
            "Action RIGHT: Visits = 27, Q-value = 0.552\n",
            "Action UP: Visits = 18, Q-value = 0.610\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.26005135e-14 2.14244459e-03 1.43241928e-15 9.97857555e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.000\n",
            "Action DOWN: Visits = 33, Q-value = 0.943\n",
            "Action RIGHT: Visits = 2, Q-value = 0.000\n",
            "Action UP: Visits = 61, Q-value = 0.724\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.32618696e-01 4.62412801e-02 1.35801545e-08 8.21140010e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.744\n",
            "Action DOWN: Visits = 27, Q-value = 0.887\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 36, Q-value = 0.785\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.39967733e-07 2.63342684e-08 9.99999771e-01 6.28647028e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 13, Q-value = 0.533\n",
            "Action DOWN: Visits = 11, Q-value = 0.635\n",
            "Action RIGHT: Visits = 63, Q-value = 0.726\n",
            "Action UP: Visits = 12, Q-value = 0.561\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.62838170e-03 1.62838170e-13 9.98371609e-01 9.61543109e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.620\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 57, Q-value = 0.614\n",
            "Action UP: Visits = 9, Q-value = 0.401\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999999e-01 6.59962156e-17 6.44494293e-10 6.75801248e-14]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 83, Q-value = 0.764\n",
            "Action DOWN: Visits = 2, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.361\n",
            "Action UP: Visits = 4, Q-value = 0.226\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.18280012e-10 1.86551584e-04 6.82062149e-09 9.99813441e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.476\n",
            "Action DOWN: Visits = 25, Q-value = 0.821\n",
            "Action RIGHT: Visits = 9, Q-value = 0.452\n",
            "Action UP: Visits = 59, Q-value = 0.851\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [3.28044091e-18 9.94975734e-01 1.14381902e-08 5.02425436e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 56, Q-value = 0.807\n",
            "Action RIGHT: Visits = 9, Q-value = 0.552\n",
            "Action UP: Visits = 33, Q-value = 0.757\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.61473057e-13 9.99798611e-03 1.61473057e-13 9.90002014e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.000\n",
            "Action DOWN: Visits = 36, Q-value = 0.755\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 57, Q-value = 0.713\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [8.58036869e-15 9.97488171e-01 2.48191159e-03 2.99178957e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 51, Q-value = 0.803\n",
            "Action RIGHT: Visits = 28, Q-value = 0.839\n",
            "Action UP: Visits = 18, Q-value = 0.713\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.05615841e-19 6.09034158e-08 1.10746236e-13 9.99999939e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.000\n",
            "Action DOWN: Visits = 15, Q-value = 0.662\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 79, Q-value = 0.746\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.18176898e-14 6.07031474e-01 1.12702273e-10 3.92968526e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 47, Q-value = 0.737\n",
            "Action RIGHT: Visits = 5, Q-value = 0.361\n",
            "Action UP: Visits = 45, Q-value = 0.847\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 1.14182124e-03 1.06340390e-12 9.98858179e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 32, Q-value = 0.796\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 63, Q-value = 0.806\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.67330554e-19 2.24963208e-08 3.59304789e-05 9.99964047e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 11, Q-value = 0.902\n",
            "Action RIGHT: Visits = 23, Q-value = 0.715\n",
            "Action UP: Visits = 64, Q-value = 0.802\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.21671246e-09 1.49277237e-07 3.08164606e-02 9.69183386e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.338\n",
            "Action DOWN: Visits = 10, Q-value = 0.769\n",
            "Action RIGHT: Visits = 34, Q-value = 0.757\n",
            "Action UP: Visits = 48, Q-value = 0.762\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.91347452e-18 4.38337658e-04 1.83621349e-04 9.99378041e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 24, Q-value = 0.703\n",
            "Action RIGHT: Visits = 22, Q-value = 0.677\n",
            "Action UP: Visits = 52, Q-value = 0.780\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.52993043e-17 2.55190575e-04 9.93308281e-01 6.43652798e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 21, Q-value = 0.796\n",
            "Action RIGHT: Visits = 48, Q-value = 0.785\n",
            "Action UP: Visits = 29, Q-value = 0.770\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.44030222e-01 1.03459747e-14 1.05942781e-11 6.55969778e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 45, Q-value = 0.797\n",
            "Action DOWN: Visits = 2, Q-value = 0.000\n",
            "Action RIGHT: Visits = 4, Q-value = 0.226\n",
            "Action UP: Visits = 48, Q-value = 0.710\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.14568407e-06 3.14568407e-16 1.85749499e-11 9.99996854e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.665\n",
            "Action DOWN: Visits = 2, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.376\n",
            "Action UP: Visits = 71, Q-value = 0.671\n",
            "\n",
            "Current state: 3\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 5.20128558e-17 5.45394323e-11 5.32611643e-14]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 85, Q-value = 0.653\n",
            "Action DOWN: Visits = 2, Q-value = 0.000\n",
            "Action RIGHT: Visits = 8, Q-value = 0.395\n",
            "Action UP: Visits = 4, Q-value = 0.226\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [5.31038878e-18 9.28720461e-01 5.56834623e-12 7.12795394e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 53, Q-value = 0.846\n",
            "Action RIGHT: Visits = 4, Q-value = 0.339\n",
            "Action UP: Visits = 41, Q-value = 1.007\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.45770301e-13 1.01329792e-01 7.91586715e-12 8.98670208e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.000\n",
            "Action DOWN: Visits = 41, Q-value = 0.784\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 51, Q-value = 0.663\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [9.00424105e-09 9.98272262e-01 1.15606104e-06 1.72657276e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 8, Q-value = 0.480\n",
            "Action DOWN: Visits = 51, Q-value = 0.689\n",
            "Action RIGHT: Visits = 13, Q-value = 0.460\n",
            "Action UP: Visits = 27, Q-value = 0.624\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 4.74310109e-08 4.85693552e-15 9.99999953e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 15, Q-value = 0.513\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 81, Q-value = 0.723\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.23651007e-15 1.45600412e-08 1.10746241e-13 9.99999985e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.133\n",
            "Action DOWN: Visits = 13, Q-value = 0.697\n",
            "Action RIGHT: Visits = 4, Q-value = 0.226\n",
            "Action UP: Visits = 79, Q-value = 0.787\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [5.48592072e-19 9.99994368e-01 5.61758282e-06 1.42290655e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 67, Q-value = 0.797\n",
            "Action RIGHT: Visits = 20, Q-value = 0.700\n",
            "Action UP: Visits = 11, Q-value = 0.634\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 4.85193928e-11 4.41281307e-13 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 8, Q-value = 0.797\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 86, Q-value = 0.883\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.08035332e-16 8.79451470e-01 8.85488137e-02 3.19997159e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 39, Q-value = 0.811\n",
            "Action RIGHT: Visits = 31, Q-value = 0.662\n",
            "Action UP: Visits = 28, Q-value = 0.701\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.18929971e-18 1.82150645e-03 7.02269587e-14 9.98178494e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.000\n",
            "Action DOWN: Visits = 33, Q-value = 0.782\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 62, Q-value = 0.712\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.03087064e-19 1.24514025e-06 1.98327211e-12 9.99998755e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 19, Q-value = 0.820\n",
            "Action RIGHT: Visits = 5, Q-value = 0.271\n",
            "Action UP: Visits = 74, Q-value = 0.739\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.80708004e-15 7.56197578e-03 2.67704013e-11 9.92438024e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = 0.200\n",
            "Action DOWN: Visits = 35, Q-value = 0.904\n",
            "Action RIGHT: Visits = 5, Q-value = 0.361\n",
            "Action UP: Visits = 57, Q-value = 0.840\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.40172548e-18 8.88741344e-05 1.93239760e-07 9.99910933e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = 0.200\n",
            "Action DOWN: Visits = 24, Q-value = 0.735\n",
            "Action RIGHT: Visits = 13, Q-value = 0.552\n",
            "Action UP: Visits = 61, Q-value = 0.792\n",
            "\n",
            "Current state: 2\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.36879290e-13 9.98631205e-01 2.48900098e-09 1.36879290e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 3, Q-value = 0.200\n",
            "Action DOWN: Visits = 58, Q-value = 0.735\n",
            "Action RIGHT: Visits = 8, Q-value = 0.395\n",
            "Action UP: Visits = 30, Q-value = 0.676\n",
            "\n",
            "Current state: 6\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.00000000e+00 6.01027576e-03 9.70693090e-14 9.93989724e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 0, Q-value = 0.000\n",
            "Action DOWN: Visits = 36, Q-value = 0.779\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 60, Q-value = 0.720\n",
            "\n",
            "Episode finished!\n",
            "Final state: 2\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action RIGHT\n",
            "Step 1: State 1 -> Action RIGHT\n",
            "Step 2: State 2 -> Action DOWN\n",
            "Step 3: State 6 -> Action UP\n",
            "Step 4: State 2 -> Action UP\n",
            "Step 5: State 2 -> Action UP\n",
            "Step 6: State 2 -> Action UP\n",
            "Step 7: State 2 -> Action DOWN\n",
            "Step 8: State 6 -> Action UP\n",
            "Step 9: State 2 -> Action UP\n",
            "Step 10: State 2 -> Action RIGHT\n",
            "Step 11: State 3 -> Action LEFT\n",
            "Step 12: State 2 -> Action UP\n",
            "Step 13: State 2 -> Action UP\n",
            "Step 14: State 2 -> Action UP\n",
            "Step 15: State 2 -> Action UP\n",
            "Step 16: State 2 -> Action UP\n",
            "Step 17: State 2 -> Action DOWN\n",
            "Step 18: State 6 -> Action UP\n",
            "Step 19: State 2 -> Action UP\n",
            "Step 20: State 2 -> Action UP\n",
            "Step 21: State 2 -> Action UP\n",
            "Step 22: State 2 -> Action UP\n",
            "Step 23: State 2 -> Action UP\n",
            "Step 24: State 2 -> Action UP\n",
            "Step 25: State 2 -> Action DOWN\n",
            "Step 26: State 6 -> Action UP\n",
            "Step 27: State 2 -> Action UP\n",
            "Step 28: State 2 -> Action UP\n",
            "Step 29: State 2 -> Action UP\n",
            "Step 30: State 2 -> Action DOWN\n",
            "Step 31: State 6 -> Action UP\n",
            "Step 32: State 2 -> Action UP\n",
            "Step 33: State 2 -> Action UP\n",
            "Step 34: State 2 -> Action UP\n",
            "Step 35: State 2 -> Action UP\n",
            "Step 36: State 2 -> Action UP\n",
            "Step 37: State 2 -> Action UP\n",
            "Step 38: State 2 -> Action UP\n",
            "Step 39: State 2 -> Action UP\n",
            "Step 40: State 2 -> Action UP\n",
            "Step 41: State 2 -> Action UP\n",
            "Step 42: State 2 -> Action DOWN\n",
            "Step 43: State 6 -> Action UP\n",
            "Step 44: State 2 -> Action UP\n",
            "Step 45: State 2 -> Action UP\n",
            "Step 46: State 2 -> Action DOWN\n",
            "Step 47: State 6 -> Action UP\n",
            "Step 48: State 2 -> Action UP\n",
            "Step 49: State 2 -> Action DOWN\n",
            "Step 50: State 6 -> Action UP\n",
            "Step 51: State 2 -> Action UP\n",
            "Step 52: State 2 -> Action UP\n",
            "Step 53: State 2 -> Action UP\n",
            "Step 54: State 2 -> Action UP\n",
            "Step 55: State 2 -> Action RIGHT\n",
            "Step 56: State 3 -> Action LEFT\n",
            "Step 57: State 2 -> Action UP\n",
            "Step 58: State 2 -> Action UP\n",
            "Step 59: State 2 -> Action DOWN\n",
            "Step 60: State 6 -> Action UP\n",
            "Step 61: State 2 -> Action UP\n",
            "Step 62: State 2 -> Action UP\n",
            "Step 63: State 2 -> Action DOWN\n",
            "Step 64: State 6 -> Action UP\n",
            "Step 65: State 2 -> Action UP\n",
            "Step 66: State 2 -> Action DOWN\n",
            "Step 67: State 6 -> Action UP\n",
            "Step 68: State 2 -> Action UP\n",
            "Step 69: State 2 -> Action RIGHT\n",
            "Step 70: State 3 -> Action RIGHT\n",
            "Step 71: State 3 -> Action LEFT\n",
            "Step 72: State 2 -> Action UP\n",
            "Step 73: State 2 -> Action DOWN\n",
            "Step 74: State 6 -> Action UP\n",
            "Step 75: State 2 -> Action DOWN\n",
            "Step 76: State 6 -> Action UP\n",
            "Step 77: State 2 -> Action DOWN\n",
            "Step 78: State 6 -> Action UP\n",
            "Step 79: State 2 -> Action UP\n",
            "Step 80: State 2 -> Action UP\n",
            "Step 81: State 2 -> Action UP\n",
            "Step 82: State 2 -> Action RIGHT\n",
            "Step 83: State 3 -> Action UP\n",
            "Step 84: State 3 -> Action UP\n",
            "Step 85: State 3 -> Action LEFT\n",
            "Step 86: State 2 -> Action DOWN\n",
            "Step 87: State 6 -> Action UP\n",
            "Step 88: State 2 -> Action DOWN\n",
            "Step 89: State 6 -> Action UP\n",
            "Step 90: State 2 -> Action UP\n",
            "Step 91: State 2 -> Action DOWN\n",
            "Step 92: State 6 -> Action UP\n",
            "Step 93: State 2 -> Action DOWN\n",
            "Step 94: State 6 -> Action UP\n",
            "Step 95: State 2 -> Action UP\n",
            "Step 96: State 2 -> Action UP\n",
            "Step 97: State 2 -> Action UP\n",
            "Step 98: State 2 -> Action DOWN\n",
            "Step 99: State 6 -> Action UP\n",
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.17153154e-12 1.19964830e-19 9.99999931e-01 6.91777661e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 5, Q-value = -0.632\n",
            "Action DOWN: Visits = 1, Q-value = -1.389\n",
            "Action RIGHT: Visits = 78, Q-value = -0.094\n",
            "Action UP: Visits = 15, Q-value = -0.358\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.39763119e-16 9.99999725e-01 8.05944574e-15 2.75157809e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 2, Q-value = -1.013\n",
            "Action DOWN: Visits = 77, Q-value = 0.000\n",
            "Action RIGHT: Visits = 3, Q-value = -0.403\n",
            "Action UP: Visits = 17, Q-value = -0.151\n",
            "\n",
            "Episode finished!\n",
            "Final state: 5\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action RIGHT\n",
            "Step 1: State 1 -> Action DOWN\n",
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999953e-01 8.42266926e-17 8.22526295e-20 4.74310109e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 81, Q-value = 0.150\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 15, Q-value = 0.010\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.08873680e-06 1.77577073e-19 1.86203057e-13 9.99998911e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.127\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.394\n",
            "Action UP: Visits = 75, Q-value = 0.208\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99671754e-01 6.71209110e-07 2.32047816e-18 3.27575162e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 58, Q-value = 0.092\n",
            "Action DOWN: Visits = 14, Q-value = -0.130\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 26, Q-value = -0.009\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.39395437e-06 2.03086222e-19 2.07960291e-16 9.99994606e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.011\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.793\n",
            "Action UP: Visits = 74, Q-value = 0.169\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.71851751e-01 2.68442618e-18 2.68442618e-18 2.81482486e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 57, Q-value = 0.032\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 40, Q-value = 0.046\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99916805e-01 1.95633737e-18 2.15101569e-06 8.10442320e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.044\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 16, Q-value = -0.121\n",
            "Action UP: Visits = 23, Q-value = -0.017\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.08330197e-05 7.97508902e-10 7.42738044e-19 9.99929166e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = 0.016\n",
            "Action DOWN: Visits = 8, Q-value = -0.179\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 65, Q-value = 0.132\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99992109e-01 5.07934037e-10 4.84403645e-16 7.89042495e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 68, Q-value = 0.043\n",
            "Action DOWN: Visits = 8, Q-value = -0.276\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 21, Q-value = -0.065\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.53650068e-01 7.28028128e-15 4.19817704e-13 8.46349932e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 43, Q-value = 0.056\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 51, Q-value = 0.055\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.35609107e-04 7.42169976e-19 7.42169976e-19 9.99164391e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.050\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 65, Q-value = 0.064\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.02795297e-02 3.23014421e-18 3.38705170e-12 9.79720470e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 38, Q-value = -0.030\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.117\n",
            "Action UP: Visits = 56, Q-value = 0.070\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.02795297e-02 3.23014421e-18 3.38705170e-12 9.79720470e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 38, Q-value = 0.028\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 56, Q-value = 0.048\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.73862535e-01 3.21083057e-18 1.89596334e-13 2.61374647e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.081\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.481\n",
            "Action UP: Visits = 39, Q-value = 0.164\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.99165447e-04 1.01469195e-18 9.90910104e-12 9.99400835e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.101\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 5, Q-value = -0.462\n",
            "Action UP: Visits = 63, Q-value = 0.086\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.89845251e-05 4.08782374e-19 4.28639387e-13 9.99961015e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = 0.157\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 69, Q-value = 0.141\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.87865727e-01 1.93275943e-18 1.93275943e-18 1.21342732e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.132\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 38, Q-value = 0.180\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.71851751e-01 2.68442618e-18 2.68442618e-18 2.81482486e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 57, Q-value = 0.041\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 40, Q-value = 0.142\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99950027e-01 3.53995627e-19 3.62491522e-16 4.99725345e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 70, Q-value = 0.087\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 26, Q-value = -0.009\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99769255e-01 5.48468576e-19 5.61631822e-16 2.30744697e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 67, Q-value = 0.192\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.793\n",
            "Action UP: Visits = 29, Q-value = 0.126\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.13832033e-06 9.82952954e-01 8.25716515e-18 1.70459081e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 13, Q-value = -0.063\n",
            "Action DOWN: Visits = 51, Q-value = 0.136\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 34, Q-value = 0.046\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.44494293e-10 6.59962156e-17 6.75801248e-14 9.99999999e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = -0.195\n",
            "Action DOWN: Visits = 2, Q-value = -0.415\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 83, Q-value = 0.148\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 4.02538546e-20 4.02538546e-20 4.02538546e-10]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 87, Q-value = 0.097\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 10, Q-value = -0.097\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.11373320e-02 3.82503424e-18 4.01083910e-12 9.68862668e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.001\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 55, Q-value = 0.236\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999991e-01 6.44494288e-20 6.59962150e-17 8.88490105e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 83, Q-value = 0.136\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.793\n",
            "Action UP: Visits = 13, Q-value = -0.040\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99801022e-01 4.72960040e-19 4.72960040e-19 1.98977710e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 68, Q-value = 0.112\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 29, Q-value = 0.042\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99997966e-01 5.48594046e-19 1.95873706e-06 7.56283478e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 67, Q-value = 0.077\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 18, Q-value = -0.115\n",
            "Action UP: Visits = 13, Q-value = -0.055\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 3.20700719e-20 3.20700719e-20 3.44349775e-11]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 89, Q-value = 0.089\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 8, Q-value = -0.105\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.69348010e-05 2.73506491e-16 2.67096183e-19 9.99983065e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = -0.050\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 72, Q-value = 0.086\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.97225903e-01 2.02173627e-01 2.76640481e-08 6.00442384e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.040\n",
            "Action DOWN: Visits = 34, Q-value = 0.127\n",
            "Action RIGHT: Visits = 7, Q-value = -0.224\n",
            "Action UP: Visits = 19, Q-value = -0.070\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.92110224e-01 1.64076892e-18 1.64076892e-18 7.88977580e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 60, Q-value = 0.059\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 37, Q-value = -0.019\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.94407184e-06 5.19615876e-05 1.95639076e-18 9.99944094e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = -0.070\n",
            "Action DOWN: Visits = 22, Q-value = 0.144\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 59, Q-value = 0.043\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.61374647e-02 3.28789050e-15 3.28789050e-15 9.73862535e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = -0.051\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 56, Q-value = 0.009\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98178494e-01 1.21784290e-15 1.21784290e-15 1.82150645e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 62, Q-value = 0.094\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 33, Q-value = 0.103\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.69348010e-05 2.47180520e-11 4.08791388e-19 9.99983065e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 23, Q-value = -0.069\n",
            "Action DOWN: Visits = 6, Q-value = -0.281\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 69, Q-value = 0.083\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 2.05514220e-11 7.27547707e-20 2.53680199e-10]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 82, Q-value = 0.100\n",
            "Action DOWN: Visits = 7, Q-value = -0.315\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 9, Q-value = -0.162\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.85454061e-02 2.27819176e-18 2.27819176e-18 9.81454594e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.071\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 58, Q-value = 0.088\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.37272920e-01 6.19336281e-18 6.19336281e-18 2.62727080e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.074\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 46, Q-value = 0.135\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.59451328e-06 1.55547470e-19 1.55547470e-19 9.99997405e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = -0.012\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 76, Q-value = 0.117\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.88490105e-09 6.59962150e-17 6.44494288e-20 9.99999991e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 13, Q-value = -0.048\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 83, Q-value = 0.184\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.92956998e-05 3.14560181e-16 3.07187677e-19 9.99970704e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = -0.058\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 71, Q-value = 0.078\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999999e-01 5.71751271e-20 3.37613408e-15 1.48297555e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 84, Q-value = 0.010\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.481\n",
            "Action UP: Visits = 11, Q-value = -0.145\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98447402e-01 1.01372393e-18 1.03805330e-15 1.55259826e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 63, Q-value = 0.065\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 33, Q-value = 0.133\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99997828e-01 3.54012549e-19 2.17047439e-06 1.23436543e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 70, Q-value = 0.087\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 19, Q-value = 0.001\n",
            "Action UP: Visits = 9, Q-value = -0.140\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.88961333e-01 2.29561670e-18 1.35553871e-13 1.10386666e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 58, Q-value = 0.171\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.481\n",
            "Action UP: Visits = 37, Q-value = 0.076\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.02538546e-10 4.02538546e-20 4.02538546e-20 1.00000000e+00]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = -0.087\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 87, Q-value = 0.129\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99609549e-01 3.76360111e-04 3.94642180e-18 1.40905697e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = -0.015\n",
            "Action DOWN: Visits = 25, Q-value = -0.151\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 18, Q-value = -0.101\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.09925130e-03 1.39470195e-18 1.39470195e-18 9.94900749e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 36, Q-value = 0.009\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 61, Q-value = 0.069\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999776e-01 2.03087272e-19 2.18063298e-10 2.23296817e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 74, Q-value = 0.055\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 8, Q-value = -0.305\n",
            "Action UP: Visits = 16, Q-value = -0.092\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.54205586e-01 9.57102129e-15 9.80072580e-12 7.45794414e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = -0.017\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.420\n",
            "Action UP: Visits = 49, Q-value = 0.014\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99969222e-01 7.42767798e-19 7.42767798e-09 3.07702785e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 65, Q-value = 0.078\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 10, Q-value = -0.206\n",
            "Action UP: Visits = 23, Q-value = 0.032\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.86304323e-02 2.34270819e-10 3.87441102e-18 9.81369567e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.171\n",
            "Action DOWN: Visits = 6, Q-value = -0.200\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 55, Q-value = 0.108\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99996603e-01 3.36346186e-06 5.48593298e-19 3.39674510e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 67, Q-value = 0.107\n",
            "Action DOWN: Visits = 19, Q-value = -0.040\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 12, Q-value = -0.101\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.81111168e-01 3.18577721e-01 5.07433849e-17 3.11111055e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 41, Q-value = 0.035\n",
            "Action DOWN: Visits = 38, Q-value = 0.033\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 19, Q-value = -0.001\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999976e-01 8.22526314e-20 4.85693563e-15 2.37919565e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 81, Q-value = 0.130\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 14, Q-value = -0.089\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.53956580e-03 4.07062233e-04 1.53261828e-17 9.95053372e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.030\n",
            "Action DOWN: Visits = 22, Q-value = -0.023\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 48, Q-value = 0.145\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.76906347e-03 2.31016996e-18 1.39687143e-10 9.95230936e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 34, Q-value = 0.001\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 6, Q-value = -0.424\n",
            "Action UP: Visits = 58, Q-value = 0.063\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.94358018e-01 3.96663221e-06 6.87874711e-18 5.63801571e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 52, Q-value = 0.161\n",
            "Action DOWN: Visits = 15, Q-value = -0.023\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 31, Q-value = 0.110\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.85636092e-01 2.34280757e-15 2.28789802e-18 1.43639084e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 58, Q-value = 0.103\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 38, Q-value = 0.055\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.57620883e-01 3.15728174e-18 3.15728174e-18 4.23791172e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.030\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 41, Q-value = 0.094\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999981e-01 6.44494281e-20 6.44494281e-20 1.86422971e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 83, Q-value = 0.185\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 14, Q-value = -0.096\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.31110882e-01 1.52832387e-04 4.28045902e-17 2.68736286e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 42, Q-value = 0.072\n",
            "Action DOWN: Visits = 18, Q-value = -0.115\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 38, Q-value = 0.026\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.71641624e-06 1.77576429e-19 1.77576429e-19 9.99995284e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.057\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 75, Q-value = 0.240\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.92956998e-05 3.14560181e-16 3.07187677e-19 9.99970704e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = -0.021\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 71, Q-value = 0.027\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98646319e-01 4.66530028e-10 1.65157843e-18 1.35368040e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 60, Q-value = 0.138\n",
            "Action DOWN: Visits = 7, Q-value = -0.260\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 31, Q-value = 0.024\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99687600e-01 7.42558611e-19 7.78629138e-13 3.12399779e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 65, Q-value = 0.056\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 29, Q-value = -0.076\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99997920e-01 2.07960980e-16 1.19920781e-14 2.07960980e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 74, Q-value = 0.188\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.481\n",
            "Action UP: Visits = 20, Q-value = 0.135\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.04839703e-04 3.94728945e-18 6.58403181e-05 9.99829320e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = -0.044\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 21, Q-value = 0.013\n",
            "Action UP: Visits = 55, Q-value = 0.082\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.53312808e-05 3.62372221e-04 5.71534538e-18 9.99542296e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = -0.069\n",
            "Action DOWN: Visits = 24, Q-value = -0.075\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 53, Q-value = 0.069\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999812e-01 9.31322400e-20 9.31322400e-20 1.87754028e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 80, Q-value = 0.108\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 17, Q-value = 0.001\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99687600e-01 7.42558611e-19 7.78629138e-13 3.12399779e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 65, Q-value = 0.120\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 29, Q-value = 0.008\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99974528e-01 2.67093902e-19 2.67093902e-19 2.54720595e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 72, Q-value = 0.139\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 25, Q-value = 0.076\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.90486513e-06 3.45714089e-12 2.09040089e-14 9.99994095e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.041\n",
            "Action DOWN: Visits = 5, Q-value = -0.198\n",
            "Action RIGHT: Visits = 3, Q-value = -0.481\n",
            "Action UP: Visits = 70, Q-value = 0.173\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.06632002e-01 4.40533003e-07 1.00291504e-12 8.93367557e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 38, Q-value = 0.002\n",
            "Action DOWN: Visits = 11, Q-value = -0.098\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 47, Q-value = 0.063\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 2.86797199e-20 2.86797199e-20 8.10131102e-12]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 90, Q-value = 0.090\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 7, Q-value = -0.175\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.85454061e-02 2.27819176e-18 2.27819176e-18 9.81454594e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.035\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 58, Q-value = 0.063\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.36812954e-07 1.39763041e-16 1.36487345e-19 9.99999163e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.058\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 77, Q-value = 0.091\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99927117e-01 3.53987516e-19 3.53987516e-19 7.28828904e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 70, Q-value = 0.173\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 27, Q-value = 0.017\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.36250999e-03 2.25241327e-11 2.30647119e-18 9.93637490e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.050\n",
            "Action DOWN: Visits = 5, Q-value = -0.295\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 58, Q-value = 0.107\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98447402e-01 1.01372393e-18 1.03805330e-15 1.55259826e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 63, Q-value = 0.137\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 33, Q-value = 0.109\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.95178421e-01 9.31568735e-18 9.76820617e-12 6.04821579e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = 0.108\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 48, Q-value = 0.055\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.88559787e-03 1.43135248e-15 1.43135248e-15 9.97114402e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 34, Q-value = -0.094\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 61, Q-value = 0.025\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999999e-01 8.22526333e-10 8.22526333e-20 2.32343331e-11]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 81, Q-value = 0.083\n",
            "Action DOWN: Visits = 10, Q-value = -0.157\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 7, Q-value = -0.189\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.73881727e-05 4.84360292e-16 4.84360292e-16 9.99902612e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.092\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 2, Q-value = -0.722\n",
            "Action UP: Visits = 68, Q-value = 0.183\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.82166004e-01 8.00937988e-18 8.39844352e-12 2.17833996e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 50, Q-value = 0.069\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 44, Q-value = 0.009\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.58533413e-01 7.59261785e-18 4.48336492e-13 7.41466587e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 45, Q-value = -0.032\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 50, Q-value = 0.088\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.31721696e-02 3.69856188e-18 3.69856188e-18 9.36827830e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 42, Q-value = 0.101\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 55, Q-value = 0.056\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99998911e-01 1.77577073e-19 1.86203057e-13 1.08873680e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 75, Q-value = -0.011\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 19, Q-value = -0.186\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.49706130e-01 6.29841395e-05 3.12422272e-17 1.50230886e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.036\n",
            "Action DOWN: Visits = 17, Q-value = -0.102\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 37, Q-value = 0.019\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 4.02538546e-20 4.22092259e-14 1.13707176e-11]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 87, Q-value = 0.097\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 4, Q-value = -0.577\n",
            "Action UP: Visits = 7, Q-value = -0.189\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999975e-01 1.25746311e-08 2.03087312e-19 1.25746311e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 74, Q-value = 0.094\n",
            "Action DOWN: Visits = 12, Q-value = -0.150\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 12, Q-value = -0.109\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.62727080e-01 6.19336281e-18 6.19336281e-18 7.37272920e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = 0.106\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 51, Q-value = 0.124\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99498991e-01 3.36392160e-10 1.19087304e-18 5.01008904e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 62, Q-value = 0.071\n",
            "Action DOWN: Visits = 7, Q-value = -0.378\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 29, Q-value = 0.004\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.82825855e-05 4.60693515e-05 2.76197124e-18 9.99925648e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = -0.087\n",
            "Action DOWN: Visits = 21, Q-value = -0.062\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 57, Q-value = 0.019\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.73881727e-05 4.73008098e-19 2.79306552e-14 9.99902612e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.103\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 68, Q-value = 0.175\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.95230629e-01 1.08551205e-17 6.56367624e-10 7.04769370e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.119\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 6, Q-value = -0.424\n",
            "Action UP: Visits = 48, Q-value = 0.099\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.18607717e-06 4.08796599e-19 1.42538560e-09 9.99995812e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = -0.158\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 9, Q-value = -0.217\n",
            "Action UP: Visits = 69, Q-value = 0.032\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.47104995e-01 4.83164584e-13 8.18243466e-18 6.52895005e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = -0.017\n",
            "Action DOWN: Visits = 3, Q-value = -0.252\n",
            "Action RIGHT: Visits = 1, Q-value = -0.722\n",
            "Action UP: Visits = 49, Q-value = 0.041\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999995e-01 7.45008848e-17 4.29609643e-15 4.50478361e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 82, Q-value = 0.103\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 12, Q-value = -0.109\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.71751272e-10 5.85473302e-17 3.37613408e-15 9.99999999e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = -0.097\n",
            "Action DOWN: Visits = 2, Q-value = -0.621\n",
            "Action RIGHT: Visits = 3, Q-value = -0.770\n",
            "Action UP: Visits = 84, Q-value = 0.073\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.96774332e-01 3.93522846e-18 2.43658974e-07 3.22542456e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = 0.088\n",
            "Action DOWN: Visits = 1, Q-value = -0.621\n",
            "Action RIGHT: Visits = 12, Q-value = -0.163\n",
            "Action UP: Visits = 31, Q-value = 0.035\n",
            "\n",
            "Episode finished!\n",
            "Final state: 0\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action LEFT\n",
            "Step 1: State 0 -> Action UP\n",
            "Step 2: State 0 -> Action LEFT\n",
            "Step 3: State 0 -> Action UP\n",
            "Step 4: State 0 -> Action LEFT\n",
            "Step 5: State 0 -> Action LEFT\n",
            "Step 6: State 0 -> Action UP\n",
            "Step 7: State 0 -> Action LEFT\n",
            "Step 8: State 0 -> Action UP\n",
            "Step 9: State 0 -> Action UP\n",
            "Step 10: State 0 -> Action UP\n",
            "Step 11: State 0 -> Action UP\n",
            "Step 12: State 0 -> Action LEFT\n",
            "Step 13: State 0 -> Action UP\n",
            "Step 14: State 0 -> Action UP\n",
            "Step 15: State 0 -> Action LEFT\n",
            "Step 16: State 0 -> Action LEFT\n",
            "Step 17: State 0 -> Action LEFT\n",
            "Step 18: State 0 -> Action LEFT\n",
            "Step 19: State 0 -> Action DOWN\n",
            "Step 20: State 4 -> Action UP\n",
            "Step 21: State 0 -> Action LEFT\n",
            "Step 22: State 0 -> Action UP\n",
            "Step 23: State 0 -> Action LEFT\n",
            "Step 24: State 0 -> Action LEFT\n",
            "Step 25: State 0 -> Action LEFT\n",
            "Step 26: State 0 -> Action LEFT\n",
            "Step 27: State 0 -> Action UP\n",
            "Step 28: State 0 -> Action LEFT\n",
            "Step 29: State 0 -> Action LEFT\n",
            "Step 30: State 0 -> Action UP\n",
            "Step 31: State 0 -> Action UP\n",
            "Step 32: State 0 -> Action LEFT\n",
            "Step 33: State 0 -> Action UP\n",
            "Step 34: State 0 -> Action LEFT\n",
            "Step 35: State 0 -> Action UP\n",
            "Step 36: State 0 -> Action LEFT\n",
            "Step 37: State 0 -> Action UP\n",
            "Step 38: State 0 -> Action UP\n",
            "Step 39: State 0 -> Action UP\n",
            "Step 40: State 0 -> Action LEFT\n",
            "Step 41: State 0 -> Action LEFT\n",
            "Step 42: State 0 -> Action LEFT\n",
            "Step 43: State 0 -> Action LEFT\n",
            "Step 44: State 0 -> Action UP\n",
            "Step 45: State 0 -> Action LEFT\n",
            "Step 46: State 0 -> Action UP\n",
            "Step 47: State 0 -> Action LEFT\n",
            "Step 48: State 0 -> Action UP\n",
            "Step 49: State 0 -> Action LEFT\n",
            "Step 50: State 0 -> Action UP\n",
            "Step 51: State 0 -> Action LEFT\n",
            "Step 52: State 0 -> Action LEFT\n",
            "Step 53: State 0 -> Action LEFT\n",
            "Step 54: State 0 -> Action UP\n",
            "Step 55: State 0 -> Action UP\n",
            "Step 56: State 0 -> Action LEFT\n",
            "Step 57: State 0 -> Action LEFT\n",
            "Step 58: State 0 -> Action LEFT\n",
            "Step 59: State 0 -> Action LEFT\n",
            "Step 60: State 0 -> Action LEFT\n",
            "Step 61: State 0 -> Action UP\n",
            "Step 62: State 0 -> Action UP\n",
            "Step 63: State 0 -> Action LEFT\n",
            "Step 64: State 0 -> Action LEFT\n",
            "Step 65: State 0 -> Action LEFT\n",
            "Step 66: State 0 -> Action UP\n",
            "Step 67: State 0 -> Action UP\n",
            "Step 68: State 0 -> Action LEFT\n",
            "Step 69: State 0 -> Action LEFT\n",
            "Step 70: State 0 -> Action LEFT\n",
            "Step 71: State 0 -> Action UP\n",
            "Step 72: State 0 -> Action UP\n",
            "Step 73: State 0 -> Action LEFT\n",
            "Step 74: State 0 -> Action UP\n",
            "Step 75: State 0 -> Action UP\n",
            "Step 76: State 0 -> Action LEFT\n",
            "Step 77: State 0 -> Action UP\n",
            "Step 78: State 0 -> Action LEFT\n",
            "Step 79: State 0 -> Action UP\n",
            "Step 80: State 0 -> Action UP\n",
            "Step 81: State 0 -> Action LEFT\n",
            "Step 82: State 0 -> Action UP\n",
            "Step 83: State 0 -> Action LEFT\n",
            "Step 84: State 0 -> Action UP\n",
            "Step 85: State 0 -> Action UP\n",
            "Step 86: State 0 -> Action LEFT\n",
            "Step 87: State 0 -> Action LEFT\n",
            "Step 88: State 0 -> Action LEFT\n",
            "Step 89: State 0 -> Action LEFT\n",
            "Step 90: State 0 -> Action UP\n",
            "Step 91: State 0 -> Action LEFT\n",
            "Step 92: State 0 -> Action UP\n",
            "Step 93: State 0 -> Action UP\n",
            "Step 94: State 0 -> Action UP\n",
            "Step 95: State 0 -> Action UP\n",
            "Step 96: State 0 -> Action UP\n",
            "Step 97: State 0 -> Action LEFT\n",
            "Step 98: State 0 -> Action UP\n",
            "Step 99: State 0 -> Action LEFT\n",
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.56495242e-18 2.67595432e-02 5.97532216e-09 9.73240451e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = -0.482\n",
            "Action DOWN: Visits = 37, Q-value = 0.382\n",
            "Action RIGHT: Visits = 8, Q-value = 0.025\n",
            "Action UP: Visits = 53, Q-value = 0.321\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.77576943e-19 9.99998182e-01 1.04857409e-14 1.81838790e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 1, Q-value = -0.482\n",
            "Action DOWN: Visits = 75, Q-value = 0.349\n",
            "Action RIGHT: Visits = 3, Q-value = 0.114\n",
            "Action UP: Visits = 20, Q-value = 0.187\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.73510375e-06 9.99997265e-01 1.61505141e-11 2.67099975e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.444\n",
            "Action DOWN: Visits = 72, Q-value = 0.430\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98398964e-01 1.15344410e-13 1.18112676e-10 1.60103543e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.446\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.145\n",
            "Action UP: Visits = 31, Q-value = 0.356\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [5.58555937e-01 1.52614101e-12 6.70361352e-07 4.41443392e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 43, Q-value = 0.435\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 11, Q-value = 0.252\n",
            "Action UP: Visits = 42, Q-value = 0.442\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [2.32348907e-02 1.72586598e-10 7.91451661e-01 1.85313448e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.348\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 37, Q-value = 0.346\n",
            "Action UP: Visits = 32, Q-value = 0.444\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [1.00000000e+00 3.68259688e-10 1.03140476e-12 6.38618640e-12]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 79, Q-value = 0.437\n",
            "Action DOWN: Visits = 9, Q-value = 0.090\n",
            "Action RIGHT: Visits = 5, Q-value = -0.079\n",
            "Action UP: Visits = 6, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.96127458e-01 4.61399752e-11 1.64741269e-08 3.87252519e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 54, Q-value = 0.436\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.286\n",
            "Action UP: Visits = 31, Q-value = 0.428\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.90062709e-01 8.47391035e-12 1.36858383e-02 2.96251453e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.417\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 25, Q-value = 0.351\n",
            "Action UP: Visits = 34, Q-value = 0.385\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.39177331e-02 1.20073292e-10 3.39177331e-02 9.32164534e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.342\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 28, Q-value = 0.380\n",
            "Action UP: Visits = 39, Q-value = 0.367\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.66071289e-10 9.94330154e-01 2.81243187e-15 5.66984549e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.322\n",
            "Action DOWN: Visits = 57, Q-value = 0.364\n",
            "Action RIGHT: Visits = 2, Q-value = 0.000\n",
            "Action UP: Visits = 34, Q-value = 0.421\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.44690966e-01 4.14969638e-11 8.55265521e-01 4.35127203e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 36, Q-value = 0.375\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 43, Q-value = 0.381\n",
            "Action UP: Visits = 16, Q-value = 0.315\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.79161045e-01 1.20838955e-01 8.79161045e-11 5.31595065e-13]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 50, Q-value = 0.421\n",
            "Action DOWN: Visits = 41, Q-value = 0.382\n",
            "Action RIGHT: Visits = 5, Q-value = -0.027\n",
            "Action UP: Visits = 3, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.97280733e-05 4.38601802e-14 2.58989978e-09 9.99980269e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.387\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.302\n",
            "Action UP: Visits = 65, Q-value = 0.438\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.19907715e-02 9.78009228e-01 2.76626864e-15 2.70143421e-18]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.429\n",
            "Action DOWN: Visits = 57, Q-value = 0.431\n",
            "Action RIGHT: Visits = 2, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99024186e-01 1.94494928e-13 2.03942713e-07 9.75609557e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.410\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 12, Q-value = 0.315\n",
            "Action UP: Visits = 28, Q-value = 0.370\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999978e-01 1.85750079e-11 3.07196670e-09 1.90208081e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 71, Q-value = 0.411\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.295\n",
            "Action UP: Visits = 12, Q-value = 0.312\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.96225365e-01 7.37240767e-13 7.65478160e-05 3.69808687e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 49, Q-value = 0.407\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 19, Q-value = 0.402\n",
            "Action UP: Visits = 28, Q-value = 0.462\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.69348010e-05 3.76500742e-14 6.84625639e-10 9.99983065e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.321\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 8, Q-value = 0.168\n",
            "Action UP: Visits = 66, Q-value = 0.412\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [9.63111422e-09 9.99999990e-01 1.59281020e-16 5.42361892e-10]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 12, Q-value = 0.479\n",
            "Action DOWN: Visits = 76, Q-value = 0.478\n",
            "Action RIGHT: Visits = 2, Q-value = 0.000\n",
            "Action UP: Visits = 9, Q-value = 0.150\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98371615e-01 2.89163573e-12 1.62838171e-03 2.96103498e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 57, Q-value = 0.350\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 30, Q-value = 0.330\n",
            "Action UP: Visits = 8, Q-value = 0.235\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.23847876e-04 6.04149191e-13 4.23847876e-04 9.99152304e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 23, Q-value = 0.454\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 23, Q-value = 0.369\n",
            "Action UP: Visits = 50, Q-value = 0.505\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [5.52439968e-01 4.47560032e-01 5.02441224e-13 8.50888624e-18]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 48, Q-value = 0.447\n",
            "Action DOWN: Visits = 47, Q-value = 0.389\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.24514025e-06 9.99998755e-01 2.12952222e-13 2.07961154e-16]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.401\n",
            "Action DOWN: Visits = 74, Q-value = 0.383\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 2, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.95634110e-01 1.02486636e-10 2.01769953e-01 2.59593690e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.379\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 34, Q-value = 0.399\n",
            "Action UP: Visits = 22, Q-value = 0.360\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.43996889e-01 6.33471513e-13 6.48674829e-10 8.56003110e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 41, Q-value = 0.411\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.119\n",
            "Action UP: Visits = 49, Q-value = 0.455\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [5.99165447e-04 9.99400835e-01 5.99165447e-14 5.99165447e-14]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.409\n",
            "Action DOWN: Visits = 63, Q-value = 0.373\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 3, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.26479912e-02 8.81554395e-09 8.01769051e-11 9.77352000e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.420\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 5, Q-value = 0.174\n",
            "Action UP: Visits = 51, Q-value = 0.442\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [7.56197578e-03 9.92438024e-01 1.65755269e-10 2.74128910e-18]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.484\n",
            "Action DOWN: Visits = 57, Q-value = 0.434\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.91676238e-01 6.14019788e-10 6.28756263e-07 8.32313238e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 50, Q-value = 0.425\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 12, Q-value = 0.333\n",
            "Action UP: Visits = 31, Q-value = 0.412\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99984061e-01 2.05150639e-12 1.19952408e-05 3.94422948e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.420\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 19, Q-value = 0.351\n",
            "Action UP: Visits = 17, Q-value = 0.301\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99741793e-01 2.33063090e-13 7.95701481e-06 2.50249587e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = 0.405\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 17, Q-value = 0.314\n",
            "Action UP: Visits = 24, Q-value = 0.379\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [5.33093240e-02 2.10463254e-09 9.46652403e-01 3.82704531e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.377\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 44, Q-value = 0.390\n",
            "Action UP: Visits = 16, Q-value = 0.360\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.94361019e-01 5.63803273e-03 9.48296565e-07 4.06184364e-13]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 52, Q-value = 0.349\n",
            "Action DOWN: Visits = 31, Q-value = 0.261\n",
            "Action RIGHT: Visits = 13, Q-value = 0.200\n",
            "Action UP: Visits = 3, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.58376491e-01 3.91485123e-13 4.00880766e-10 4.16235082e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 52, Q-value = 0.434\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.221\n",
            "Action UP: Visits = 38, Q-value = 0.388\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.48017972e-01 2.95505969e-09 1.27305214e-01 6.24676811e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 31, Q-value = 0.393\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 29, Q-value = 0.385\n",
            "Action UP: Visits = 34, Q-value = 0.436\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [3.57994558e-07 9.99999642e-01 1.07374144e-11 1.77577203e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = 0.492\n",
            "Action DOWN: Visits = 75, Q-value = 0.426\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.46486645e-01 2.05494533e-12 2.13365273e-04 5.32999896e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.388\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 19, Q-value = 0.288\n",
            "Action UP: Visits = 33, Q-value = 0.412\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99827981e-01 9.01827336e-09 8.60049569e-05 8.60049569e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.347\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 20, Q-value = 0.275\n",
            "Action UP: Visits = 20, Q-value = 0.319\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.20645176e-01 1.28152332e-12 6.27760986e-06 7.93485460e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = 0.462\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 14, Q-value = 0.469\n",
            "Action UP: Visits = 36, Q-value = 0.426\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.82953862e-01 4.87577796e-13 2.14169796e-07 1.70459238e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.419\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 11, Q-value = 0.354\n",
            "Action UP: Visits = 34, Q-value = 0.363\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.97406058e-01 1.98835230e-11 7.85546291e-04 1.80839588e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 47, Q-value = 0.437\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 23, Q-value = 0.383\n",
            "Action UP: Visits = 25, Q-value = 0.400\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.86675838e-01 2.42266221e-12 1.46489120e-04 1.13177673e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 43, Q-value = 0.440\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 18, Q-value = 0.314\n",
            "Action UP: Visits = 35, Q-value = 0.377\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.90629507e-01 4.91385171e-13 1.14721026e-06 9.36934610e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.392\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 13, Q-value = 0.329\n",
            "Action UP: Visits = 32, Q-value = 0.357\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.47193127e-01 1.62935434e-10 2.37987982e-01 1.48188914e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.369\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 33, Q-value = 0.331\n",
            "Action UP: Visits = 25, Q-value = 0.365\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.37299363e-01 5.75420501e-11 9.15331409e-04 6.17853059e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 42, Q-value = 0.366\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 21, Q-value = 0.350\n",
            "Action UP: Visits = 32, Q-value = 0.367\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [2.30460895e-02 8.88526514e-13 9.76945234e-01 8.67701674e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.415\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 48, Q-value = 0.413\n",
            "Action UP: Visits = 15, Q-value = 0.392\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99955598e-01 4.78353074e-07 4.39233060e-05 9.76519139e-14]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 60, Q-value = 0.439\n",
            "Action DOWN: Visits = 14, Q-value = 0.234\n",
            "Action RIGHT: Visits = 22, Q-value = 0.289\n",
            "Action UP: Visits = 3, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99917289e-01 9.09419476e-13 5.24417096e-11 8.27112195e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 64, Q-value = 0.442\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.145\n",
            "Action UP: Visits = 25, Q-value = 0.409\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.69343220e-05 1.63096391e-13 9.99954782e-01 2.82834095e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.421\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 57, Q-value = 0.468\n",
            "Action UP: Visits = 20, Q-value = 0.451\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99320684e-01 9.53026470e-07 6.78363150e-04 3.37384062e-15]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.396\n",
            "Action DOWN: Visits = 14, Q-value = 0.147\n",
            "Action RIGHT: Visits = 27, Q-value = 0.291\n",
            "Action UP: Visits = 2, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.08019758e-01 4.45283590e-12 1.25781593e-03 7.90722426e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.460\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 21, Q-value = 0.386\n",
            "Action UP: Visits = 40, Q-value = 0.442\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.08873680e-06 9.99998911e-01 1.86203057e-13 1.77577073e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.311\n",
            "Action DOWN: Visits = 75, Q-value = 0.353\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.43161401e-01 6.97971623e-13 1.18202107e-07 5.68384805e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 49, Q-value = 0.434\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.236\n",
            "Action UP: Visits = 37, Q-value = 0.385\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99420820e-01 1.15462465e-13 6.81794307e-09 5.79173376e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.409\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.302\n",
            "Action UP: Visits = 28, Q-value = 0.394\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.65129487e-01 2.38565249e-11 3.48454973e-02 2.50153794e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 46, Q-value = 0.417\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 33, Q-value = 0.425\n",
            "Action UP: Visits = 16, Q-value = 0.424\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.48825040e-02 7.57946025e-11 9.70234992e-01 1.48825040e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.377\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 41, Q-value = 0.451\n",
            "Action UP: Visits = 27, Q-value = 0.463\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999275e-01 7.25116085e-07 1.98327314e-12 2.07961262e-16]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 74, Q-value = 0.327\n",
            "Action DOWN: Visits = 18, Q-value = 0.217\n",
            "Action RIGHT: Visits = 5, Q-value = -0.079\n",
            "Action UP: Visits = 2, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.08164606e-02 4.21671246e-09 1.49277237e-07 9.69183386e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 34, Q-value = 0.399\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.278\n",
            "Action UP: Visits = 48, Q-value = 0.427\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.18798581e-01 1.81201419e-01 8.79178282e-12 8.58572541e-15]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 50, Q-value = 0.463\n",
            "Action DOWN: Visits = 43, Q-value = 0.348\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 2, Q-value = -0.482\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [9.53673407e-07 9.99999046e-01 9.18493760e-15 1.55547725e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.416\n",
            "Action DOWN: Visits = 76, Q-value = 0.419\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99684719e-01 3.45607133e-12 9.53373641e-07 3.14327856e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.412\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 14, Q-value = 0.288\n",
            "Action UP: Visits = 25, Q-value = 0.399\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99944068e-01 1.63094644e-13 9.86170944e-06 4.60702001e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 57, Q-value = 0.455\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 18, Q-value = 0.265\n",
            "Action UP: Visits = 21, Q-value = 0.392\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999531e-01 1.37399054e-14 1.40696631e-11 4.69094573e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 73, Q-value = 0.496\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.221\n",
            "Action UP: Visits = 17, Q-value = 0.402\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99950643e-01 1.27926801e-09 7.20400777e-11 4.93560084e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 62, Q-value = 0.402\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.145\n",
            "Action UP: Visits = 23, Q-value = 0.486\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [4.38387121e-04 4.08279822e-13 9.99490811e-01 7.08019674e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = 0.400\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 52, Q-value = 0.438\n",
            "Action UP: Visits = 20, Q-value = 0.424\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99964069e-01 3.59304797e-05 9.09462022e-13 9.31289111e-10]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 64, Q-value = 0.340\n",
            "Action DOWN: Visits = 23, Q-value = 0.240\n",
            "Action RIGHT: Visits = 4, Q-value = -0.099\n",
            "Action UP: Visits = 8, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.17258646e-01 2.20877037e-11 7.34472853e-08 2.82741281e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 45, Q-value = 0.433\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.305\n",
            "Action UP: Visits = 41, Q-value = 0.390\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.39667226e-03 5.95736528e-12 5.68138626e-08 9.93603271e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.415\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.256\n",
            "Action UP: Visits = 53, Q-value = 0.434\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.12938122e-04 9.99887062e-01 5.75178753e-13 5.48533204e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.411\n",
            "Action DOWN: Visits = 67, Q-value = 0.428\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.71172464e-01 8.09391095e-11 4.73254245e-04 6.28354282e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.395\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 19, Q-value = 0.405\n",
            "Action UP: Visits = 39, Q-value = 0.425\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [7.27547706e-10 9.99999999e-01 4.39920277e-12 7.27547706e-20]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = 0.296\n",
            "Action DOWN: Visits = 82, Q-value = 0.380\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99997493e-01 2.41390709e-14 4.38942744e-10 2.50636325e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 69, Q-value = 0.463\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 8, Q-value = 0.267\n",
            "Action UP: Visits = 19, Q-value = 0.395\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99995672e-01 3.36559323e-10 7.37723615e-08 4.25408611e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 62, Q-value = 0.400\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 12, Q-value = 0.254\n",
            "Action UP: Visits = 18, Q-value = 0.344\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99024382e-01 1.36933182e-13 9.75609748e-04 8.08576745e-09]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 58, Q-value = 0.396\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 29, Q-value = 0.357\n",
            "Action UP: Visits = 9, Q-value = 0.304\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.88626899e-01 2.19795179e-09 1.07667833e-02 6.06315409e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.390\n",
            "Action DOWN: Visits = 6, Q-value = 0.000\n",
            "Action RIGHT: Visits = 28, Q-value = 0.403\n",
            "Action UP: Visits = 21, Q-value = 0.377\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.93161390e-01 8.14738258e-11 6.83809303e-03 5.16571602e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.442\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 31, Q-value = 0.384\n",
            "Action UP: Visits = 12, Q-value = 0.377\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.55283377e-02 5.49723834e-11 3.40374509e-10 9.84471662e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.430\n",
            "Action DOWN: Visits = 5, Q-value = 0.000\n",
            "Action RIGHT: Visits = 6, Q-value = 0.184\n",
            "Action UP: Visits = 53, Q-value = 0.457\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.81838790e-06 9.99998182e-01 1.04857409e-14 1.77576943e-19]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.382\n",
            "Action DOWN: Visits = 75, Q-value = 0.451\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.50240296e-01 3.35481874e-08 3.12441843e-07 8.49759358e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.388\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 10, Q-value = 0.237\n",
            "Action UP: Visits = 44, Q-value = 0.440\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [7.12795394e-02 9.28720461e-01 5.56834623e-12 5.31038878e-18]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 41, Q-value = 0.436\n",
            "Action DOWN: Visits = 53, Q-value = 0.393\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 1, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99741793e-01 2.33063090e-13 7.95701481e-06 2.50249587e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = 0.409\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 17, Q-value = 0.366\n",
            "Action UP: Visits = 24, Q-value = 0.300\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.01462570e-01 1.01462570e-11 8.26248251e-01 7.22891788e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.410\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 37, Q-value = 0.421\n",
            "Action UP: Visits = 29, Q-value = 0.446\n",
            "\n",
            "Current state: 9\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99998027e-01 1.42291176e-08 1.95873718e-06 3.23939318e-14]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 67, Q-value = 0.391\n",
            "Action DOWN: Visits = 11, Q-value = 0.109\n",
            "Action RIGHT: Visits = 18, Q-value = 0.183\n",
            "Action UP: Visits = 3, Q-value = 0.000\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.76916754e-01 3.04293515e-08 1.79682278e-03 1.21286393e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.415\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 21, Q-value = 0.358\n",
            "Action UP: Visits = 32, Q-value = 0.429\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98056901e-01 1.94306612e-13 1.94306612e-03 3.29059953e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 56, Q-value = 0.444\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 30, Q-value = 0.482\n",
            "Action UP: Visits = 10, Q-value = 0.354\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.69555146e-01 1.45369111e-12 3.30444216e-01 6.38537546e-07]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.376\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 41, Q-value = 0.354\n",
            "Action UP: Visits = 11, Q-value = 0.343\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.12777100e-02 2.10221857e-10 2.25724000e-01 7.32998290e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.359\n",
            "Action DOWN: Visits = 4, Q-value = 0.000\n",
            "Action RIGHT: Visits = 32, Q-value = 0.346\n",
            "Action UP: Visits = 36, Q-value = 0.416\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.14182124e-03 9.98858179e-01 1.03848037e-15 1.03848037e-15]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.439\n",
            "Action DOWN: Visits = 63, Q-value = 0.401\n",
            "Action RIGHT: Visits = 2, Q-value = 0.000\n",
            "Action UP: Visits = 2, Q-value = -0.482\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98831977e-01 1.11389933e-09 1.37496181e-08 1.16800810e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 55, Q-value = 0.395\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.238\n",
            "Action UP: Visits = 28, Q-value = 0.395\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99152304e-01 6.04149191e-13 4.23847876e-04 4.23847876e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 50, Q-value = 0.413\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 23, Q-value = 0.405\n",
            "Action UP: Visits = 23, Q-value = 0.352\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99420820e-01 1.15462465e-13 6.81794307e-09 5.79173376e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 59, Q-value = 0.426\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.336\n",
            "Action UP: Visits = 28, Q-value = 0.478\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.90984924e-01 1.63890788e-08 9.01295538e-03 2.10420572e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 48, Q-value = 0.418\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 30, Q-value = 0.376\n",
            "Action UP: Visits = 13, Q-value = 0.315\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.69637052e-01 1.67098192e-12 2.30359047e-01 3.90115069e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 44, Q-value = 0.419\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 39, Q-value = 0.376\n",
            "Action UP: Visits = 13, Q-value = 0.339\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.84194841e-01 2.68911314e-12 6.42880138e-03 9.37635776e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 43, Q-value = 0.425\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 26, Q-value = 0.305\n",
            "Action UP: Visits = 27, Q-value = 0.419\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98474272e-01 7.38905034e-13 3.32355513e-04 1.19337288e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 49, Q-value = 0.383\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 22, Q-value = 0.254\n",
            "Action UP: Visits = 25, Q-value = 0.385\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.98439118e-01 4.34399798e-09 1.46659196e-03 9.42855685e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 48, Q-value = 0.414\n",
            "Action DOWN: Visits = 7, Q-value = 0.000\n",
            "Action RIGHT: Visits = 25, Q-value = 0.352\n",
            "Action UP: Visits = 19, Q-value = 0.371\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.96583590e-01 9.06387495e-13 3.16037778e-03 2.56032033e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 48, Q-value = 0.432\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 27, Q-value = 0.364\n",
            "Action UP: Visits = 21, Q-value = 0.414\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99983002e-01 5.99514471e-14 6.28636486e-08 1.69348000e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 63, Q-value = 0.425\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 12, Q-value = 0.254\n",
            "Action UP: Visits = 21, Q-value = 0.408\n",
            "\n",
            "Current state: 8\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99999939e-01 1.81396554e-14 1.90208073e-08 4.23496679e-08]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 71, Q-value = 0.440\n",
            "Action DOWN: Visits = 3, Q-value = 0.000\n",
            "Action RIGHT: Visits = 12, Q-value = 0.322\n",
            "Action UP: Visits = 13, Q-value = 0.310\n",
            "\n",
            "Episode finished!\n",
            "Final state: 8\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action UP\n",
            "Step 1: State 0 -> Action DOWN\n",
            "Step 2: State 4 -> Action DOWN\n",
            "Step 3: State 8 -> Action LEFT\n",
            "Step 4: State 8 -> Action LEFT\n",
            "Step 5: State 8 -> Action RIGHT\n",
            "Step 6: State 9 -> Action LEFT\n",
            "Step 7: State 8 -> Action LEFT\n",
            "Step 8: State 8 -> Action LEFT\n",
            "Step 9: State 8 -> Action UP\n",
            "Step 10: State 4 -> Action DOWN\n",
            "Step 11: State 8 -> Action RIGHT\n",
            "Step 12: State 9 -> Action LEFT\n",
            "Step 13: State 8 -> Action UP\n",
            "Step 14: State 4 -> Action DOWN\n",
            "Step 15: State 8 -> Action LEFT\n",
            "Step 16: State 8 -> Action LEFT\n",
            "Step 17: State 8 -> Action LEFT\n",
            "Step 18: State 8 -> Action UP\n",
            "Step 19: State 4 -> Action DOWN\n",
            "Step 20: State 8 -> Action LEFT\n",
            "Step 21: State 8 -> Action UP\n",
            "Step 22: State 4 -> Action LEFT\n",
            "Step 23: State 4 -> Action DOWN\n",
            "Step 24: State 8 -> Action LEFT\n",
            "Step 25: State 8 -> Action UP\n",
            "Step 26: State 4 -> Action DOWN\n",
            "Step 27: State 8 -> Action UP\n",
            "Step 28: State 4 -> Action DOWN\n",
            "Step 29: State 8 -> Action LEFT\n",
            "Step 30: State 8 -> Action LEFT\n",
            "Step 31: State 8 -> Action LEFT\n",
            "Step 32: State 8 -> Action RIGHT\n",
            "Step 33: State 9 -> Action LEFT\n",
            "Step 34: State 8 -> Action LEFT\n",
            "Step 35: State 8 -> Action UP\n",
            "Step 36: State 4 -> Action DOWN\n",
            "Step 37: State 8 -> Action LEFT\n",
            "Step 38: State 8 -> Action LEFT\n",
            "Step 39: State 8 -> Action LEFT\n",
            "Step 40: State 8 -> Action LEFT\n",
            "Step 41: State 8 -> Action LEFT\n",
            "Step 42: State 8 -> Action LEFT\n",
            "Step 43: State 8 -> Action LEFT\n",
            "Step 44: State 8 -> Action LEFT\n",
            "Step 45: State 8 -> Action LEFT\n",
            "Step 46: State 8 -> Action RIGHT\n",
            "Step 47: State 9 -> Action LEFT\n",
            "Step 48: State 8 -> Action LEFT\n",
            "Step 49: State 8 -> Action RIGHT\n",
            "Step 50: State 9 -> Action LEFT\n",
            "Step 51: State 8 -> Action UP\n",
            "Step 52: State 4 -> Action DOWN\n",
            "Step 53: State 8 -> Action LEFT\n",
            "Step 54: State 8 -> Action LEFT\n",
            "Step 55: State 8 -> Action LEFT\n",
            "Step 56: State 8 -> Action RIGHT\n",
            "Step 57: State 9 -> Action LEFT\n",
            "Step 58: State 8 -> Action UP\n",
            "Step 59: State 4 -> Action LEFT\n",
            "Step 60: State 4 -> Action DOWN\n",
            "Step 61: State 8 -> Action LEFT\n",
            "Step 62: State 8 -> Action LEFT\n",
            "Step 63: State 8 -> Action LEFT\n",
            "Step 64: State 8 -> Action LEFT\n",
            "Step 65: State 8 -> Action RIGHT\n",
            "Step 66: State 9 -> Action LEFT\n",
            "Step 67: State 8 -> Action LEFT\n",
            "Step 68: State 8 -> Action UP\n",
            "Step 69: State 4 -> Action DOWN\n",
            "Step 70: State 8 -> Action UP\n",
            "Step 71: State 4 -> Action DOWN\n",
            "Step 72: State 8 -> Action LEFT\n",
            "Step 73: State 8 -> Action LEFT\n",
            "Step 74: State 8 -> Action LEFT\n",
            "Step 75: State 8 -> Action LEFT\n",
            "Step 76: State 8 -> Action LEFT\n",
            "Step 77: State 8 -> Action UP\n",
            "Step 78: State 4 -> Action DOWN\n",
            "Step 79: State 8 -> Action UP\n",
            "Step 80: State 4 -> Action DOWN\n",
            "Step 81: State 8 -> Action LEFT\n",
            "Step 82: State 8 -> Action RIGHT\n",
            "Step 83: State 9 -> Action LEFT\n",
            "Step 84: State 8 -> Action LEFT\n",
            "Step 85: State 8 -> Action LEFT\n",
            "Step 86: State 8 -> Action LEFT\n",
            "Step 87: State 8 -> Action UP\n",
            "Step 88: State 4 -> Action DOWN\n",
            "Step 89: State 8 -> Action LEFT\n",
            "Step 90: State 8 -> Action LEFT\n",
            "Step 91: State 8 -> Action LEFT\n",
            "Step 92: State 8 -> Action LEFT\n",
            "Step 93: State 8 -> Action LEFT\n",
            "Step 94: State 8 -> Action LEFT\n",
            "Step 95: State 8 -> Action LEFT\n",
            "Step 96: State 8 -> Action LEFT\n",
            "Step 97: State 8 -> Action LEFT\n",
            "Step 98: State 8 -> Action LEFT\n",
            "Step 99: State 8 -> Action LEFT\n",
            "Starting MCTS test...\n",
            "Initial state: 0\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.40106046e-01 1.73137400e-02 2.58776593e-05 3.42554336e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.304\n",
            "Action DOWN: Visits = 23, Q-value = 0.287\n",
            "Action RIGHT: Visits = 12, Q-value = 0.188\n",
            "Action UP: Visits = 31, Q-value = 0.305\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.46438473e-07 7.51282333e-03 7.35622066e-05 9.92412668e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 11, Q-value = 0.256\n",
            "Action DOWN: Visits = 27, Q-value = 0.265\n",
            "Action RIGHT: Visits = 17, Q-value = 0.244\n",
            "Action UP: Visits = 44, Q-value = 0.300\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.26544362e-01 3.21775463e-01 4.73571333e-05 4.51632818e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.356\n",
            "Action DOWN: Visits = 29, Q-value = 0.266\n",
            "Action RIGHT: Visits = 12, Q-value = 0.176\n",
            "Action UP: Visits = 30, Q-value = 0.289\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.37027890e-01 5.62941151e-01 5.24279802e-10 3.09581980e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.341\n",
            "Action DOWN: Visits = 40, Q-value = 0.277\n",
            "Action RIGHT: Visits = 5, Q-value = 0.075\n",
            "Action UP: Visits = 15, Q-value = 0.221\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.40696693e-11 3.20778246e-08 6.57282069e-11 9.99999968e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.050\n",
            "Action DOWN: Visits = 13, Q-value = 0.069\n",
            "Action RIGHT: Visits = 7, Q-value = 0.000\n",
            "Action UP: Visits = 73, Q-value = 0.194\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.55887131e-04 8.19426027e-03 3.59408823e-06 9.91446259e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.297\n",
            "Action DOWN: Visits = 26, Q-value = 0.229\n",
            "Action RIGHT: Visits = 12, Q-value = 0.253\n",
            "Action UP: Visits = 42, Q-value = 0.308\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.40093279e-04 2.92852846e-08 2.99881314e-05 9.99829889e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.305\n",
            "Action DOWN: Visits = 9, Q-value = 0.118\n",
            "Action RIGHT: Visits = 18, Q-value = 0.197\n",
            "Action UP: Visits = 51, Q-value = 0.291\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [4.20626986e-08 9.34873175e-01 2.10208470e-02 4.41059363e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.209\n",
            "Action DOWN: Visits = 38, Q-value = 0.280\n",
            "Action RIGHT: Visits = 26, Q-value = 0.188\n",
            "Action UP: Visits = 28, Q-value = 0.250\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.54023702e-07 9.31322430e-10 1.57720271e-14 9.99999845e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.251\n",
            "Action DOWN: Visits = 9, Q-value = -0.046\n",
            "Action RIGHT: Visits = 3, Q-value = 0.000\n",
            "Action UP: Visits = 72, Q-value = 0.312\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99989593e-01 1.01528971e-08 6.13906865e-11 1.03965667e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 63, Q-value = 0.316\n",
            "Action DOWN: Visits = 10, Q-value = 0.140\n",
            "Action RIGHT: Visits = 6, Q-value = 0.130\n",
            "Action UP: Visits = 20, Q-value = 0.233\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.09519728e-05 3.16108929e-03 4.33690113e-09 9.96807954e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = 0.217\n",
            "Action DOWN: Visits = 27, Q-value = 0.198\n",
            "Action RIGHT: Visits = 7, Q-value = 0.111\n",
            "Action UP: Visits = 48, Q-value = 0.277\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.01395593e-05 4.28953211e-08 5.76644516e-09 9.99989812e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.268\n",
            "Action DOWN: Visits = 11, Q-value = 0.091\n",
            "Action RIGHT: Visits = 9, Q-value = 0.169\n",
            "Action UP: Visits = 60, Q-value = 0.327\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.56832227e-10 7.67402346e-05 1.19367803e-03 9.98729581e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.153\n",
            "Action DOWN: Visits = 19, Q-value = 0.223\n",
            "Action RIGHT: Visits = 25, Q-value = 0.260\n",
            "Action UP: Visits = 49, Q-value = 0.317\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.48374220e-03 6.26460292e-02 3.86225035e-06 9.34866366e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.269\n",
            "Action DOWN: Visits = 29, Q-value = 0.259\n",
            "Action RIGHT: Visits = 11, Q-value = 0.132\n",
            "Action UP: Visits = 38, Q-value = 0.261\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.71882592e-04 6.27611081e-05 1.09914367e-08 9.99665345e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.317\n",
            "Action DOWN: Visits = 19, Q-value = 0.135\n",
            "Action RIGHT: Visits = 8, Q-value = 0.121\n",
            "Action UP: Visits = 50, Q-value = 0.313\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.23710952e-03 4.21007862e-01 5.15494393e-05 5.72703479e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.280\n",
            "Action DOWN: Visits = 32, Q-value = 0.278\n",
            "Action RIGHT: Visits = 13, Q-value = 0.250\n",
            "Action UP: Visits = 33, Q-value = 0.335\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.46653339e-01 3.27489961e-04 2.59091605e-08 2.53019145e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.253\n",
            "Action DOWN: Visits = 18, Q-value = 0.197\n",
            "Action RIGHT: Visits = 7, Q-value = 0.054\n",
            "Action UP: Visits = 35, Q-value = 0.273\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.46214572e-02 4.20219653e-03 7.28725168e-05 9.21103474e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.255\n",
            "Action DOWN: Visits = 21, Q-value = 0.258\n",
            "Action RIGHT: Visits = 14, Q-value = 0.205\n",
            "Action UP: Visits = 36, Q-value = 0.296\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.27934503 0.58612168 0.05255745 0.08197583]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.348\n",
            "Action DOWN: Visits = 28, Q-value = 0.250\n",
            "Action RIGHT: Visits = 22, Q-value = 0.275\n",
            "Action UP: Visits = 23, Q-value = 0.294\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.33821480e-05 1.36896234e-11 8.67968407e-08 9.99976531e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.193\n",
            "Action DOWN: Visits = 5, Q-value = -0.092\n",
            "Action RIGHT: Visits = 12, Q-value = 0.000\n",
            "Action UP: Visits = 61, Q-value = 0.231\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [0.54893551 0.39110085 0.0589415  0.00102214]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.284\n",
            "Action DOWN: Visits = 29, Q-value = 0.237\n",
            "Action RIGHT: Visits = 24, Q-value = 0.165\n",
            "Action UP: Visits = 16, Q-value = 0.174\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.17165079e-01 1.53492275e-02 9.61025729e-06 5.67476083e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.295\n",
            "Action DOWN: Visits = 23, Q-value = 0.257\n",
            "Action RIGHT: Visits = 11, Q-value = 0.207\n",
            "Action UP: Visits = 33, Q-value = 0.264\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [0.49463518 0.01006609 0.00066354 0.49463518]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 31, Q-value = 0.296\n",
            "Action DOWN: Visits = 21, Q-value = 0.250\n",
            "Action RIGHT: Visits = 16, Q-value = 0.204\n",
            "Action UP: Visits = 31, Q-value = 0.242\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.55663106e-04 6.28553933e-01 4.66883990e-09 3.71290399e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 17, Q-value = 0.277\n",
            "Action DOWN: Visits = 39, Q-value = 0.270\n",
            "Action RIGHT: Visits = 6, Q-value = 0.130\n",
            "Action UP: Visits = 37, Q-value = 0.321\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.54493162e-11 2.93680246e-07 2.80075307e-13 9.99999706e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.096\n",
            "Action DOWN: Visits = 16, Q-value = 0.044\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 72, Q-value = 0.262\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.04249399e-05 1.40953050e-05 2.38705228e-10 9.99945480e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.274\n",
            "Action DOWN: Visits = 18, Q-value = 0.247\n",
            "Action RIGHT: Visits = 6, Q-value = 0.130\n",
            "Action UP: Visits = 55, Q-value = 0.316\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.71898471e-04 2.96114936e-06 1.41128095e-06 9.99723729e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.320\n",
            "Action DOWN: Visits = 14, Q-value = 0.211\n",
            "Action RIGHT: Visits = 13, Q-value = 0.233\n",
            "Action UP: Visits = 50, Q-value = 0.309\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.62111309e-01 2.26440915e-01 2.76272694e-06 1.14450139e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.271\n",
            "Action DOWN: Visits = 31, Q-value = 0.236\n",
            "Action RIGHT: Visits = 10, Q-value = 0.137\n",
            "Action UP: Visits = 23, Q-value = 0.238\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.25094447 0.05371675 0.00093153 0.69440725]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.310\n",
            "Action DOWN: Visits = 24, Q-value = 0.297\n",
            "Action RIGHT: Visits = 16, Q-value = 0.148\n",
            "Action UP: Visits = 31, Q-value = 0.290\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.84236568e-06 3.47875107e-04 8.39740293e-08 9.99647199e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.295\n",
            "Action DOWN: Visits = 23, Q-value = 0.219\n",
            "Action RIGHT: Visits = 10, Q-value = 0.171\n",
            "Action UP: Visits = 51, Q-value = 0.310\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.50888608e-02 1.22239554e-03 7.32856275e-07 9.83688011e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.296\n",
            "Action DOWN: Visits = 21, Q-value = 0.222\n",
            "Action RIGHT: Visits = 10, Q-value = 0.151\n",
            "Action UP: Visits = 41, Q-value = 0.265\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.45222586e-01 5.71539952e-03 6.90784940e-04 4.83712295e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.291\n",
            "Action DOWN: Visits = 21, Q-value = 0.262\n",
            "Action RIGHT: Visits = 17, Q-value = 0.240\n",
            "Action UP: Visits = 26, Q-value = 0.251\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.43554548e-05 8.71912338e-02 1.19306486e-08 9.12784399e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.266\n",
            "Action DOWN: Visits = 34, Q-value = 0.221\n",
            "Action RIGHT: Visits = 7, Q-value = 0.054\n",
            "Action UP: Visits = 43, Q-value = 0.262\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.92386503e-02 5.78561352e-06 9.56834697e-04 9.79798729e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.303\n",
            "Action DOWN: Visits = 12, Q-value = 0.154\n",
            "Action RIGHT: Visits = 20, Q-value = 0.225\n",
            "Action UP: Visits = 40, Q-value = 0.310\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99655998e-01 1.04821529e-04 1.79995417e-04 5.91854088e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 45, Q-value = 0.311\n",
            "Action DOWN: Visits = 18, Q-value = 0.239\n",
            "Action RIGHT: Visits = 19, Q-value = 0.292\n",
            "Action UP: Visits = 17, Q-value = 0.248\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.93758429e-01 5.05723477e-03 4.35872838e-04 7.48463738e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 39, Q-value = 0.319\n",
            "Action DOWN: Visits = 23, Q-value = 0.187\n",
            "Action RIGHT: Visits = 18, Q-value = 0.266\n",
            "Action UP: Visits = 19, Q-value = 0.228\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.71027239e-01 1.88395389e-01 2.64675038e-04 5.40312698e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.331\n",
            "Action DOWN: Visits = 27, Q-value = 0.270\n",
            "Action RIGHT: Visits = 14, Q-value = 0.246\n",
            "Action UP: Visits = 30, Q-value = 0.317\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [0.60060802 0.00111835 0.09700155 0.30127208]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.308\n",
            "Action DOWN: Visits = 16, Q-value = 0.198\n",
            "Action RIGHT: Visits = 25, Q-value = 0.205\n",
            "Action UP: Visits = 28, Q-value = 0.234\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [7.73858949e-02 9.22572688e-01 1.78274357e-06 3.96346125e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.325\n",
            "Action DOWN: Visits = 41, Q-value = 0.300\n",
            "Action RIGHT: Visits = 11, Q-value = 0.222\n",
            "Action UP: Visits = 15, Q-value = 0.252\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.54012588e-09 2.04683495e-06 1.01529819e-08 9.99997939e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 9, Q-value = 0.181\n",
            "Action DOWN: Visits = 17, Q-value = 0.192\n",
            "Action RIGHT: Visits = 10, Q-value = 0.000\n",
            "Action UP: Visits = 63, Q-value = 0.264\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.29059379e-05 9.17469904e-07 2.53088418e-01 7.46857759e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.233\n",
            "Action DOWN: Visits = 10, Q-value = 0.049\n",
            "Action RIGHT: Visits = 35, Q-value = 0.278\n",
            "Action UP: Visits = 39, Q-value = 0.297\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.59110374e-05 2.41315340e-04 4.75088139e-05 9.99685265e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 16, Q-value = 0.294\n",
            "Action DOWN: Visits = 20, Q-value = 0.213\n",
            "Action RIGHT: Visits = 17, Q-value = 0.283\n",
            "Action UP: Visits = 46, Q-value = 0.367\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.30074637e-02 2.66290879e-01 1.16968951e-05 6.90689960e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = 0.262\n",
            "Action DOWN: Visits = 30, Q-value = 0.262\n",
            "Action RIGHT: Visits = 11, Q-value = 0.202\n",
            "Action UP: Visits = 33, Q-value = 0.289\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.53862147e-01 5.26859945e-03 2.73465092e-05 4.08419073e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 37, Q-value = 0.312\n",
            "Action DOWN: Visits = 22, Q-value = 0.154\n",
            "Action RIGHT: Visits = 13, Q-value = 0.204\n",
            "Action UP: Visits = 27, Q-value = 0.273\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.69993815e-05 4.90954380e-01 1.80642409e-02 4.90954380e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 12, Q-value = 0.250\n",
            "Action DOWN: Visits = 32, Q-value = 0.246\n",
            "Action RIGHT: Visits = 23, Q-value = 0.172\n",
            "Action UP: Visits = 32, Q-value = 0.270\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.75609724e-04 3.29378924e-08 3.21659106e-11 9.99024357e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 28, Q-value = 0.232\n",
            "Action DOWN: Visits = 10, Q-value = 0.092\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 56, Q-value = 0.264\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.36118287e-09 4.93584793e-03 2.16808386e-07 9.95063933e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 7, Q-value = 0.185\n",
            "Action DOWN: Visits = 30, Q-value = 0.279\n",
            "Action RIGHT: Visits = 11, Q-value = 0.135\n",
            "Action UP: Visits = 51, Q-value = 0.298\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.80111848e-07 9.67619658e-04 8.18927047e-03 9.90842530e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = 0.222\n",
            "Action DOWN: Visits = 21, Q-value = 0.251\n",
            "Action RIGHT: Visits = 26, Q-value = 0.228\n",
            "Action UP: Visits = 42, Q-value = 0.295\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.13167136 0.61511911 0.05515764 0.19805189]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 24, Q-value = 0.257\n",
            "Action DOWN: Visits = 28, Q-value = 0.237\n",
            "Action RIGHT: Visits = 22, Q-value = 0.252\n",
            "Action UP: Visits = 25, Q-value = 0.227\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.87060921e-07 3.44637882e-07 1.27932983e-09 9.99998967e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.212\n",
            "Action DOWN: Visits = 14, Q-value = 0.151\n",
            "Action RIGHT: Visits = 8, Q-value = 0.000\n",
            "Action UP: Visits = 62, Q-value = 0.289\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [7.48743192e-07 1.70458469e-02 3.97960125e-06 9.82949425e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 11, Q-value = 0.145\n",
            "Action DOWN: Visits = 30, Q-value = 0.231\n",
            "Action RIGHT: Visits = 13, Q-value = 0.221\n",
            "Action UP: Visits = 45, Q-value = 0.300\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.40342997e-10 9.99903705e-01 1.43711229e-07 9.61516191e-05]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.112\n",
            "Action DOWN: Visits = 58, Q-value = 0.240\n",
            "Action RIGHT: Visits = 12, Q-value = 0.173\n",
            "Action UP: Visits = 23, Q-value = 0.218\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.67682530e-07 1.84433905e-07 6.68590680e-13 9.99999448e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.162\n",
            "Action DOWN: Visits = 14, Q-value = 0.106\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 66, Q-value = 0.266\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.85083357e-01 5.95642237e-03 9.39448697e-07 8.95928094e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 40, Q-value = 0.268\n",
            "Action DOWN: Visits = 24, Q-value = 0.137\n",
            "Action RIGHT: Visits = 10, Q-value = 0.137\n",
            "Action UP: Visits = 25, Q-value = 0.204\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [8.63146231e-01 9.26796209e-02 3.12898827e-06 4.41710187e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.313\n",
            "Action DOWN: Visits = 28, Q-value = 0.292\n",
            "Action RIGHT: Visits = 10, Q-value = 0.196\n",
            "Action UP: Visits = 26, Q-value = 0.271\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.93957069e-01 7.57128058e-07 3.20953648e-05 6.01007831e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 45, Q-value = 0.341\n",
            "Action DOWN: Visits = 11, Q-value = 0.183\n",
            "Action RIGHT: Visits = 16, Q-value = 0.241\n",
            "Action UP: Visits = 27, Q-value = 0.319\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.15997008 0.02797896 0.10635333 0.70569763]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 25, Q-value = 0.264\n",
            "Action DOWN: Visits = 21, Q-value = 0.213\n",
            "Action RIGHT: Visits = 24, Q-value = 0.213\n",
            "Action UP: Visits = 29, Q-value = 0.219\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.78373928e-07 1.02399917e-07 2.27992556e-07 9.99999191e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 14, Q-value = 0.259\n",
            "Action DOWN: Visits = 12, Q-value = 0.181\n",
            "Action RIGHT: Visits = 13, Q-value = 0.236\n",
            "Action UP: Visits = 60, Q-value = 0.278\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.74080047e-04 1.68920383e-05 1.55105968e-03 9.97457968e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.302\n",
            "Action DOWN: Visits = 14, Q-value = 0.140\n",
            "Action RIGHT: Visits = 22, Q-value = 0.276\n",
            "Action UP: Visits = 42, Q-value = 0.272\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.64522749e-01 3.82229806e-05 1.82170360e-05 6.35420811e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.342\n",
            "Action DOWN: Visits = 14, Q-value = 0.167\n",
            "Action RIGHT: Visits = 13, Q-value = 0.279\n",
            "Action UP: Visits = 37, Q-value = 0.316\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.0016769  0.71931798 0.0016769  0.27732822]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 18, Q-value = 0.206\n",
            "Action DOWN: Visits = 33, Q-value = 0.271\n",
            "Action RIGHT: Visits = 18, Q-value = 0.241\n",
            "Action UP: Visits = 30, Q-value = 0.276\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.35442510e-01 7.82906790e-08 2.19272924e-10 7.64557412e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 40, Q-value = 0.286\n",
            "Action DOWN: Visits = 9, Q-value = -0.038\n",
            "Action RIGHT: Visits = 5, Q-value = 0.000\n",
            "Action UP: Visits = 45, Q-value = 0.244\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [0.491571   0.00297234 0.01388566 0.491571  ]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 30, Q-value = 0.306\n",
            "Action DOWN: Visits = 18, Q-value = 0.240\n",
            "Action RIGHT: Visits = 21, Q-value = 0.301\n",
            "Action UP: Visits = 30, Q-value = 0.314\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [0.65131899 0.00116623 0.00592371 0.34159107]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.315\n",
            "Action DOWN: Visits = 17, Q-value = 0.203\n",
            "Action RIGHT: Visits = 20, Q-value = 0.161\n",
            "Action UP: Visits = 30, Q-value = 0.256\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.98556985e-06 1.21140469e-07 1.95648620e-08 9.99992874e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 18, Q-value = 0.277\n",
            "Action DOWN: Visits = 12, Q-value = 0.199\n",
            "Action RIGHT: Visits = 10, Q-value = 0.237\n",
            "Action UP: Visits = 59, Q-value = 0.337\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.01026468 0.82160399 0.02662395 0.14150738]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 20, Q-value = 0.351\n",
            "Action DOWN: Visits = 31, Q-value = 0.316\n",
            "Action RIGHT: Visits = 22, Q-value = 0.279\n",
            "Action UP: Visits = 26, Q-value = 0.310\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.39048477e-01 1.29498986e-10 8.01823588e-10 8.60951522e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 40, Q-value = 0.264\n",
            "Action DOWN: Visits = 5, Q-value = -0.092\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 48, Q-value = 0.290\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: RIGHT\n",
            "Action probabilities: [1.14626650e-09 1.94121238e-04 9.97129752e-01 2.67612611e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 6, Q-value = 0.112\n",
            "Action DOWN: Visits = 20, Q-value = 0.137\n",
            "Action RIGHT: Visits = 47, Q-value = 0.250\n",
            "Action UP: Visits = 26, Q-value = 0.216\n",
            "\n",
            "Current state: 1\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.93161866e-01 8.95814617e-09 2.90899764e-08 6.83809631e-03]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.264\n",
            "Action DOWN: Visits = 8, Q-value = 0.000\n",
            "Action RIGHT: Visits = 9, Q-value = 0.047\n",
            "Action UP: Visits = 31, Q-value = 0.217\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99300669e-01 1.94543872e-04 1.89984250e-07 5.04596701e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 47, Q-value = 0.341\n",
            "Action DOWN: Visits = 20, Q-value = 0.273\n",
            "Action RIGHT: Visits = 10, Q-value = 0.254\n",
            "Action UP: Visits = 22, Q-value = 0.316\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [5.64269432e-02 8.25167690e-01 1.03676397e-05 1.18394999e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.275\n",
            "Action DOWN: Visits = 34, Q-value = 0.303\n",
            "Action RIGHT: Visits = 11, Q-value = 0.167\n",
            "Action UP: Visits = 28, Q-value = 0.282\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.28330157e-07 4.28330157e-07 7.78871791e-13 9.99999143e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.221\n",
            "Action DOWN: Visits = 15, Q-value = 0.123\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 65, Q-value = 0.298\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [7.41451239e-01 2.04532835e-05 2.46551572e-07 2.58528061e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 40, Q-value = 0.283\n",
            "Action DOWN: Visits = 14, Q-value = 0.180\n",
            "Action RIGHT: Visits = 9, Q-value = 0.173\n",
            "Action UP: Visits = 36, Q-value = 0.238\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [5.45907981e-03 7.33867553e-04 5.45907981e-03 9.88347973e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.335\n",
            "Action DOWN: Visits = 18, Q-value = 0.203\n",
            "Action RIGHT: Visits = 22, Q-value = 0.286\n",
            "Action UP: Visits = 37, Q-value = 0.310\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [1.60192403e-01 7.86051692e-01 3.80769310e-06 5.37520975e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 29, Q-value = 0.306\n",
            "Action DOWN: Visits = 34, Q-value = 0.277\n",
            "Action RIGHT: Visits = 10, Q-value = 0.156\n",
            "Action UP: Visits = 26, Q-value = 0.266\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.67018256e-10 4.03451121e-09 1.63103766e-13 9.99999996e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 8, Q-value = 0.061\n",
            "Action DOWN: Visits = 11, Q-value = 0.053\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 76, Q-value = 0.285\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.28652907e-01 1.71039256e-03 8.06937959e-05 7.69556007e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 31, Q-value = 0.307\n",
            "Action DOWN: Visits = 19, Q-value = 0.212\n",
            "Action RIGHT: Visits = 14, Q-value = 0.261\n",
            "Action UP: Visits = 35, Q-value = 0.301\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.44087527e-01 1.11694631e-02 5.79747672e-05 3.44685035e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 33, Q-value = 0.276\n",
            "Action DOWN: Visits = 22, Q-value = 0.152\n",
            "Action RIGHT: Visits = 13, Q-value = 0.245\n",
            "Action UP: Visits = 31, Q-value = 0.241\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.64675038e-04 1.88395389e-01 2.71027239e-01 5.40312698e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 14, Q-value = 0.324\n",
            "Action DOWN: Visits = 27, Q-value = 0.330\n",
            "Action RIGHT: Visits = 28, Q-value = 0.253\n",
            "Action UP: Visits = 30, Q-value = 0.309\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.16408574e-06 1.11015867e-02 1.88006345e-07 9.88897061e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 12, Q-value = 0.270\n",
            "Action DOWN: Visits = 30, Q-value = 0.228\n",
            "Action RIGHT: Visits = 10, Q-value = 0.230\n",
            "Action UP: Visits = 47, Q-value = 0.355\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.22769594 0.06681903 0.02690387 0.67858115]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.276\n",
            "Action DOWN: Visits = 23, Q-value = 0.271\n",
            "Action RIGHT: Visits = 21, Q-value = 0.242\n",
            "Action UP: Visits = 29, Q-value = 0.291\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [2.83487168e-04 9.99267935e-01 1.65090638e-04 2.83487168e-04]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.307\n",
            "Action DOWN: Visits = 43, Q-value = 0.258\n",
            "Action RIGHT: Visits = 18, Q-value = 0.241\n",
            "Action UP: Visits = 19, Q-value = 0.253\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.19955468e-07 1.47152766e-04 2.43363771e-12 9.99852527e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 13, Q-value = 0.227\n",
            "Action DOWN: Visits = 24, Q-value = 0.168\n",
            "Action RIGHT: Visits = 4, Q-value = 0.000\n",
            "Action UP: Visits = 58, Q-value = 0.271\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.25020501e-07 2.25108610e-01 1.25020501e-07 7.74891140e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 9, Q-value = 0.095\n",
            "Action DOWN: Visits = 38, Q-value = 0.148\n",
            "Action RIGHT: Visits = 9, Q-value = 0.079\n",
            "Action UP: Visits = 43, Q-value = 0.210\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [6.79602304e-06 3.31671356e-03 3.23897808e-06 9.96673251e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 14, Q-value = 0.306\n",
            "Action DOWN: Visits = 26, Q-value = 0.283\n",
            "Action RIGHT: Visits = 13, Q-value = 0.249\n",
            "Action UP: Visits = 46, Q-value = 0.298\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.46038436e-01 1.94866406e-02 1.07164182e-06 6.34473851e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 32, Q-value = 0.288\n",
            "Action DOWN: Visits = 24, Q-value = 0.185\n",
            "Action RIGHT: Visits = 9, Q-value = 0.108\n",
            "Action UP: Visits = 34, Q-value = 0.231\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [1.79419177e-07 1.39454080e-05 4.24109518e-05 9.99943464e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 11, Q-value = 0.241\n",
            "Action DOWN: Visits = 17, Q-value = 0.207\n",
            "Action RIGHT: Visits = 19, Q-value = 0.261\n",
            "Action UP: Visits = 52, Q-value = 0.253\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [0.169959   0.00429869 0.11481821 0.7109241 ]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 26, Q-value = 0.316\n",
            "Action DOWN: Visits = 18, Q-value = 0.212\n",
            "Action RIGHT: Visits = 25, Q-value = 0.267\n",
            "Action UP: Visits = 30, Q-value = 0.316\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [3.02430245e-09 2.50888343e-07 5.37047361e-08 9.99999692e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 9, Q-value = 0.228\n",
            "Action DOWN: Visits = 14, Q-value = 0.203\n",
            "Action RIGHT: Visits = 12, Q-value = 0.237\n",
            "Action UP: Visits = 64, Q-value = 0.304\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.0183087  0.77612269 0.0013897  0.20417892]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 22, Q-value = 0.323\n",
            "Action DOWN: Visits = 32, Q-value = 0.272\n",
            "Action RIGHT: Visits = 17, Q-value = 0.239\n",
            "Action UP: Visits = 28, Q-value = 0.301\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.67357118e-09 5.31782396e-06 5.24457681e-11 9.99994673e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 10, Q-value = 0.222\n",
            "Action DOWN: Visits = 19, Q-value = 0.234\n",
            "Action RIGHT: Visits = 6, Q-value = 0.000\n",
            "Action UP: Visits = 64, Q-value = 0.311\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: DOWN\n",
            "Action probabilities: [0.00164327 0.95010979 0.02918079 0.01906615]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 18, Q-value = 0.277\n",
            "Action DOWN: Visits = 34, Q-value = 0.211\n",
            "Action RIGHT: Visits = 24, Q-value = 0.285\n",
            "Action UP: Visits = 23, Q-value = 0.198\n",
            "\n",
            "Current state: 4\n",
            "Chosen action: UP\n",
            "Action probabilities: [4.02663825e-04 1.90987532e-11 2.09993012e-09 9.99597334e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 27, Q-value = 0.217\n",
            "Action DOWN: Visits = 5, Q-value = -0.098\n",
            "Action RIGHT: Visits = 8, Q-value = 0.000\n",
            "Action UP: Visits = 59, Q-value = 0.264\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [6.42747972e-01 2.56188776e-04 1.34360887e-04 3.56861478e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.350\n",
            "Action DOWN: Visits = 16, Q-value = 0.145\n",
            "Action RIGHT: Visits = 15, Q-value = 0.247\n",
            "Action UP: Visits = 33, Q-value = 0.302\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [2.42250283e-03 8.57598265e-02 1.45235019e-06 9.11816218e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 21, Q-value = 0.285\n",
            "Action DOWN: Visits = 30, Q-value = 0.259\n",
            "Action RIGHT: Visits = 10, Q-value = 0.173\n",
            "Action UP: Visits = 38, Q-value = 0.320\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.37733569e-01 1.96025067e-04 1.40824228e-02 4.79879833e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 35, Q-value = 0.317\n",
            "Action DOWN: Visits = 15, Q-value = 0.249\n",
            "Action RIGHT: Visits = 23, Q-value = 0.264\n",
            "Action UP: Visits = 26, Q-value = 0.295\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.99951916e-01 2.99917914e-05 1.69342735e-05 1.15800618e-06]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 51, Q-value = 0.340\n",
            "Action DOWN: Visits = 18, Q-value = 0.169\n",
            "Action RIGHT: Visits = 17, Q-value = 0.280\n",
            "Action UP: Visits = 13, Q-value = 0.198\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [9.02308255e-05 2.60996863e-03 1.49225288e-02 9.82377272e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 15, Q-value = 0.288\n",
            "Action DOWN: Visits = 21, Q-value = 0.247\n",
            "Action RIGHT: Visits = 25, Q-value = 0.228\n",
            "Action UP: Visits = 38, Q-value = 0.280\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: LEFT\n",
            "Action probabilities: [9.34873175e-01 4.20626986e-08 4.41059363e-02 2.10208470e-02]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 38, Q-value = 0.315\n",
            "Action DOWN: Visits = 7, Q-value = 0.043\n",
            "Action RIGHT: Visits = 28, Q-value = 0.210\n",
            "Action UP: Visits = 26, Q-value = 0.245\n",
            "\n",
            "Current state: 0\n",
            "Chosen action: UP\n",
            "Action probabilities: [8.59476309e-06 1.50521234e-09 3.63600735e-08 9.99991367e-01]\n",
            "\n",
            "Root node statistics:\n",
            "Action LEFT: Visits = 19, Q-value = 0.287\n",
            "Action DOWN: Visits = 8, Q-value = 0.133\n",
            "Action RIGHT: Visits = 11, Q-value = 0.237\n",
            "Action UP: Visits = 61, Q-value = 0.319\n",
            "\n",
            "Episode finished!\n",
            "Final state: 0\n",
            "Total reward: 0.0\n",
            "Failed - either fell in a hole or exceeded steps\n",
            "\n",
            "Complete path:\n",
            "Step 0: State 0 -> Action LEFT\n",
            "Step 1: State 0 -> Action UP\n",
            "Step 2: State 0 -> Action UP\n",
            "Step 3: State 0 -> Action DOWN\n",
            "Step 4: State 4 -> Action UP\n",
            "Step 5: State 0 -> Action UP\n",
            "Step 6: State 0 -> Action UP\n",
            "Step 7: State 0 -> Action DOWN\n",
            "Step 8: State 4 -> Action UP\n",
            "Step 9: State 0 -> Action LEFT\n",
            "Step 10: State 0 -> Action UP\n",
            "Step 11: State 0 -> Action UP\n",
            "Step 12: State 0 -> Action UP\n",
            "Step 13: State 0 -> Action UP\n",
            "Step 14: State 0 -> Action UP\n",
            "Step 15: State 0 -> Action UP\n",
            "Step 16: State 0 -> Action LEFT\n",
            "Step 17: State 0 -> Action UP\n",
            "Step 18: State 0 -> Action DOWN\n",
            "Step 19: State 4 -> Action UP\n",
            "Step 20: State 0 -> Action LEFT\n",
            "Step 21: State 0 -> Action UP\n",
            "Step 22: State 0 -> Action LEFT\n",
            "Step 23: State 0 -> Action DOWN\n",
            "Step 24: State 4 -> Action UP\n",
            "Step 25: State 0 -> Action UP\n",
            "Step 26: State 0 -> Action UP\n",
            "Step 27: State 0 -> Action LEFT\n",
            "Step 28: State 0 -> Action UP\n",
            "Step 29: State 0 -> Action UP\n",
            "Step 30: State 0 -> Action UP\n",
            "Step 31: State 0 -> Action LEFT\n",
            "Step 32: State 0 -> Action UP\n",
            "Step 33: State 0 -> Action UP\n",
            "Step 34: State 0 -> Action LEFT\n",
            "Step 35: State 0 -> Action LEFT\n",
            "Step 36: State 0 -> Action UP\n",
            "Step 37: State 0 -> Action LEFT\n",
            "Step 38: State 0 -> Action DOWN\n",
            "Step 39: State 4 -> Action UP\n",
            "Step 40: State 0 -> Action UP\n",
            "Step 41: State 0 -> Action UP\n",
            "Step 42: State 0 -> Action UP\n",
            "Step 43: State 0 -> Action LEFT\n",
            "Step 44: State 0 -> Action DOWN\n",
            "Step 45: State 4 -> Action UP\n",
            "Step 46: State 0 -> Action UP\n",
            "Step 47: State 0 -> Action UP\n",
            "Step 48: State 0 -> Action DOWN\n",
            "Step 49: State 4 -> Action UP\n",
            "Step 50: State 0 -> Action UP\n",
            "Step 51: State 0 -> Action DOWN\n",
            "Step 52: State 4 -> Action UP\n",
            "Step 53: State 0 -> Action LEFT\n",
            "Step 54: State 0 -> Action LEFT\n",
            "Step 55: State 0 -> Action LEFT\n",
            "Step 56: State 0 -> Action UP\n",
            "Step 57: State 0 -> Action UP\n",
            "Step 58: State 0 -> Action UP\n",
            "Step 59: State 0 -> Action UP\n",
            "Step 60: State 0 -> Action DOWN\n",
            "Step 61: State 4 -> Action UP\n",
            "Step 62: State 0 -> Action LEFT\n",
            "Step 63: State 0 -> Action LEFT\n",
            "Step 64: State 0 -> Action UP\n",
            "Step 65: State 0 -> Action DOWN\n",
            "Step 66: State 4 -> Action UP\n",
            "Step 67: State 0 -> Action RIGHT\n",
            "Step 68: State 1 -> Action LEFT\n",
            "Step 69: State 0 -> Action LEFT\n",
            "Step 70: State 0 -> Action DOWN\n",
            "Step 71: State 4 -> Action UP\n",
            "Step 72: State 0 -> Action LEFT\n",
            "Step 73: State 0 -> Action UP\n",
            "Step 74: State 0 -> Action DOWN\n",
            "Step 75: State 4 -> Action UP\n",
            "Step 76: State 0 -> Action UP\n",
            "Step 77: State 0 -> Action LEFT\n",
            "Step 78: State 0 -> Action UP\n",
            "Step 79: State 0 -> Action UP\n",
            "Step 80: State 0 -> Action UP\n",
            "Step 81: State 0 -> Action DOWN\n",
            "Step 82: State 4 -> Action UP\n",
            "Step 83: State 0 -> Action UP\n",
            "Step 84: State 0 -> Action UP\n",
            "Step 85: State 0 -> Action UP\n",
            "Step 86: State 0 -> Action UP\n",
            "Step 87: State 0 -> Action UP\n",
            "Step 88: State 0 -> Action UP\n",
            "Step 89: State 0 -> Action DOWN\n",
            "Step 90: State 4 -> Action UP\n",
            "Step 91: State 0 -> Action DOWN\n",
            "Step 92: State 4 -> Action UP\n",
            "Step 93: State 0 -> Action LEFT\n",
            "Step 94: State 0 -> Action UP\n",
            "Step 95: State 0 -> Action LEFT\n",
            "Step 96: State 0 -> Action LEFT\n",
            "Step 97: State 0 -> Action UP\n",
            "Step 98: State 0 -> Action LEFT\n",
            "Step 99: State 0 -> Action UP\n",
            "\n",
            "Results over 5 episodes:\n",
            "Average reward: 0.00\n",
            "Rewards: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.BFS"
      ],
      "metadata": {
        "id": "uybNDLxcACVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from queue import PriorityQueue\n",
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "\n",
        "class PathNode:\n",
        "    def __init__(self, state, action=None, parent=None, is_terminated=False, is_hole=False):\n",
        "        self.state = state        # 当前状态\n",
        "        self.action = action      # 到达此状态的动作\n",
        "        self.parent = parent      # 前一个节点\n",
        "        self.children = []        # 后续可能的路径\n",
        "        self.is_terminated = is_terminated\n",
        "        self.is_hole = is_hole    # 是否是陷阱\n",
        "\n",
        "    def get_path(self):\n",
        "        \"\"\"回溯完整路径\"\"\"\n",
        "        path = []\n",
        "        current = self\n",
        "        while current:\n",
        "            path.append((current.state, current.action))\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    def add_child(self, child_node):\n",
        "        self.children.append(child_node)\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return False\n",
        "\n",
        "class PathFinder:\n",
        "    def __init__(self, shared_network, grid_size=4):\n",
        "        self.grid_size = grid_size\n",
        "        self.goal = grid_size * grid_size - 1  # 最后一个格子\n",
        "        self.env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "        self.shared_network = shared_network\n",
        "\n",
        "    def get_valid_actions(self, state):\n",
        "        \"\"\"获取有效动作\"\"\"\n",
        "        row = state // self.grid_size    # 行号\n",
        "        col = state % self.grid_size     # 列号\n",
        "        actions = []\n",
        "\n",
        "        # 检查四个方向的有效性\n",
        "        if col > 0: actions.append(0)    # 左\n",
        "        if row < 3: actions.append(1)    # 下\n",
        "        if col < 3: actions.append(2)    # 右\n",
        "        if row > 0: actions.append(3)    # 上\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "    def get_state_value(self, state):\n",
        "        \"\"\"使用神经网络获取状态价值\"\"\"\n",
        "        state_onehot = torch.zeros(16)\n",
        "        state_onehot[state] = 1\n",
        "        return self.shared_network.get_value(state_onehot)\n",
        "\n",
        "    def best_first_search(self, start_state, max_path_length=20):\n",
        "        \"\"\"使用启发式搜索寻找最佳路径\"\"\"\n",
        "        frontier = PriorityQueue()\n",
        "\n",
        "        # 优先级是神经网络预测的价值和曼哈顿距离的组合\n",
        "        value = self.get_state_value(start_state)\n",
        "        initial_priority = -value\n",
        "\n",
        "        root = PathNode(start_state)\n",
        "        frontier.put((initial_priority, start_state, root))\n",
        "\n",
        "        # 记录不同类型的路径\n",
        "        all_paths = {\n",
        "            'success': [],  # 成功到达目标的路径\n",
        "            'hole': [],    # 掉进洞里的路径\n",
        "            'timeout': []  # 超过最大长度的路径\n",
        "        }\n",
        "\n",
        "        def get_path_states(node):\n",
        "            \"\"\"获取路径上的所有状态\"\"\"\n",
        "            states = set()\n",
        "            current = node\n",
        "            while current:\n",
        "                states.add(current.state)\n",
        "                current = current.parent\n",
        "            return states\n",
        "\n",
        "        def get_path_length(node):\n",
        "            \"\"\"获取路径长度\"\"\"\n",
        "            length = 0\n",
        "            current = node\n",
        "            while current.parent:\n",
        "                length += 1\n",
        "                current = current.parent\n",
        "            return length\n",
        "\n",
        "        while not frontier.empty():\n",
        "            priority, _, current_node = frontier.get()\n",
        "            current_path_states = get_path_states(current_node)\n",
        "            current_length = get_path_length(current_node)\n",
        "\n",
        "            # 检查是否超过最大路径长度\n",
        "            if current_length >= max_path_length:\n",
        "                all_paths['timeout'].append(current_node.get_path())\n",
        "                continue\n",
        "\n",
        "            # 检查当前状态\n",
        "            if current_node.is_hole:\n",
        "                all_paths['hole'].append(current_node.get_path())\n",
        "                continue\n",
        "\n",
        "            if current_node.state == self.goal:\n",
        "                all_paths['success'].append(current_node.get_path())\n",
        "\n",
        "\n",
        "            # 展开当前节点\n",
        "            for action in self.get_valid_actions(current_node.state):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
        "\n",
        "                is_hole = done and reward == 0\n",
        "\n",
        "                if next_state not in current_path_states:\n",
        "                    new_node = PathNode(\n",
        "                        state=next_state,\n",
        "                        action=action,\n",
        "                        parent=current_node,\n",
        "                        is_hole=is_hole,\n",
        "                    )\n",
        "                    current_node.add_child(new_node)\n",
        "\n",
        "                    # 使用神经网络预测的价值作为启发\n",
        "                    value = self.get_state_value(next_state)\n",
        "\n",
        "                    new_priority = -value\n",
        "\n",
        "                    frontier.put((new_priority, next_state, new_node))\n",
        "\n",
        "        return all_paths, root\n",
        "\n",
        "    def print_path(self, path):\n",
        "        \"\"\"可视化路径\"\"\"\n",
        "        if not path:\n",
        "            print(\"No path found!\")\n",
        "            return\n",
        "\n",
        "        actions_map = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
        "        for state, action in path:\n",
        "            if action is not None:\n",
        "                print(f\"State: {state}, Action: {actions_map[action]}\")\n",
        "            else:\n",
        "                print(f\"Start State: {state}\")"
      ],
      "metadata": {
        "id": "IFlgBnq2--zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.1 test"
      ],
      "metadata": {
        "id": "HDuwOsoUS07Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def test_pathfinder():\n",
        "    # 1. 初始化环境和网络\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    shared_network = SharedNetwork(input_size=16, hidden_size=128, num_actions=4)\n",
        "    pathfinder = PathFinder(shared_network)\n",
        "\n",
        "    # 2. 从起始状态开始搜索\n",
        "    start_state = 0  # FrozenLake的起始位置\n",
        "    print(f\"\\nStarting search from state {start_state}\")\n",
        "\n",
        "    # 3. 记录开始时间\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 4. 执行搜索\n",
        "    all_paths, root = pathfinder.best_first_search(start_state)\n",
        "\n",
        "    # 5. 计算搜索时间\n",
        "    end_time = time.time()\n",
        "    search_time = end_time - start_time\n",
        "\n",
        "    # 6. 打印搜索结果\n",
        "    print(f\"\\nSearch completed in {search_time:.4f} seconds\")\n",
        "\n",
        "    # 打印成功的路径\n",
        "    for i in all_paths['success']:\n",
        "        print(\"\\nSuccessful path found!\")\n",
        "        print(\"Path details:\")\n",
        "        pathfinder.print_path(i)\n",
        "        path_length = len(i) - 1  # 减去起始状态\n",
        "        print(f\"Path length: {path_length}\")\n",
        "    else:\n",
        "        print(\"\\nNo successful path found\")\n",
        "\n",
        "    # 打印统计信息\n",
        "    print(\"\\nSearch statistics:\")\n",
        "    print(f\"Number of success paths: {len(all_paths['success'])}\")\n",
        "    print(f\"Number of hole paths: {len(all_paths['hole'])}\")\n",
        "    print(f\"Number of timeout paths: {len(all_paths['timeout'])}\")\n",
        "\n",
        "    # 7. 验证路径的可行性\n",
        "    if all_paths['success']:\n",
        "        print(\"\\nVerifying path...\")\n",
        "        env.reset()\n",
        "        path = all_paths['success'][0]\n",
        "        total_reward = 0\n",
        "\n",
        "        for state, action in path[1:]:  # 跳过起始状态\n",
        "            if action is not None:\n",
        "                next_state, reward, done, truncated, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                if done:\n",
        "                    if reward == 1:\n",
        "                        print(\"Successfully reached the goal!\")\n",
        "                    else:\n",
        "                        print(\"Failed - fell into a hole!\")\n",
        "                    break\n",
        "\n",
        "        print(f\"Total reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_pathfinder()\n",
        "\n",
        "    # 额外测试：测试不同起始状态\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    shared_network = SharedNetwork(input_size=16, hidden_size=128, num_actions=4)\n",
        "    pathfinder = PathFinder(shared_network)\n",
        "\n",
        "\n",
        "    state =0\n",
        "\n",
        "    all_paths, _ = pathfinder.best_first_search(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O_wN_d7JKJ6h",
        "outputId": "7171b6b6-2648-4fcb-8080-d1f15557ff45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting search from state 0\n",
            "\n",
            "Search completed in 0.0328 seconds\n",
            "\n",
            "Successful path found!\n",
            "Path details:\n",
            "Start State: 0\n",
            "State: 1, Action: →\n",
            "State: 2, Action: →\n",
            "State: 6, Action: ↓\n",
            "State: 10, Action: ↓\n",
            "State: 14, Action: ↓\n",
            "State: 15, Action: →\n",
            "Path length: 6\n",
            "\n",
            "Successful path found!\n",
            "Path details:\n",
            "Start State: 0\n",
            "State: 1, Action: →\n",
            "State: 2, Action: →\n",
            "State: 6, Action: ↓\n",
            "State: 10, Action: ↓\n",
            "State: 9, Action: ←\n",
            "State: 13, Action: ↓\n",
            "State: 14, Action: →\n",
            "State: 15, Action: →\n",
            "Path length: 8\n",
            "\n",
            "Successful path found!\n",
            "Path details:\n",
            "Start State: 0\n",
            "State: 4, Action: ↓\n",
            "State: 8, Action: ↓\n",
            "State: 9, Action: →\n",
            "State: 13, Action: ↓\n",
            "State: 14, Action: →\n",
            "State: 15, Action: →\n",
            "Path length: 6\n",
            "\n",
            "Successful path found!\n",
            "Path details:\n",
            "Start State: 0\n",
            "State: 4, Action: ↓\n",
            "State: 8, Action: ↓\n",
            "State: 9, Action: →\n",
            "State: 10, Action: →\n",
            "State: 14, Action: ↓\n",
            "State: 15, Action: →\n",
            "Path length: 6\n",
            "\n",
            "No successful path found\n",
            "\n",
            "Search statistics:\n",
            "Number of success paths: 4\n",
            "Number of hole paths: 28\n",
            "Number of timeout paths: 0\n",
            "\n",
            "Verifying path...\n",
            "Successfully reached the goal!\n",
            "Total reward: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f48b08736cb>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = F.one_hot(torch.tensor(state.argmax()), num_classes=16).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.HYBRID\n"
      ],
      "metadata": {
        "id": "tueacZqbAHBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size=16, hidden_size=128, num_actions=4):\n",
        "        super(Network, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # 共享层\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size)\n",
        "        )\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, num_actions)\n",
        "        )\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            state: 游戏状态的one-hot编码 (batch_size, 16)\n",
        "        返回:\n",
        "            policy_logits: 动作概率的对数 (batch_size, 4)\n",
        "            value: 状态价值估计 (batch_size, 1)\n",
        "        \"\"\"\n",
        "        shared_features = self.shared_layers(state)\n",
        "\n",
        "        policy_logits = self.policy_head(shared_features)\n",
        "        value = self.value_head(shared_features)\n",
        "\n",
        "        return policy_logits, value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        short cut for BFS\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            _, value = self.forward(state)\n",
        "            return value.item()\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load(self, filepath):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=self.device))\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "\n",
        "\n",
        "# 2. 初始化主要组件\n"
      ],
      "metadata": {
        "id": "LxFKYMngq3t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "from queue import PriorityQueue\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    prior: float\n",
        "    action_taken: int\n",
        "    visit_count: int = 0\n",
        "    value_sum: float = 0\n",
        "    state: int = 0\n",
        "    parent: Optional['Node'] = None\n",
        "    children: Dict[int, 'Node'] = None\n",
        "    done = False\n",
        "    has_children = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.children is None:\n",
        "            self.children = {}\n",
        "\n",
        "    @property\n",
        "    def Q_value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "    def __lt__(self, other):\n",
        "      # 任意返回 False，因为我们只关心优先级的比较\n",
        "      return False\n",
        "\n",
        "class HybridMCTS:\n",
        "    def __init__(self, model, num_simulations: int = 100, local_search_depth: int = 2):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "        self.c_puct = 2.0\n",
        "        self.local_search_depth = local_search_depth\n",
        "        self.epsilon = 0.4\n",
        "\n",
        "\n",
        "    def bfs_search(self, start_state, depth_limit=3):\n",
        "        \"\"\"使用真实环境进行局部BFS搜索\"\"\"\n",
        "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "        path_node = Node(prior=1.0, action_taken=None,state = start_state)\n",
        "        #print('start_state',start_state)\n",
        "\n",
        "        queue = PriorityQueue()\n",
        "        queue.put((0, path_node))\n",
        "\n",
        "        def get_path_states(node):\n",
        "          # 获取当前路径上的所有状态\n",
        "          states = set()\n",
        "          current = node\n",
        "          while current:\n",
        "              states.add(current.state)\n",
        "              current = current.parent\n",
        "              #print('记录的',current.state)\n",
        "          return states\n",
        "        def get_action_path(node):\n",
        "          # 回溯完整路径\n",
        "          path_action = []\n",
        "          current = node\n",
        "          while current.parent:\n",
        "            path_action.append(current.action_taken)\n",
        "            #print('current.state',current.state)\n",
        "            current = current.parent\n",
        "          return list(reversed(path_action))\n",
        "        def get_path(node):\n",
        "          # 回溯完整路径\n",
        "          path = []\n",
        "          current = node\n",
        "          while current:\n",
        "            path.append(current.state)\n",
        "            current = current.parent\n",
        "          return path\n",
        "        while not queue.empty():\n",
        "\n",
        "          priority, current_node = queue.get()\n",
        "          visited = get_path_states(current_node)\n",
        "          # 如果达到深度限制\n",
        "          if len(get_path(current_node)) > depth_limit:\n",
        "              continue\n",
        "          #reach to gaol\n",
        "          if current_node.state == 15:\n",
        "            return get_action_path(current_node), 1\n",
        "          # 获取当前状态下的有效动作\n",
        "          row = current_node.state // 4\n",
        "          col = current_node.state % 4\n",
        "          valid_actions = []\n",
        "          if col > 0: valid_actions.append(0)    # 左\n",
        "          if row < 3: valid_actions.append(1)    # 下\n",
        "          if col < 3: valid_actions.append(2)    # 右\n",
        "          if row > 0: valid_actions.append(3)    # 上\n",
        "\n",
        "          for action in valid_actions:\n",
        "              # 使用环境模拟动作\n",
        "              env.reset()\n",
        "              env.unwrapped.s = current_node.state\n",
        "              #assert env.unwrapped.s == current_state, f\"State mismatch: expected {current_state}, got {env.unwrapped.s}\"\n",
        "              next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "              done = terminated or truncated\n",
        "              # 如果是合法的下一个状态\n",
        "              if next_state not in visited:\n",
        "\n",
        "                  # 使用价值网络评估优先级\n",
        "                  _, value = self.evaluate_state(next_state)\n",
        "                  next_node = Node(prior=1.0,\n",
        "                            action_taken=action,\n",
        "                            state = next_state,\n",
        "                            parent=current_node)\n",
        "                  current_node.children[action] = next_node\n",
        "                  #print('value',value)\n",
        "                  queue.put((-value, next_node))\n",
        "\n",
        "        return None, 0\n",
        "\n",
        "    def search(self, root_state):\n",
        "      root = Node(prior=1.0, action_taken=None)\n",
        "\n",
        "      for times in range(self.num_simulations):\n",
        "        node = root\n",
        "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "        env.reset()\n",
        "        state = root_state\n",
        "        env.unwrapped.s = state  # 设置环境状态\n",
        "\n",
        "        search_path = [node]\n",
        "        value = 0\n",
        "        switched_to_bfs = False\n",
        "\n",
        "        #BFS\n",
        "        if self.should_use_bfs(state):\n",
        "          best_path, path_value = self.bfs_search(state, self.local_search_depth)\n",
        "          if best_path:\n",
        "            first_action = best_path[0]\n",
        "            policy = np.ones(4) * 0.01\n",
        "            policy[first_action] = 0.7\n",
        "            self.expand_node(node, policy)\n",
        "            search_path.append(node.children[first_action])\n",
        "            value = path_value * 0.98 ** (len(best_path)-1)\n",
        "            switched_to_bfs = True\n",
        "        #print('Alphazero')\n",
        "        if not switched_to_bfs:\n",
        "          while node.has_children and not node.done:\n",
        "            action, node = self.select_child(node, state)\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            node.done = terminated or truncated\n",
        "            state = observation\n",
        "            search_path.append(node)\n",
        "\n",
        "            # 在selection过程中检查是否可以切换到BFS\n",
        "            if not node.done and self.should_use_bfs(state):\n",
        "              best_path, path_value = self.bfs_search(state, self.local_search_depth)\n",
        "              if best_path:\n",
        "                # 找到BFS路径，记录value并结束selection\n",
        "                value = path_value * 0.98 ** (len(best_path))\n",
        "                switched_to_bfs = True\n",
        "                break\n",
        "\n",
        "          # 如果没有切换到BFS且没有结束，进行常规expansion\n",
        "          if not switched_to_bfs and not node.done:\n",
        "              policy, value = self.evaluate_state(state)\n",
        "              self.expand_node(node, policy)\n",
        "        '''# 1. Selection\n",
        "        while node.has_children and not node.done:\n",
        "          action, node = self.select_child(node,state)\n",
        "          #print('action',action)\n",
        "          observation, reward, terminated, truncated, info = env.step(action)\n",
        "          #print('observation',observation)\n",
        "          node.done = terminated or truncated\n",
        "          state = observation\n",
        "          search_path.append(node)\n",
        "\n",
        "\n",
        "        # 2. Expansion and Evaluation\n",
        "\n",
        "        if not node.done:\n",
        "          policy, value = self.evaluate_state(state)\n",
        "          self.expand_node(node, policy)'''\n",
        "\n",
        "        # 3. Backup\n",
        "        self.backup(search_path, value)\n",
        "\n",
        "      return root\n",
        "\n",
        "    def should_use_bfs(self, state):\n",
        "        \"\"\"判断是否应该使用BFS\"\"\"\n",
        "        # 可以基于以下因素:\n",
        "        # 1. 到目标的估计距离\n",
        "        # 2. 状态的复杂度\n",
        "        # 3. 当前的计算资源\n",
        "        distance_to_goal = self.estimate_distance(state)\n",
        "        #print('distance_to_goal',distance_to_goal)\n",
        "        return distance_to_goal <= self.local_search_depth\n",
        "\n",
        "    def estimate_distance(self, state):\n",
        "      \"\"\"估计当前状态到目标的曼哈顿距离\"\"\"\n",
        "      # FrozenLake 环境中，目标在右下角 (3,3)，状态编号为 15\n",
        "      current_row = state // 4  # 当前行\n",
        "      current_col = state % 4   # 当前列\n",
        "      goal_row = 3  # 目标行\n",
        "      goal_col = 3  # 目标列\n",
        "\n",
        "      manhattan_dist = abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "      return manhattan_dist\n",
        "\n",
        "    def select_child(self, node, state):\n",
        "        def is_valid_move(state=0):\n",
        "          row = state // 4\n",
        "          col = state % 4\n",
        "          valid_actions = []\n",
        "          if col > 0: valid_actions.append(0)    # 左\n",
        "          if row < 3: valid_actions.append(1)    # 下\n",
        "          if col < 3: valid_actions.append(2)    # 右\n",
        "          if row > 0: valid_actions.append(3)\n",
        "          return valid_actions\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        sqrt_total_count = math.sqrt(node.visit_count)\n",
        "\n",
        "        for action, child in node.children.items():\n",
        "          if action not in is_valid_move(state):\n",
        "            continue\n",
        "          ucb_score = child.Q_value + self.c_puct * child.prior * sqrt_total_count / (1 + child.visit_count)\n",
        "          if ucb_score > best_score:\n",
        "            best_score = ucb_score\n",
        "            best_action = action\n",
        "            best_child = child\n",
        "\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def expand_node(self, node: Node, policy: np.ndarray):\n",
        "\n",
        "      noise = np.random.dirichlet([0.3] * len(policy))\n",
        "      #policy = 0.75 * policy + 0.25 * noise\n",
        "\n",
        "\n",
        "      policy = (1 - self.epsilon) * policy + self.epsilon * noise\n",
        "\n",
        "      for action, prob in enumerate(policy):\n",
        "\n",
        "        child = Node(\n",
        "            prior=prob,\n",
        "            action_taken=action,\n",
        "            parent=node\n",
        "        )\n",
        "\n",
        "        node.children[action] = child\n",
        "\n",
        "        node.has_children = True\n",
        "\n",
        "    def evaluate_state(self, state):\n",
        "        \"\"\"使用神经网络评估状态\"\"\"\n",
        "        with torch.no_grad():\n",
        "          if isinstance(state, (int, np.integer)):\n",
        "            # 如果是整数状态，直接转one-hot\n",
        "            state_tensor = F.one_hot(torch.tensor(state), num_classes=16).float()\n",
        "          else:\n",
        "              # 如果是其他格式，先转换为tensor\n",
        "              state_tensor = torch.FloatTensor(state)\n",
        "          policy, value = self.model(state_tensor)\n",
        "          return F.softmax(policy, dim=-1), value.item()\n",
        "\n",
        "\n",
        "\n",
        "    def backup1(self, search_path: List[Node], value: float):\n",
        "      \"\"\"带长度惩罚的回传更新\"\"\"\n",
        "      # 基础参数\n",
        "      gamma = 0.95  # 折扣因子\n",
        "      base_length = 8  # 基准长度\n",
        "      length_penalty = 0.1  # 长度惩罚系数\n",
        "      path_length = len(search_path)\n",
        "      # 计算长度惩罚\n",
        "      if path_length > base_length:\n",
        "          # 超过基准长度的部分施加惩罚\n",
        "          length_multiplier = 1.0 - length_penalty * ((path_length - base_length) / base_length)\n",
        "          # 确保惩罚不会太过严重\n",
        "          length_multiplier = max(0.2, length_multiplier)\n",
        "          current_value = value * length_multiplier\n",
        "      else:\n",
        "          current_value = value\n",
        "\n",
        "      # 从后往前更新\n",
        "      for idx, node in enumerate(reversed(search_path)):\n",
        "          # 基础更新\n",
        "          node.visit_count += 1\n",
        "\n",
        "          # 使用折扣更新价值\n",
        "          discounted_value = current_value * (gamma ** idx)\n",
        "          node.value_sum += discounted_value\n",
        "          #print('node.value_sum',node.value_sum)\n",
        "    def backup(self, search_path: List[Node], value: float):\n",
        "        \"\"\"回传更新\"\"\"\n",
        "        gamma = 0.98\n",
        "        current_value = value\n",
        "        for node in reversed(search_path):\n",
        "            node.visit_count += 1\n",
        "            node.value_sum += current_value\n",
        "            current_value *= gamma\n",
        "\n",
        "\n",
        "    def get_action_probs(self, root: Node, temperature: float = 1.0):\n",
        "        \"\"\"获取动作概率\"\"\"\n",
        "        counts = np.array([child.visit_count for child in root.children.values()])\n",
        "        if temperature == 0:\n",
        "            probs = np.zeros_like(counts)\n",
        "            probs[np.argmax(counts)] = 1\n",
        "            return probs\n",
        "        else:\n",
        "            counts = counts ** (1.0 / temperature)\n",
        "            probs = counts / np.sum(counts)\n",
        "            return probs"
      ],
      "metadata": {
        "id": "aVC6nAccyy-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs_search(start_state, depth_limit=3):\n",
        "        \"\"\"使用真实环境进行局部BFS搜索\"\"\"\n",
        "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "        path_node = Node(prior=1.0, action_taken=None,state = start_state)\n",
        "        #print('start_state',start_state)\n",
        "\n",
        "        queue = PriorityQueue()\n",
        "        queue.put((0, path_node))\n",
        "\n",
        "        def get_path_states(node):\n",
        "          # 获取当前路径上的所有状态\n",
        "          states = set()\n",
        "          current = node\n",
        "          while current:\n",
        "              states.add(current.state)\n",
        "              current = current.parent\n",
        "              #print('记录的',current.state)\n",
        "          return states\n",
        "        def get_action_path(node):\n",
        "          # 回溯完整路径\n",
        "          path_action = []\n",
        "          current = node\n",
        "          while current.parent:\n",
        "            path_action.append(current.action_taken)\n",
        "            #print('current.state',current.state)\n",
        "            current = current.parent\n",
        "          return list(reversed(path_action))\n",
        "        def get_path(node):\n",
        "          # 回溯完整路径\n",
        "          path = []\n",
        "          current = node\n",
        "          while current:\n",
        "            path.append(current.state)\n",
        "            current = current.parent\n",
        "          return path\n",
        "        while not queue.empty():\n",
        "\n",
        "          priority, current_node = queue.get()\n",
        "          visited = get_path_states(current_node)\n",
        "\n",
        "          #reach to gaol\n",
        "          if current_node.state == 15:\n",
        "            return get_action_path(current_node), 1\n",
        "          if len(get_path(current_node)) >= depth_limit:\n",
        "              continue\n",
        "          # 获取当前状态下的有效动作\n",
        "          row = current_node.state // 4\n",
        "          col = current_node.state % 4\n",
        "          valid_actions = []\n",
        "          if col > 0: valid_actions.append(0)    # 左\n",
        "          if row < 3: valid_actions.append(1)    # 下\n",
        "          if col < 3: valid_actions.append(2)    # 右\n",
        "          if row > 0: valid_actions.append(3)    # 上\n",
        "\n",
        "          for action in valid_actions:\n",
        "              # 使用环境模拟动作\n",
        "\n",
        "              env.reset()\n",
        "              env.unwrapped.s = current_node.state\n",
        "              print('current_node.state',current_node.state)\n",
        "              print('action',action)\n",
        "              #assert env.unwrapped.s == current_state, f\"State mismatch: expected {current_state}, got {env.unwrapped.s}\"\n",
        "              next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "              done = terminated or truncated\n",
        "              print('next_state',next_state)\n",
        "              # 如果是合法的下一个状态\n",
        "              if next_state not in visited:\n",
        "                  print('next,next_state',next_state)\n",
        "                  net=Network()\n",
        "                  with torch.no_grad():\n",
        "                    if isinstance(next_state, (int, np.integer)):\n",
        "                      # 如果是整数状态，直接转one-hot\n",
        "                      state_tensor = F.one_hot(torch.tensor(next_state), num_classes=16).float()\n",
        "                    else:\n",
        "                        # 如果是其他格式，先转换为tensor\n",
        "                        state_tensor = torch.FloatTensor(next_state)\n",
        "                  value=net.get_value(state_tensor)\n",
        "                  print('value',value)\n",
        "                  # 使用价值网络评估优先级\n",
        "\n",
        "                  next_node = Node(prior=1.0,\n",
        "                            action_taken=action,\n",
        "                            state = next_state,\n",
        "                            parent=current_node)\n",
        "                  current_node.children[action] = next_node\n",
        "                  #print('value',value)\n",
        "                  queue.put((-value, next_node))\n",
        "\n",
        "        return None, 0\n",
        "bfs_search(10, depth_limit=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_2z9B0rsOkV-",
        "outputId": "d3bf6ad7-3ee9-4102-9f36-1f9bd1fd8fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_node.state 10\n",
            "action 0\n",
            "next_state 9\n",
            "next,next_state 9\n",
            "value 0.4209800064563751\n",
            "current_node.state 10\n",
            "action 1\n",
            "next_state 14\n",
            "next,next_state 14\n",
            "value -0.3975231647491455\n",
            "current_node.state 10\n",
            "action 2\n",
            "next_state 11\n",
            "next,next_state 11\n",
            "value -0.4413963556289673\n",
            "current_node.state 10\n",
            "action 3\n",
            "next_state 6\n",
            "next,next_state 6\n",
            "value 0.038615211844444275\n",
            "current_node.state 9\n",
            "action 0\n",
            "next_state 8\n",
            "next,next_state 8\n",
            "value -0.4255306124687195\n",
            "current_node.state 9\n",
            "action 1\n",
            "next_state 13\n",
            "next,next_state 13\n",
            "value 0.39316731691360474\n",
            "current_node.state 9\n",
            "action 2\n",
            "next_state 10\n",
            "current_node.state 9\n",
            "action 3\n",
            "next_state 5\n",
            "next,next_state 5\n",
            "value -0.27908843755722046\n",
            "current_node.state 13\n",
            "action 0\n",
            "next_state 12\n",
            "next,next_state 12\n",
            "value 0.614520788192749\n",
            "current_node.state 13\n",
            "action 2\n",
            "next_state 14\n",
            "next,next_state 14\n",
            "value 0.6405401825904846\n",
            "current_node.state 13\n",
            "action 3\n",
            "next_state 9\n",
            "current_node.state 6\n",
            "action 0\n",
            "next_state 5\n",
            "next,next_state 5\n",
            "value -0.08646199852228165\n",
            "current_node.state 6\n",
            "action 1\n",
            "next_state 10\n",
            "current_node.state 6\n",
            "action 2\n",
            "next_state 7\n",
            "next,next_state 7\n",
            "value 0.4689267873764038\n",
            "current_node.state 6\n",
            "action 3\n",
            "next_state 2\n",
            "next,next_state 2\n",
            "value -0.7003492116928101\n",
            "current_node.state 7\n",
            "action 0\n",
            "next_state 7\n",
            "current_node.state 7\n",
            "action 1\n",
            "next_state 7\n",
            "current_node.state 7\n",
            "action 3\n",
            "next_state 7\n",
            "current_node.state 5\n",
            "action 0\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 1\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 2\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 3\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 0\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 1\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 2\n",
            "next_state 5\n",
            "current_node.state 5\n",
            "action 3\n",
            "next_state 5\n",
            "current_node.state 14\n",
            "action 0\n",
            "next_state 13\n",
            "next,next_state 13\n",
            "value -0.15375736355781555\n",
            "current_node.state 14\n",
            "action 2\n",
            "next_state 15\n",
            "next,next_state 15\n",
            "value -0.42986804246902466\n",
            "current_node.state 14\n",
            "action 3\n",
            "next_state 10\n",
            "current_node.state 13\n",
            "action 0\n",
            "next_state 12\n",
            "next,next_state 12\n",
            "value 0.6369121670722961\n",
            "current_node.state 13\n",
            "action 2\n",
            "next_state 14\n",
            "current_node.state 13\n",
            "action 3\n",
            "next_state 9\n",
            "next,next_state 9\n",
            "value 0.2700065076351166\n",
            "current_node.state 8\n",
            "action 1\n",
            "next_state 12\n",
            "next,next_state 12\n",
            "value -0.00018152594566345215\n",
            "current_node.state 8\n",
            "action 2\n",
            "next_state 9\n",
            "current_node.state 8\n",
            "action 3\n",
            "next_state 4\n",
            "next,next_state 4\n",
            "value 0.043469324707984924\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "def analyze_mcts_search(model, num_simulations=100):\n",
        "    \"\"\"分析MCTS搜索过程中的访问统计\"\"\"\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    mcts = HybridMCTS(model, num_simulations=num_simulations)\n",
        "    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    state = 9\n",
        "\n",
        "    print(f\"\\nInitial state: {state}\")\n",
        "\n",
        "    # 执行MCTS搜索\n",
        "    root = mcts.search(state)\n",
        "\n",
        "    # 分析根节点的子节点\n",
        "    print(\"\\nRoot node analysis:\")\n",
        "    print(f\"Total visits to root: {root.visit_count}\")\n",
        "    print(f\"Root value: {root.Q_value:.4f}\")\n",
        "    print(\"\\nChildren statistics:\")\n",
        "    print(\"Action | Visits | Q-Value | Prior | UCB Score\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 计算UCB分数用于比较\n",
        "    sqrt_total_count = math.sqrt(root.visit_count)\n",
        "\n",
        "    for action, child in root.children.items():\n",
        "        ucb_score = child.Q_value + mcts.c_puct * child.prior * sqrt_total_count / (1 + child.visit_count)\n",
        "        print(f\"{action_names[action]:<6} | {child.visit_count:>6} | {child.Q_value:>7.4f} | {child.prior:>5.3f} | {ucb_score:>9.4f}\")\n",
        "\n",
        "    # 显示访问最多的前3条路径\n",
        "    print(\"\\nTop visited paths:\")\n",
        "    def get_most_visited_path(node, depth=0, max_depth=7):\n",
        "        if depth >= max_depth or not node.children:\n",
        "            if not node:\n",
        "              print('no children in this node')\n",
        "            return []\n",
        "\n",
        "        most_visited_child = max(node.children.items(),\n",
        "                               key=lambda x: x[1].visit_count)\n",
        "        action, child = most_visited_child\n",
        "        return [(action, child.visit_count, child.Q_value)] + get_most_visited_path(child, depth + 1)\n",
        "\n",
        "    path = get_most_visited_path(root)\n",
        "    for depth, (action, visits, value) in enumerate(path):\n",
        "        print(f\"Depth {depth}: Action={action_names[action]}, Visits={visits}, Value={value:.4f}\")\n",
        "\n",
        "    return root\n",
        "\n",
        "# 使用示例\n",
        "model = Network()\n",
        "#model.load_state_dict(torch.load('best_model.pth'))  # 如果有保存的模型\n",
        "model.eval()\n",
        "\n",
        "# 分析MCTS搜索\n",
        "root = analyze_mcts_search(model, num_simulations=100)\n",
        "print(root.children)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaQWV9rzBKVd",
        "outputId": "1c826e4c-bb5d-4282-b209-bfbb28088cbc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial state: 9\n",
            "\n",
            "Root node analysis:\n",
            "Total visits to root: 100\n",
            "Root value: 0.6705\n",
            "\n",
            "Children statistics:\n",
            "Action | Visits | Q-Value | Prior | UCB Score\n",
            "--------------------------------------------------\n",
            "LEFT   |      8 | -0.0284 | 0.379 |    0.8133\n",
            "DOWN   |     18 |  0.7065 | 0.160 |    0.8745\n",
            "RIGHT  |     68 |  0.8203 | 0.222 |    0.8848\n",
            "UP     |      5 |  0.0000 | 0.239 |    0.7977\n",
            "\n",
            "Top visited paths:\n",
            "Depth 0: Action=RIGHT, Visits=68, Value=0.8203\n",
            "Depth 1: Action=DOWN, Visits=57, Value=0.9800\n",
            "{0: Node(prior=tensor(0.3787, dtype=torch.float64), action_taken=0, visit_count=8, value_sum=-0.22704900028705596, state=0, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=67.04786851206273, state=0, parent=None, children={...}), children={0: Node(prior=tensor(0.2679, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.2946, dtype=torch.float64), action_taken=1, visit_count=3, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.2284, dtype=torch.float64), action_taken=2, visit_count=2, value_sum=-0.14389528214931485, state=0, parent=..., children={0: Node(prior=tensor(0.5194, dtype=torch.float64), action_taken=0, visit_count=1, value_sum=-0.2913207411766052, state=0, parent=..., children={0: Node(prior=tensor(0.2764, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.1386, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.4610, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.1240, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 1: Node(prior=tensor(0.0906, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.2274, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.1626, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 3: Node(prior=tensor(0.2090, dtype=torch.float64), action_taken=3, visit_count=2, value_sum=0.2094786912202835, state=0, parent=..., children={0: Node(prior=tensor(0.2835, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.0642, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.3498, dtype=torch.float64), action_taken=2, visit_count=1, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.3026, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})})}), 1: Node(prior=tensor(0.1596, dtype=torch.float64), action_taken=1, visit_count=18, value_sum=12.717529067182777, state=0, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=67.04786851206273, state=0, parent=None, children={...}), children={0: Node(prior=tensor(0.1828, dtype=torch.float64), action_taken=0, visit_count=1, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.1133, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.2344, dtype=torch.float64), action_taken=2, visit_count=12, value_sum=11.760000000000003, state=0, parent=..., children={}), 3: Node(prior=tensor(0.4694, dtype=torch.float64), action_taken=3, visit_count=4, value_sum=0.23479408084154124, state=0, parent=..., children={0: Node(prior=tensor(0.2433, dtype=torch.float64), action_taken=0, visit_count=1, value_sum=-0.2913207411766052, state=0, parent=..., children={0: Node(prior=tensor(0.4782, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.2467, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.1959, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.0792, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 1: Node(prior=tensor(0.2036, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.4062, dtype=torch.float64), action_taken=2, visit_count=2, value_sum=0.3864177173376083, state=0, parent=..., children={0: Node(prior=tensor(0.3120, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.0405, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.1518, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.4957, dtype=torch.float64), action_taken=3, visit_count=1, value_sum=0.07498857378959656, state=0, parent=..., children={0: Node(prior=tensor(0.3097, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.1302, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.2337, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.3264, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})})}), 3: Node(prior=tensor(0.1469, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})})}), 2: Node(prior=tensor(0.2223, dtype=torch.float64), action_taken=2, visit_count=68, value_sum=55.781223471735835, state=0, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=67.04786851206273, state=0, parent=None, children={...}), children={0: Node(prior=tensor(0.4291, dtype=torch.float64), action_taken=0, visit_count=7, value_sum=0.6653119942839145, state=0, parent=..., children={0: Node(prior=tensor(0.2229, dtype=torch.float64), action_taken=0, visit_count=1, value_sum=-0.2913207411766052, state=0, parent=..., children={0: Node(prior=tensor(0.2853, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.1341, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.4829, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.0976, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 1: Node(prior=tensor(0.0902, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.4191, dtype=torch.float64), action_taken=2, visit_count=4, value_sum=0.8257217106461524, state=0, parent=..., children={0: Node(prior=tensor(0.3005, dtype=torch.float64), action_taken=0, visit_count=1, value_sum=0.14159904420375824, state=0, parent=..., children={0: Node(prior=tensor(0.1705, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.0903, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.4437, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.2956, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 1: Node(prior=tensor(0.0357, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.1536, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.5101, dtype=torch.float64), action_taken=3, visit_count=2, value_sum=0.38165891051292417, state=0, parent=..., children={0: Node(prior=tensor(0.2584, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.4473, dtype=torch.float64), action_taken=1, visit_count=1, value_sum=0.3129289150238037, state=0, parent=..., children={0: Node(prior=tensor(0.2957, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.3624, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.2076, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.1343, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})}), 2: Node(prior=tensor(0.2565, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.0377, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})})}), 3: Node(prior=tensor(0.2678, dtype=torch.float64), action_taken=3, visit_count=1, value_sum=0, state=0, parent=..., children={})}), 1: Node(prior=tensor(0.3003, dtype=torch.float64), action_taken=1, visit_count=57, value_sum=55.859999999999935, state=0, parent=..., children={}), 2: Node(prior=tensor(0.1515, dtype=torch.float64), action_taken=2, visit_count=2, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.1191, dtype=torch.float64), action_taken=3, visit_count=1, value_sum=0.07498857378959656, state=0, parent=..., children={0: Node(prior=tensor(0.4430, dtype=torch.float64), action_taken=0, visit_count=0, value_sum=0, state=0, parent=..., children={}), 1: Node(prior=tensor(0.1122, dtype=torch.float64), action_taken=1, visit_count=0, value_sum=0, state=0, parent=..., children={}), 2: Node(prior=tensor(0.3216, dtype=torch.float64), action_taken=2, visit_count=0, value_sum=0, state=0, parent=..., children={}), 3: Node(prior=tensor(0.1232, dtype=torch.float64), action_taken=3, visit_count=0, value_sum=0, state=0, parent=..., children={})})}), 3: Node(prior=tensor(0.2393, dtype=torch.float64), action_taken=3, visit_count=5, value_sum=0, state=0, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=67.04786851206273, state=0, parent=None, children={...}), children={})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.Replay_buffer"
      ],
      "metadata": {
        "id": "ztKvyBlNWT5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GameHistory:\n",
        "   def __init__(self):\n",
        "       self.observations = []\n",
        "       self.actions = []\n",
        "       self.mcts_policies = []\n",
        "       self.values = []\n",
        "\n",
        "   def store(self, observation, action, action_probs, value):\n",
        "       self.observations.append(observation)\n",
        "       self.actions.append(action)\n",
        "       self.mcts_policies.append(action_probs)\n",
        "       self.values.append(value)\n",
        "\n",
        "   def clear(self):\n",
        "       self.observations.clear()\n",
        "       self.actions.clear()\n",
        "       self.mcts_policies.clear()\n",
        "       self.values.clear()\n",
        "\n",
        "class ReplayBuffer:\n",
        "   def __init__(self, batch_size, minimum_size, capacity=500):\n",
        "       self.batch_size = batch_size\n",
        "       self.minimum_size = minimum_size\n",
        "       self.capacity = capacity\n",
        "       self.game_history = GameHistory()\n",
        "\n",
        "   def store(self, observation, action, action_probs, value):\n",
        "       if len(self.game_history.observations) >= self.capacity:\n",
        "           self.game_history.observations.pop(0)\n",
        "           self.game_history.actions.pop(0)\n",
        "           self.game_history.mcts_policies.pop(0)\n",
        "           self.game_history.values.pop(0)\n",
        "\n",
        "       self.game_history.store(observation, action, action_probs, value)\n",
        "\n",
        "   def sample_batch(self, batch_size=32):\n",
        "       device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "       if batch_size is None:\n",
        "           batch_size = self.batch_size\n",
        "\n",
        "       indices = np.random.choice(len(self.game_history.observations), batch_size)\n",
        "\n",
        "       observations = torch.tensor([self.game_history.observations[i] for i in indices], dtype=torch.float32)\n",
        "       actions = torch.tensor([self.game_history.actions[i] for i in indices], dtype=torch.long)\n",
        "       policies = torch.tensor([self.game_history.mcts_policies[i] for i in indices], dtype=torch.float32)\n",
        "       values = torch.tensor([self.game_history.values[i] for i in indices], dtype=torch.float32)\n",
        "\n",
        "       return observations.to(device), actions.to(device), policies.to(device), values.to(device)\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.game_history.observations)\n",
        "\n",
        "   def clear(self):\n",
        "       self.game_history.clear()\n"
      ],
      "metadata": {
        "id": "0uU1CHAaV-je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Self_play"
      ],
      "metadata": {
        "id": "QM3mmWGjQXSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_play(replay_buffer, model, epoch_of_training=0):\n",
        "   env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "   model.eval()\n",
        "\n",
        "   observation,_ = env.reset()\n",
        "   done = False\n",
        "   episode_reward = 0\n",
        "\n",
        "   trajectory = []\n",
        "   while not done:\n",
        "       print('observation',observation)\n",
        "       mcts = HybridMCTS(model)\n",
        "       root = mcts.search(observation)\n",
        "       action_probs = mcts.get_action_probs(root, temperature=1)\n",
        "       action = np.argmax(action_probs)\n",
        "       print('action',action)\n",
        "       #print('action_probs',action_probs)\n",
        "       next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "       print('next_observation',next_observation)\n",
        "\n",
        "       episode_reward += reward\n",
        "       done = terminated or truncated\n",
        "\n",
        "       trajectory.append({\n",
        "           'state': observation,\n",
        "           'action': action,\n",
        "           'action_probs': action_probs\n",
        "       })\n",
        "       observation = next_observation\n",
        "\n",
        "   # Store episode with decaying rewards\n",
        "   for idx, t in enumerate(trajectory):\n",
        "       decay_reward = episode_reward * (0.95 ** (len(trajectory) - idx - 1))\n",
        "       replay_buffer.store(\n",
        "           t['state'],\n",
        "           t['action'],\n",
        "           t['action_probs'],\n",
        "           decay_reward\n",
        "       )\n",
        "\n",
        "   print('Episode reward:', episode_reward)\n",
        "   return episode_reward"
      ],
      "metadata": {
        "id": "0Z9RrCpIXjP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 初始化组件\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Network().to(device)\n",
        "replay_buffer = ReplayBuffer(batch_size=32, minimum_size=100)\n",
        "\n",
        "# 运行一次self_play\n",
        "reward = self_play(replay_buffer, model)\n",
        "\n",
        "# 检查存储的数据\n",
        "\n",
        "observations, actions, policies, values = replay_buffer.sample_batch(10)\n",
        "print(\"Sample from buffer:\")\n",
        "print(\"States shape:\", observations.shape)\n",
        "print(\"Actions:\", actions)\n",
        "print(\"Policies shape:\", policies.shape)\n",
        "print(\"Values:\", values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9fqmPyOX8bZ",
        "outputId": "27300bf4-0cad-4ae5-d66c-e7ec3ba1ba0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 3\n",
            "next_observation 0\n",
            "observation 0\n",
            "action 2\n",
            "next_observation 1\n",
            "observation 1\n",
            "action 2\n",
            "next_observation 2\n",
            "observation 2\n",
            "action 1\n",
            "next_observation 6\n",
            "observation 6\n",
            "action 1\n",
            "next_observation 10\n",
            "observation 10\n",
            "action 1\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Sample from buffer:\n",
            "States shape: torch.Size([10])\n",
            "Actions: tensor([3, 1, 2, 2, 1, 2, 1, 2, 2, 1])\n",
            "Policies shape: torch.Size([10, 4])\n",
            "Values: tensor([0.7351, 0.6983, 0.8145, 1.0000, 0.6983, 0.8145, 0.9025, 0.8145, 0.8145,\n",
            "        0.9500])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-e5db10c1ff8d>:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  policies = torch.tensor([self.game_history.mcts_policies[i] for i in indices], dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.Training\n"
      ],
      "metadata": {
        "id": "r3ydR2y6JrLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs,lr):\n",
        "  # 创建环境\n",
        "\n",
        "  model = Network()\n",
        "  replay_buffer = ReplayBuffer(batch_size=32, minimum_size=200)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "  #  first self to fill some data into replay buffer\n",
        "\n",
        "  get_one_time_success = 0\n",
        "  while get_one_time_success == 0 :\n",
        "    get_one_time_success=self_play(replay_buffer, model)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    #self play\n",
        "    print('epoch',epoch)\n",
        "    self_play(replay_buffer, model)\n",
        "    #some setting\n",
        "    epoch_value_loss = 0\n",
        "    epoch_policy_loss = 0\n",
        "    num_batches = 4\n",
        "    for _ in range(num_batches):\n",
        "      # simple the data to get target value\n",
        "      observations, actions, policies, values = replay_buffer.sample_batch(32)\n",
        "      optimizer.zero_grad()\n",
        "      # feed observations to model\n",
        "      state_tensor =  torch.tensor(observations, dtype=torch.long)\n",
        "      one_hot_state = F.one_hot(state_tensor, num_classes=16)\n",
        "      one_hot_state = one_hot_state.float()\n",
        "      model.train()\n",
        "      policy_logits, pred_values = model(one_hot_state)\n",
        "      # get the loss\n",
        "      value_loss = F.mse_loss(pred_values.squeeze(), values)\n",
        "      probs = F.softmax(policy_logits, dim=1)\n",
        "      policy_loss = -torch.sum(policies * torch.log(probs))\n",
        "      total_loss = value_loss + policy_loss\n",
        "      # optimize\n",
        "      total_loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      epoch_value_loss += value_loss.item()\n",
        "      epoch_policy_loss += policy_loss.item()\n",
        "    # 打印训练信息\n",
        "    avg_value_loss = epoch_value_loss / num_batches\n",
        "    avg_policy_loss = epoch_policy_loss / num_batches\n",
        "    print(f\"Value Loss: {avg_value_loss:.4f}, Policy Loss: {avg_policy_loss:.4f}\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "RDG5dy4VJvrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=train(10, lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64PHsWjavss",
        "outputId": "86c9b478-cd23-46f7-9f8f-6e139ad31b5f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation 0\n",
            "action 2\n",
            "next_observation 1\n",
            "observation 1\n",
            "action 1\n",
            "next_observation 5\n",
            "Episode reward: 0.0\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "epoch 0\n",
            "observation 0\n",
            "action 2\n",
            "next_observation 1\n",
            "observation 1\n",
            "action 1\n",
            "next_observation 5\n",
            "Episode reward: 0.0\n",
            "Value Loss: 0.3721, Policy Loss: 29.4700\n",
            "epoch 1\n",
            "observation 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a0da8cbceffb>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state_tensor =  torch.tensor(observations, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0828, Policy Loss: 19.0769\n",
            "epoch 2\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0597, Policy Loss: 19.6469\n",
            "epoch 3\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0321, Policy Loss: 17.8768\n",
            "epoch 4\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0414, Policy Loss: 16.4271\n",
            "epoch 5\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0290, Policy Loss: 15.5056\n",
            "epoch 6\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0238, Policy Loss: 16.7638\n",
            "epoch 7\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0327, Policy Loss: 18.0412\n",
            "epoch 8\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0285, Policy Loss: 15.7008\n",
            "epoch 9\n",
            "observation 0\n",
            "action 1\n",
            "next_observation 4\n",
            "observation 4\n",
            "action 1\n",
            "next_observation 8\n",
            "observation 8\n",
            "action 2\n",
            "next_observation 9\n",
            "observation 9\n",
            "action 1\n",
            "next_observation 13\n",
            "observation 13\n",
            "action 2\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n",
            "Value Loss: 0.0210, Policy Loss: 15.1797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化组件\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "replay_buffer = ReplayBuffer(batch_size=32, minimum_size=100)\n",
        "\n",
        "# 运行一次self_play\n",
        "reward = self_play(replay_buffer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_QwXKxgeRxEc",
        "outputId": "06cb0690-7bc6-48ee-c46d-3d0bd5660d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation 0\n",
            "action 2\n",
            "next_observation 1\n",
            "observation 1\n",
            "action 2\n",
            "next_observation 2\n",
            "observation 2\n",
            "action 1\n",
            "next_observation 6\n",
            "observation 6\n",
            "action 1\n",
            "next_observation 10\n",
            "observation 10\n",
            "action 1\n",
            "next_observation 14\n",
            "observation 14\n",
            "action 2\n",
            "next_observation 15\n",
            "Episode reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_path_with_mcts(start_state=0, num_simulations=100, max_steps=50, model=Network()):\n",
        "    \"\"\"使用MCTS直接寻找从起点到终点的路径\"\"\"\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "    mcts = HybridMCTS(model, num_simulations=num_simulations)\n",
        "    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
        "\n",
        "    # 重置环境到起始状态\n",
        "    env.reset()\n",
        "    env.unwrapped.s = start_state\n",
        "    state = start_state\n",
        "\n",
        "    path = []\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    print(\"\\nFrozenLake地图:\")\n",
        "    desc = env.unwrapped.desc\n",
        "    for row in desc:\n",
        "        print(\"\".join([c.decode('utf-8') for c in row]))\n",
        "\n",
        "    print(f\"\\n开始寻路，起始状态: {start_state}\")\n",
        "\n",
        "    while steps < max_steps:\n",
        "        # 执行MCTS搜索\n",
        "        root = mcts.search(state)\n",
        "\n",
        "        # 获取最佳动作\n",
        "        action_probs = mcts.get_action_probs(root, temperature=1)\n",
        "        action = np.argmax(action_probs)\n",
        "\n",
        "        # 记录访问统计\n",
        "        print(f\"\\n步骤 {steps + 1}:\")\n",
        "        print(f\"当前状态: {state}\")\n",
        "        print(f\"选择动作: {action_names[action]}\")\n",
        "        print(\"各动作访问次数:\")\n",
        "        for act, child in root.children.items():\n",
        "            print(f\"{action_names[act]}: {child.visit_count} 访问, Q值: {child.Q_value:.4f}\")\n",
        "\n",
        "        # 执行动作\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        print(f\"next_state: {next_state}\")\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # 更新路径信息\n",
        "        path.append((state, action))\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        # 检查是否到达目标\n",
        "        if done:\n",
        "            path.append((state, None))  # 添加最终状态\n",
        "            if reward > 0:\n",
        "                print(f\"\\n成功找到路径! 总步数: {steps}\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"\\n失败! 掉入陷阱. 总步数: {steps}\")\n",
        "                path = None\n",
        "                break\n",
        "\n",
        "    # 可视化最终路径\n",
        "    if path:\n",
        "        print(\"\\n找到的路径:\")\n",
        "        for i, (s, a) in enumerate(path):\n",
        "            if a is not None:\n",
        "                print(f\"Step {i + 1}: State {s} -> Action {action_names[a]}\")\n",
        "            else:\n",
        "                print(f\"Final State: {s}\")\n",
        "\n",
        "        # 在网格上显示路径\n",
        "        grid = [['.' for _ in range(4)] for _ in range(4)]\n",
        "        for i, row in enumerate(env.unwrapped.desc):\n",
        "            for j, cell in enumerate(row):\n",
        "                grid[i][j] = cell.decode('utf-8')\n",
        "\n",
        "        path_states = [p[0] for p in path]\n",
        "        for state in path_states[:-1]:  # 除了最后一个状态\n",
        "            row, col = state // 4, state % 4\n",
        "            if grid[row][col] == 'F':  # 只在安全的格子上标记\n",
        "                grid[row][col] = '*'\n",
        "\n",
        "        print(\"\\n路径可视化 (* 表示路径):\")\n",
        "        for row in grid:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "    return path, total_reward\n",
        "\n",
        "# 使用示例\n",
        "#model = Network()\n",
        "\n",
        "path, reward = find_path_with_mcts(0, 100,10, model)\n",
        "if path:\n",
        "    print(f\"\\n总奖励: {reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_N6VIKlcRFM",
        "outputId": "6602dd9d-cf4f-4809-e8c0-1579b31ec691",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FrozenLake地图:\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "开始寻路，起始状态: 0\n",
            "\n",
            "步骤 1:\n",
            "当前状态: 0\n",
            "选择动作: DOWN\n",
            "各动作访问次数:\n",
            "LEFT: 0 访问, Q值: 0.0000\n",
            "DOWN: 80 访问, Q值: 0.7896\n",
            "RIGHT: 19 访问, Q值: 0.5358\n",
            "UP: 0 访问, Q值: 0.0000\n",
            "next_state: 4\n",
            "\n",
            "步骤 2:\n",
            "当前状态: 4\n",
            "选择动作: DOWN\n",
            "各动作访问次数:\n",
            "LEFT: 0 访问, Q值: 0.0000\n",
            "DOWN: 92 访问, Q值: 0.8954\n",
            "RIGHT: 1 访问, Q值: 0.0000\n",
            "UP: 6 访问, Q值: 0.7989\n",
            "next_state: 8\n",
            "\n",
            "步骤 3:\n",
            "当前状态: 8\n",
            "选择动作: RIGHT\n",
            "各动作访问次数:\n",
            "LEFT: 0 访问, Q值: 0.0000\n",
            "DOWN: 5 访问, Q值: 0.0000\n",
            "RIGHT: 82 访问, Q值: 0.8970\n",
            "UP: 12 访问, Q值: 0.8182\n",
            "next_state: 9\n",
            "\n",
            "步骤 4:\n",
            "当前状态: 9\n",
            "选择动作: DOWN\n",
            "各动作访问次数:\n",
            "LEFT: 17 访问, Q值: 0.7829\n",
            "DOWN: 74 访问, Q值: 0.8930\n",
            "RIGHT: 7 访问, Q值: 0.7725\n",
            "UP: 1 访问, Q值: 0.0000\n",
            "next_state: 13\n",
            "\n",
            "步骤 5:\n",
            "当前状态: 13\n",
            "选择动作: RIGHT\n",
            "各动作访问次数:\n",
            "LEFT: 0 访问, Q值: 0.0000\n",
            "DOWN: 0 访问, Q值: 0.0000\n",
            "RIGHT: 93 访问, Q值: 0.9800\n",
            "UP: 6 访问, Q值: 0.7928\n",
            "next_state: 14\n",
            "\n",
            "步骤 6:\n",
            "当前状态: 14\n",
            "选择动作: RIGHT\n",
            "各动作访问次数:\n",
            "LEFT: 0 访问, Q值: 0.0000\n",
            "DOWN: 0 访问, Q值: 0.0000\n",
            "RIGHT: 1 访问, Q值: 1.0000\n",
            "UP: 0 访问, Q值: 0.0000\n",
            "next_state: 15\n",
            "\n",
            "成功找到路径! 总步数: 6\n",
            "\n",
            "找到的路径:\n",
            "Step 1: State 0 -> Action DOWN\n",
            "Step 2: State 4 -> Action DOWN\n",
            "Step 3: State 8 -> Action RIGHT\n",
            "Step 4: State 9 -> Action DOWN\n",
            "Step 5: State 13 -> Action RIGHT\n",
            "Step 6: State 14 -> Action RIGHT\n",
            "Final State: 15\n",
            "\n",
            "路径可视化 (* 表示路径):\n",
            "S F F F\n",
            "* H F H\n",
            "* * F H\n",
            "H * * G\n",
            "\n",
            "总奖励: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.some ideas"
      ],
      "metadata": {
        "id": "H8DYFXNlQsAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, lr, batch_size=32):\n",
        "    model = Network()\n",
        "    replay_buffer = ReplayBuffer(batch_size=batch_size, minimum_size=200)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "    best_episode_reward = 0\n",
        "\n",
        "    # 初始填充经验池\n",
        "    print(\"Filling replay buffer with initial experiences...\")\n",
        "    for i in range(4):\n",
        "        episode_reward = self_play(replay_buffer, model)\n",
        "        print(f\"Initial episode {i+1}: reward = {episode_reward}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # 1. Self-play阶段\n",
        "        episode_reward = self_play(replay_buffer, model)\n",
        "        print(f\"Episode reward: {episode_reward}\")\n",
        "\n",
        "        # 2. 训练阶段\n",
        "        epoch_value_loss = 0\n",
        "        epoch_policy_loss = 0\n",
        "        num_batches = 4  # 每个epoch训练多个batch\n",
        "\n",
        "        model.train()\n",
        "        for _ in range(num_batches):\n",
        "            # 采样batch数据\n",
        "            observations, actions, policies, values = replay_buffer.sample_batch(batch_size)\n",
        "\n",
        "            # 转换状态表示\n",
        "            state_tensor = torch.tensor(observations, dtype=torch.long)  # 确保是LongTensor\n",
        "            one_hot_state = F.one_hot(state_tensor, num_classes=16).float()\n",
        "\n",
        "            # 前向传播\n",
        "            optimizer.zero_grad()\n",
        "            policy_logits, pred_values = model(one_hot_state)\n",
        "\n",
        "            # 计算损失\n",
        "            value_loss = F.mse_loss(pred_values.squeeze(), values)\n",
        "            probs = F.softmax(policy_logits, dim=1)\n",
        "            policy_loss = -torch.sum(policies * torch.log_softmax(policy_logits, dim=1))\n",
        "\n",
        "            # 添加L2正则化和熵正则化\n",
        "            l2_reg = 0.001\n",
        "            l2_loss = 0\n",
        "            for param in model.parameters():\n",
        "                l2_loss += torch.norm(param)\n",
        "\n",
        "            policy_entropy = -(probs * torch.log(probs + 1e-10)).sum(1).mean()\n",
        "            entropy_reg = 0.01\n",
        "\n",
        "            total_loss = value_loss + policy_loss + l2_reg * l2_loss - entropy_reg * policy_entropy\n",
        "\n",
        "            # 反向传播\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # 记录损失\n",
        "            epoch_value_loss += value_loss.item()\n",
        "            epoch_policy_loss += policy_loss.item()\n",
        "\n",
        "        # 学习率调整\n",
        "        scheduler.step()\n",
        "\n",
        "        # 打印训练信息\n",
        "        avg_value_loss = epoch_value_loss / num_batches\n",
        "        avg_policy_loss = epoch_policy_loss / num_batches\n",
        "        print(f\"Value Loss: {avg_value_loss:.4f}, Policy Loss: {avg_policy_loss:.4f}\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if episode_reward > best_episode_reward:\n",
        "            best_episode_reward = episode_reward\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"New best model saved with reward: {best_episode_reward}\")\n",
        "\n",
        "        # 每隔几个epoch进行评估\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model.eval()\n",
        "            eval_reward = evaluate(model)\n",
        "            print(f\"Evaluation reward: {eval_reward}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, num_episodes=5):\n",
        "    \"\"\"评估模型性能\"\"\"\n",
        "    model.eval()\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    total_rewards = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = F.one_hot(torch.tensor(state), num_classes=16).float()\n",
        "                policy_logits, _ = model(state_tensor)\n",
        "                action = torch.argmax(policy_logits).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards += episode_reward\n",
        "\n",
        "    return total_rewards / num_episodes"
      ],
      "metadata": {
        "id": "Te4TfwOuZ8BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_hybrid_mcts():\n",
        "    # 创建环境\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "    # 创建神经网络模型\n",
        "    model = Network()\n",
        "\n",
        "    # 创建HybridMCTS实例\n",
        "    mcts = HybridMCTS(model=model, num_simulations=100, local_search_depth=3)\n",
        "\n",
        "    # 添加环境到MCTS实例\n",
        "    mcts.env = env\n",
        "\n",
        "    # 设置其他必要的参数\n",
        "    mcts.epsilon = 0.25  # Dirichlet噪声参数\n",
        "\n",
        "    return env, model, mcts\n",
        "\n",
        "# 3. 实现搜索和动作选择的主循环\n",
        "def play_episode(env, mcts, model, render=False):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # 将当前状态转换为one-hot编码\n",
        "        state_tensor = F.one_hot(torch.tensor(state), num_classes=16).float()\n",
        "\n",
        "        # 执行MCTS搜索\n",
        "        root = mcts.search(state)\n",
        "\n",
        "        # 获取动作概率\n",
        "        action_probs = mcts.get_action_probs(root, temperature=1.0)\n",
        "\n",
        "        # 选择动作\n",
        "        action = np.argmax(action_probs)\n",
        "\n",
        "        # 执行动作\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# 4. 测试代码\n",
        "def test_hybrid_mcts():\n",
        "    env, model, mcts = setup_hybrid_mcts()\n",
        "\n",
        "    # 运行几个测试episode\n",
        "    for episode in range(5):\n",
        "        reward = play_episode(env, mcts, model, render=True)\n",
        "        print(f\"Episode {episode}: Total Reward = {reward}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_hybrid_mcts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dM6mjNN_i95",
        "outputId": "fb3c81c2-ab95-4cbd-e307-2e84c207097c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward = 0.0\n",
            "Episode 1: Total Reward = 0\n",
            "Episode 2: Total Reward = 0.0\n",
            "Episode 3: Total Reward = 0\n",
            "Episode 4: Total Reward = 0.0\n"
          ]
        }
      ]
    }
  ]
}