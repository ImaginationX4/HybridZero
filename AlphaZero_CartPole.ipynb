{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero-/blob/main/AlphaZero_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1XJwNxUtu2F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Network"
      ],
      "metadata": {
        "id": "E-v-uMnc-adK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3_bqbsLMHkI"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
        "      super(Network, self).__init__()\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "      self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "      #self.policy_head = nn.Linear(hidden_size, output_size)\n",
        "      #torch.nn.init.uniform_(self.policy_head.weight, -0.1, 0.1)\n",
        "      self.value_head = nn.Linear(hidden_size, 1)\n",
        "      self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, output_size)\n",
        "        )\n",
        "      self.v_net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, 1)\n",
        "            #nn.Tanh()\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        x = torch.FloatTensor(x)\n",
        "\n",
        "    x = x.to(self.device)\n",
        "    policy = self.net(x)\n",
        "    value = self.v_net(x)\n",
        "    #x = F.leaky_relu(self.fc1(x))\n",
        "    #x = F.leaky_relu(self.fc2(x))\n",
        "    #policy_logits = 1.0 * self.policy_head(x)\n",
        "    #policy = torch.softmax(policy_logits, dim=-1)\n",
        "    #policy = policy_logits\n",
        "\n",
        "\n",
        "    #value =  torch.tanh(self.value_head(x))\n",
        "    #value = (value + 1) * 20\n",
        "    return policy, value\n",
        "\n",
        "  def save(self, filepath):\n",
        "    torch.save(self.state_dict(), filepath)\n",
        "    print(f\"Model saved to {filepath}\")\n",
        "\n",
        "  def load(self, filepath):\n",
        "    self.load_state_dict(torch.load(filepath, map_location=self.device))\n",
        "    print(f\"Model loaded from {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net_w(nn.Module):\n",
        "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
        "        super(Net_w, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),  # 加入BN层帮助训练\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.FloatTensor(x)\n",
        "        logits = self.net(x)\n",
        "        # 用temperature参数调节softmax的平滑程度\n",
        "        temperature = 1\n",
        "        return logits / temperature"
      ],
      "metadata": {
        "id": "iQ2KKuAJCdZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gvpvu0FpM0Oz",
        "outputId": "2af2105d-80db-42a5-d4b9-061064c36206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how to set env\n"
      ],
      "metadata": {
        "id": "rstpF5Xe97KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6InjNGWRMoRR",
        "outputId": "fca58bad-1a5a-4e01-e8cb-961c39919ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation [ 0.00363835 -0.01579056  0.0161241  -0.00074579]\n",
            "current_state [ 0.00363835 -0.01579056  0.0161241  -0.00074579]\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# 创建环境\n",
        "env = gym.make('CartPole-v1')\n",
        "a,_=env.reset()\n",
        "print('observation',a)\n",
        "# 1. 保存当前状态\n",
        "current_state = env.unwrapped.state.copy()  # [x, x_dot, theta, theta_dot]\n",
        "print('current_state',current_state)\n",
        "# 2. 设置到特定状态\n",
        "# 比如，将小车放在中间，静止，杆子稍微倾斜\n",
        "desired_state = np.array([0.0,  # 位置 cart position (x)\n",
        "                         0.0,  # 速度 cart velocity (x_dot)\n",
        "                         0.1,  # 杆子角度 pole angle (theta) - 略微倾斜\n",
        "                         0.0]) # 角速度 pole angular velocity (theta_dot)\n",
        "\n",
        "env.unwrapped.state = desired_state\n",
        "\n",
        "# 3. 从这个状态开始执行动作\n",
        "observation, reward, terminated, truncated, info = env.step(1)  # 1: 向右推\n",
        "\n",
        "# 4. 如果需要回到之前保存的状态\n",
        "env.unwrapped.state = current_state.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690kFxTpJT86",
        "outputId": "3b8e4cfd-687b-48e2-be5f-2f5ae1aa91bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observations [-0.00716547  0.04397519 -0.00126024  0.00752538]\n",
            "[ 0.37035656 -0.3788243 ]\n",
            "value tensor([0.7007], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "a= Network().to(device)\n",
        "env = gym.make('CartPole-v1')\n",
        "observations,_ = env.reset()\n",
        "print('observations',observations)\n",
        "observations = torch.FloatTensor(observations)\n",
        "\n",
        "policy,value = a(observations)\n",
        "print(policy.cpu().detach().numpy())\n",
        "print('value',value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.MCTS"
      ],
      "metadata": {
        "id": "BuzHh8Of-BfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TBMbLn3_coU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "\n",
        "    prior: float  #  P(s,a)\n",
        "    action_taken: Optional[int]\n",
        "    visit_count: int = 0  # N(s,a)\n",
        "    value_sum: float = 0  # Q=value_sum/visit_count\n",
        "    parent: Optional['Node'] = None\n",
        "    children: Dict[int, 'Node'] = field(default_factory=dict)\n",
        "    done = False\n",
        "    has_children = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.children is None:\n",
        "            self.children = {}\n",
        "    @property\n",
        "    def Q_value(self) -> float:\n",
        "\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, num_simulations: int = 100, epoch_of_training=0):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "        self.c_puct =max(10 * (1 - epoch_of_training/10), 1) # UCB exploration constant\n",
        "        self.epsilon = max(0.25 * (1 - epoch_of_training/10), 0)\n",
        "\n",
        "    def search(self, root_state) -> np.ndarray:\n",
        "      root = Node(prior=1.0, action_taken=None)\n",
        "\n",
        "      for _ in range(self.num_simulations):\n",
        "        node = root\n",
        "        env = gym.make('CartPole-v1')\n",
        "        env.reset()\n",
        "        state = root_state\n",
        "        env.unwrapped.state = state\n",
        "        search_path = [node]\n",
        "        rewards = []\n",
        "        # 1. Selection\n",
        "        while node.has_children and not node.done:\n",
        "\n",
        "\n",
        "          action, node = self.select_child(node)\n",
        "          observation, reward, terminated, truncated, info = env.step(action)\n",
        "          rewards.append(reward)\n",
        "          node.done = terminated or truncated\n",
        "          state = observation\n",
        "          search_path.append(node)\n",
        "\n",
        "        # 2. Expansion and Evaluation\n",
        "        value = 0\n",
        "        if not node.done:\n",
        "\n",
        "          policy, value = self.evaluate_state(state, node.done)\n",
        "\n",
        "\n",
        "          self.expand_node(node, policy)\n",
        "\n",
        "        # 3. Backup\n",
        "        self.backup(search_path, value, rewards)\n",
        "\n",
        "      visits = np.array([child.visit_count for child in root.children.values()])\n",
        "      total_visits = np.sum(visits)\n",
        "      action_probs = visits / total_visits\n",
        "\n",
        "\n",
        "      '''print(\"\\nMCTS搜索结果：\")\n",
        "      for action, prob in enumerate(action_probs):\n",
        "          print(f\"动作 {action}: 访问次数 = {visits[action]}, 概率 = {prob:.3f}\")'''\n",
        "\n",
        "\n",
        "\n",
        "      return root\n",
        "\n",
        "    def select_child(self, node: Node) -> Tuple[int, Node]:\n",
        "\n",
        "\n",
        "      best_score = -float('inf')\n",
        "      best_action = -1\n",
        "      best_child = None\n",
        "\n",
        "\n",
        "      sqrt_total_count = math.sqrt(node.visit_count)\n",
        "\n",
        "      def calculate_ucb(node: Node, child: Node, c_puct: float = self.c_puct) -> float:\n",
        "        pb_c = np.log((node.visit_count + c_puct + 1) / c_puct) + c_puct\n",
        "        pb_c *= np.sqrt(node.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "        prior_score = pb_c * child.prior\n",
        "        value_score = child.Q_value\n",
        "\n",
        "        return value_score + prior_score #+ depth_factor\n",
        "      for action, child in node.children.items():\n",
        "        # UCB score = Q + U\n",
        "        # Q\n",
        "        # U = c_puct * P * sqrt(N(s)) / (1 + N(s,a))\n",
        "        Q = child.Q_value\n",
        "        U = self.c_puct * child.prior * sqrt_total_count / (1 + child.visit_count)\n",
        "        #print('Q',Q)\n",
        "        #print('U',U)\n",
        "        ucb_score = calculate_ucb(node, child) #Q + U\n",
        "        #exploration_bonus = 20.0 * math.sqrt(2.0 * math.log(node.visit_count + 1) / (child.visit_count + 1))\n",
        "        #ucb_score += exploration_bonus\n",
        "        if ucb_score > best_score:\n",
        "            best_score = ucb_score\n",
        "            best_action = action\n",
        "            best_child = child\n",
        "\n",
        "      return best_action, best_child\n",
        "\n",
        "\n",
        "    def evaluate_state(self, state: np.ndarray, done) -> Tuple[np.ndarray, float]:\n",
        "        if done:\n",
        "\n",
        "          return np.zeros(2), -1.0\n",
        "        with torch.no_grad():\n",
        "\n",
        "\n",
        "          state_t = torch.FloatTensor(state)\n",
        "          policy, value = self.model(state_t)\n",
        "\n",
        "          policy = F.softmax(policy, dim=0)\n",
        "\n",
        "          return policy.cpu().detach().numpy(), value.item()\n",
        "\n",
        "    def expand_node(self, node: Node, policy: np.ndarray):\n",
        "\n",
        "      noise = np.random.dirichlet([0.3] * len(policy))\n",
        "      #policy = 0.75 * policy + 0.25 * noise\n",
        "\n",
        "\n",
        "      policy = (1 - self.epsilon) * policy + self.epsilon * noise\n",
        "\n",
        "      for action, prob in enumerate(policy):\n",
        "\n",
        "        child = Node(\n",
        "            prior=prob,\n",
        "            action_taken=action,\n",
        "            parent=node\n",
        "        )\n",
        "\n",
        "        node.children[action] = child\n",
        "\n",
        "        node.has_children = True\n",
        "\n",
        "    def backup(self, search_path: List[Node], value: float, rewards: List[float]):\n",
        "      # 从叶子节点开始，向上传播\n",
        "      G = value  # 最后一个状态的值估计\n",
        "      #print('search_path len',len(search_path))\n",
        "\n",
        "      if len(rewards)<500 and len(rewards)>0:\n",
        "        rewards[-1]=-1\n",
        "\n",
        "      #print('rewards',rewards)\n",
        "\n",
        "      #print(\"\\nBackup过程:\")\n",
        "      for node in reversed(search_path):\n",
        "        # 如果有对应的奖励，用实际奖励更新G\n",
        "        if rewards:  # 还有未使用的奖励\n",
        "          r = rewards.pop()  # 获取当前步骤的奖励\n",
        "          survival_bonus = 0.01\n",
        "          r+= survival_bonus\n",
        "          G = r + 0.99 * G  # G = 即时奖励 + 折扣 * 未来价值\n",
        "          #print('G',G)\n",
        "\n",
        "        # 更新节点统计\n",
        "        old_value = node.value_sum / max(1, node.visit_count)\n",
        "        node.visit_count += 1\n",
        "\n",
        "        decay = 0.95\n",
        "        node.value_sum = decay * node.value_sum + G\n",
        "        new_value = node.value_sum / node.visit_count\n",
        "\n",
        "        '''print(f\"节点更新:\")\n",
        "        print(f\"  旧值: {old_value:.3f}\")\n",
        "        print(f\"  新值: {new_value:.3f}\")\n",
        "        print(f\"  访问次数: {node.visit_count}\")\n",
        "        print(f\"  使用的G值: {G:.3f}\")'''\n",
        "\n",
        "    def backup1(self, search_path: List[Node], value: float):\n",
        "      for idx, node in enumerate(reversed(search_path)):\n",
        "        if idx == 0:\n",
        "          discount = 1.0\n",
        "        else:\n",
        "          discount = 0.99 ** idx\n",
        "        node.visit_count += 1\n",
        "        node.value_sum += value * discount\n",
        "\n",
        "\n",
        "    def get_action_probs(self, root: Node, temperature: float = 0.0) -> np.ndarray:\n",
        "\n",
        "        counts = np.array([child.visit_count for child in root.children.values()])\n",
        "        if temperature == 0:\n",
        "            probs = np.zeros_like(counts)\n",
        "            probs[np.argmax(counts)] = 1\n",
        "            return probs\n",
        "        else:\n",
        "            counts = counts ** (1.0 / temperature)\n",
        "            probs = counts / np.sum(counts)\n",
        "            return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e9ZTmsMUiv",
        "outputId": "f90038b6-21c6-485a-858c-4e4d39fb9fa4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root value 1.2455006518617993\n",
            "root visit 100\n",
            "root children left visit Node(prior=0.567622721195221, action_taken=0, visit_count=84, value_sum=187.68598482206252, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=124.55006518617994, parent=None, children={0: ..., 1: Node(prior=0.4323772192001343, action_taken=1, visit_count=15, value_sum=19.36107507304724, parent=..., children={0: Node(prior=0.5361707210540771, action_taken=0, visit_count=13, value_sum=11.237434869016916, parent=..., children={0: Node(prior=0.5694295763969421, action_taken=0, visit_count=11, value_sum=4.198807600505871, parent=..., children={0: Node(prior=0.47154173254966736, action_taken=0, visit_count=3, value_sum=-3.2907117472940683, parent=..., children={0: Node(prior=0.3857184052467346, action_taken=0, visit_count=1, value_sum=-1.7807540452480315, parent=..., children={0: Node(prior=0.36542534828186035, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6345745921134949, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6142815947532654, action_taken=1, visit_count=1, value_sum=-1.9129426288604736, parent=..., children={0: Node(prior=0.4659036695957184, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5340963006019592, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5284582376480103, action_taken=1, visit_count=7, value_sum=-0.5855545580628474, parent=..., children={0: Node(prior=0.5684077739715576, action_taken=0, visit_count=5, value_sum=-2.975070796689784, parent=..., children={0: Node(prior=0.4672994911670685, action_taken=0, visit_count=1, value_sum=-1.8786221659183502, parent=..., children={0: Node(prior=0.38593369722366333, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6140663027763367, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5327004790306091, action_taken=1, visit_count=3, value_sum=-3.596926851428747, parent=..., children={0: Node(prior=0.5679386854171753, action_taken=0, visit_count=1, value_sum=-1.92354434967041, parent=..., children={0: Node(prior=0.46206292510032654, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5379371047019958, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4320612847805023, action_taken=1, visit_count=1, value_sum=-2.051801780462265, parent=..., children={0: Node(prior=0.5399029850959778, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4600970447063446, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4315922260284424, action_taken=1, visit_count=1, value_sum=-2.0537051618099214, parent=..., children={0: Node(prior=0.538829505443573, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4611704647541046, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4305703639984131, action_taken=1, visit_count=1, value_sum=-2.055764561891556, parent=..., children={0: Node(prior=0.5388778448104858, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.46112218499183655, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.4638291895389557, action_taken=1, visit_count=1, value_sum=-2.1793695187568662, parent=..., children={0: Node(prior=0.4556121826171875, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5443878173828125, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), children={0: Node(prior=0.474322110414505, action_taken=0, visit_count=7, value_sum=-0.2695752017196349, parent=..., children={0: Node(prior=0.38621020317077637, action_taken=0, visit_count=1, value_sum=-1.780452629327774, parent=..., children={0: Node(prior=0.36519503593444824, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6348049640655518, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6137897968292236, action_taken=1, visit_count=5, value_sum=-2.907578662968862, parent=..., children={0: Node(prior=0.467843234539032, action_taken=0, visit_count=1, value_sum=-1.8740499436855316, parent=..., children={0: Node(prior=0.38670608401298523, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6132939457893372, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5321568250656128, action_taken=1, visit_count=3, value_sum=-3.548915843714475, parent=..., children={0: Node(prior=0.5639476776123047, action_taken=0, visit_count=1, value_sum=-1.9062996768951415, parent=..., children={0: Node(prior=0.4590819478034973, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5409180521965027, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4360523521900177, action_taken=1, visit_count=1, value_sum=-2.0379181194305422, parent=..., children={0: Node(prior=0.5366846919059753, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.46331527829170227, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.5256779193878174, action_taken=1, visit_count=76, value_sum=197.57110557717525, parent=..., children={0: Node(prior=0.5667717456817627, action_taken=0, visit_count=71, value_sum=197.39992449844428, parent=..., children={0: Node(prior=0.4691005349159241, action_taken=0, visit_count=4, value_sum=-2.9416348691850605, parent=..., children={0: Node(prior=0.38625043630599976, action_taken=0, visit_count=1, value_sum=-1.773827143907547, parent=..., children={0: Node(prior=0.36738595366477966, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6326140761375427, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6137495040893555, action_taken=1, visit_count=2, value_sum=-2.6602006115436554, parent=..., children={0: Node(prior=0.460921972990036, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5390780568122864, action_taken=1, visit_count=1, value_sum=-1.8806425023078919, parent=..., children={0: Node(prior=0.5634855031967163, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43651440739631653, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.5308994054794312, action_taken=1, visit_count=66, value_sum=196.62078161817018, parent=..., children={0: Node(prior=0.5664531588554382, action_taken=0, visit_count=62, value_sum=192.01653732918504, parent=..., children={0: Node(prior=0.4621329605579376, action_taken=0, visit_count=3, value_sum=-3.257522531522512, parent=..., children={0: Node(prior=0.386139839887619, action_taken=0, visit_count=1, value_sum=-1.768183207511902, parent=..., children={0: Node(prior=0.370535284280777, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6294646859169006, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6138601899147034, action_taken=1, visit_count=1, value_sum=-1.8995561742782594, parent=..., children={0: Node(prior=0.4558551609516144, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5441448092460632, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.53786700963974, action_taken=1, visit_count=58, value_sum=185.98673591363294, parent=..., children={0: Node(prior=0.5661457180976868, action_taken=0, visit_count=54, value_sum=184.36182951181584, parent=..., children={0: Node(prior=0.45730018615722656, action_taken=0, visit_count=3, value_sum=-3.2435329908317323, parent=..., children={0: Node(prior=0.3871479332447052, action_taken=0, visit_count=1, value_sum=-1.760520350933075, parent=..., children={0: Node(prior=0.37359684705734253, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6264031529426575, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6128520369529724, action_taken=1, visit_count=1, value_sum=-1.9006603431701659, parent=..., children={0: Node(prior=0.45179301500320435, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5482069253921509, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5426997542381287, action_taken=1, visit_count=50, value_sum=182.6732736809449, parent=..., children={0: Node(prior=0.5655089616775513, action_taken=0, visit_count=47, value_sum=173.92152005111706, parent=..., children={0: Node(prior=0.45416057109832764, action_taken=0, visit_count=2, value_sum=-2.6340929181098938, parent=..., children={0: Node(prior=0.3880867660045624, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.61191326379776, action_taken=1, visit_count=1, value_sum=-1.9021841812133788, parent=..., children={0: Node(prior=0.4530058801174164, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.546994149684906, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5458393692970276, action_taken=1, visit_count=44, value_sum=165.18676130967657, parent=..., children={0: Node(prior=0.5632600784301758, action_taken=0, visit_count=41, value_sum=157.14880818497903, parent=..., children={0: Node(prior=0.4531548321247101, action_taken=0, visit_count=2, value_sum=-2.6158942710280417, parent=..., children={0: Node(prior=0.39108338952064514, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6089165806770325, action_taken=1, visit_count=1, value_sum=-1.9009556221961974, parent=..., children={0: Node(prior=0.4518287181854248, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5481712818145752, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5468451380729675, action_taken=1, visit_count=38, value_sum=149.17124474465314, parent=..., children={0: Node(prior=0.5613667368888855, action_taken=0, visit_count=36, value_sum=133.93829499396546, parent=..., children={0: Node(prior=0.45308035612106323, action_taken=0, visit_count=2, value_sum=-2.599102846837044, parent=..., children={0: Node(prior=0.39519229531288147, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6048077344894409, action_taken=1, visit_count=1, value_sum=-1.8991761589050293, parent=..., children={0: Node(prior=0.44910067319869995, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5508992671966553, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5469196438789368, action_taken=1, visit_count=33, value_sum=126.0898246103666, parent=..., children={0: Node(prior=0.5565626621246338, action_taken=0, visit_count=31, value_sum=111.68968972867955, parent=..., children={0: Node(prior=0.4505053460597992, action_taken=0, visit_count=1, value_sum=-1.8031083905696867, parent=..., children={0: Node(prior=0.3975910246372223, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6024089455604553, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5494946837425232, action_taken=1, visit_count=29, value_sum=97.61472461793771, parent=..., children={0: Node(prior=0.5513266324996948, action_taken=0, visit_count=27, value_sum=83.99711113166781, parent=..., children={0: Node(prior=0.44654417037963867, action_taken=0, visit_count=1, value_sum=-1.7907626116275788, parent=..., children={0: Node(prior=0.4006522595882416, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.599347710609436, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5534558296203613, action_taken=1, visit_count=25, value_sum=70.82040749978498, parent=..., children={0: Node(prior=0.5400351285934448, action_taken=0, visit_count=23, value_sum=58.24515871496024, parent=..., children={0: Node(prior=0.4526875615119934, action_taken=0, visit_count=1, value_sum=-1.779567972421646, parent=..., children={0: Node(prior=0.40502604842185974, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5949739813804626, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5473124980926514, action_taken=1, visit_count=21, value_sum=46.24878884289697, parent=..., children={0: Node(prior=0.5300087928771973, action_taken=0, visit_count=19, value_sum=35.037917144565064, parent=..., children={0: Node(prior=0.45844611525535583, action_taken=0, visit_count=1, value_sum=-1.7725157368183135, parent=..., children={0: Node(prior=0.4112602770328522, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5887397527694702, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.541553795337677, action_taken=1, visit_count=17, value_sum=24.56597839929125, parent=..., children={0: Node(prior=0.5183643102645874, action_taken=0, visit_count=14, value_sum=16.765017798750208, parent=..., children={0: Node(prior=0.4629034698009491, action_taken=0, visit_count=1, value_sum=-1.761679515838623, parent=..., children={0: Node(prior=0.41444775462150574, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5855522751808167, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5370965600013733, action_taken=1, visit_count=12, value_sum=8.875744273292883, parent=..., children={0: Node(prior=0.4963553845882416, action_taken=0, visit_count=9, value_sum=3.287691020903633, parent=..., children={0: Node(prior=0.46623092889785767, action_taken=0, visit_count=1, value_sum=-1.764437223672867, parent=..., children={0: Node(prior=0.4166909456253052, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5833090543746948, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5337690114974976, action_taken=1, visit_count=7, value_sum=-1.1561865229578094, parent=..., children={0: Node(prior=0.5005074143409729, action_taken=0, visit_count=3, value_sum=-2.823975, parent=..., children={}), 1: Node(prior=0.4994926452636719, action_taken=1, visit_count=3, value_sum=-2.823975, parent=..., children={})})}), 1: Node(prior=0.5036445260047913, action_taken=1, visit_count=2, value_sum=-2.5739504247188565, parent=..., children={0: Node(prior=0.5495519638061523, action_taken=0, visit_count=1, value_sum=-1.7973179948329925, parent=..., children={0: Node(prior=0.49999070167541504, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5000093579292297, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45044803619384766, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4816356599330902, action_taken=1, visit_count=2, value_sum=-2.6495756814718243, parent=..., children={0: Node(prior=0.54984050989151, action_taken=0, visit_count=1, value_sum=-1.8399526512622835, parent=..., children={0: Node(prior=0.4974624514579773, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5025375485420227, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4501595199108124, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.46999120712280273, action_taken=1, visit_count=1, value_sum=-1.9580119800567628, parent=..., children={0: Node(prior=0.5585036277770996, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4414963722229004, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.45996490120887756, action_taken=1, visit_count=1, value_sum=-1.9790839505195619, parent=..., children={0: Node(prior=0.5671595931053162, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43284040689468384, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4486733376979828, action_taken=1, visit_count=1, value_sum=-2.0023785388469695, parent=..., children={0: Node(prior=0.5653825402259827, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43461740016937256, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4434373676776886, action_taken=1, visit_count=1, value_sum=-2.0290331947803497, parent=..., children={0: Node(prior=0.5620538592338562, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4379461109638214, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4386332631111145, action_taken=1, visit_count=1, value_sum=-2.0418746459484103, parent=..., children={0: Node(prior=0.5572227835655212, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.44277718663215637, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.43673989176750183, action_taken=1, visit_count=2, value_sum=-2.789444588208198, parent=..., children={0: Node(prior=0.5513436794281006, action_taken=0, visit_count=1, value_sum=-1.8757336950302124, parent=..., children={0: Node(prior=0.5632596015930176, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43674036860466003, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4486563205718994, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4344910979270935, action_taken=1, visit_count=2, value_sum=-2.794865756523609, parent=..., children={0: Node(prior=0.5456045866012573, action_taken=0, visit_count=1, value_sum=-1.8843275892734528, parent=..., children={0: Node(prior=0.5667170286178589, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4332830011844635, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45439544320106506, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4338543117046356, action_taken=1, visit_count=3, value_sum=-3.8543451334726804, parent=..., children={0: Node(prior=0.5411300659179688, action_taken=0, visit_count=1, value_sum=-1.897102006673813, parent=..., children={0: Node(prior=0.5682271718978882, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4317728281021118, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45886993408203125, action_taken=1, visit_count=1, value_sum=-2.2167485153675077, parent=..., children={0: Node(prior=0.47163909673690796, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.528360903263092, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.43354690074920654, action_taken=1, visit_count=3, value_sum=-3.857354231330752, parent=..., children={0: Node(prior=0.5379495024681091, action_taken=0, visit_count=1, value_sum=-1.9094592332839966, parent=..., children={0: Node(prior=0.5688297748565674, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43117016553878784, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.46205052733421326, action_taken=1, visit_count=1, value_sum=-2.2059683525562286, parent=..., children={0: Node(prior=0.4648236930370331, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5351762771606445, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.4332282245159149, action_taken=1, visit_count=4, value_sum=-3.5416411952714912, parent=..., children={0: Node(prior=0.5364394783973694, action_taken=0, visit_count=2, value_sum=-2.7240092052936555, parent=..., children={0: Node(prior=0.5689586997032166, action_taken=0, visit_count=1, value_sum=-1.9279455649852753, parent=..., children={0: Node(prior=0.46526652574539185, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5347334742546082, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.43104124069213867, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.46356046199798584, action_taken=1, visit_count=1, value_sum=-2.191757901906967, parent=..., children={0: Node(prior=0.45953065156936646, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5404694080352783, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})})\n",
            "root children right Node(prior=0.4323772192001343, action_taken=1, visit_count=15, value_sum=19.36107507304724, parent=Node(prior=1.0, action_taken=None, visit_count=100, value_sum=124.55006518617994, parent=None, children={0: Node(prior=0.567622721195221, action_taken=0, visit_count=84, value_sum=187.68598482206252, parent=..., children={0: Node(prior=0.474322110414505, action_taken=0, visit_count=7, value_sum=-0.2695752017196349, parent=..., children={0: Node(prior=0.38621020317077637, action_taken=0, visit_count=1, value_sum=-1.780452629327774, parent=..., children={0: Node(prior=0.36519503593444824, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6348049640655518, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6137897968292236, action_taken=1, visit_count=5, value_sum=-2.907578662968862, parent=..., children={0: Node(prior=0.467843234539032, action_taken=0, visit_count=1, value_sum=-1.8740499436855316, parent=..., children={0: Node(prior=0.38670608401298523, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6132939457893372, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5321568250656128, action_taken=1, visit_count=3, value_sum=-3.548915843714475, parent=..., children={0: Node(prior=0.5639476776123047, action_taken=0, visit_count=1, value_sum=-1.9062996768951415, parent=..., children={0: Node(prior=0.4590819478034973, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5409180521965027, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4360523521900177, action_taken=1, visit_count=1, value_sum=-2.0379181194305422, parent=..., children={0: Node(prior=0.5366846919059753, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.46331527829170227, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.5256779193878174, action_taken=1, visit_count=76, value_sum=197.57110557717525, parent=..., children={0: Node(prior=0.5667717456817627, action_taken=0, visit_count=71, value_sum=197.39992449844428, parent=..., children={0: Node(prior=0.4691005349159241, action_taken=0, visit_count=4, value_sum=-2.9416348691850605, parent=..., children={0: Node(prior=0.38625043630599976, action_taken=0, visit_count=1, value_sum=-1.773827143907547, parent=..., children={0: Node(prior=0.36738595366477966, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6326140761375427, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6137495040893555, action_taken=1, visit_count=2, value_sum=-2.6602006115436554, parent=..., children={0: Node(prior=0.460921972990036, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5390780568122864, action_taken=1, visit_count=1, value_sum=-1.8806425023078919, parent=..., children={0: Node(prior=0.5634855031967163, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43651440739631653, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.5308994054794312, action_taken=1, visit_count=66, value_sum=196.62078161817018, parent=..., children={0: Node(prior=0.5664531588554382, action_taken=0, visit_count=62, value_sum=192.01653732918504, parent=..., children={0: Node(prior=0.4621329605579376, action_taken=0, visit_count=3, value_sum=-3.257522531522512, parent=..., children={0: Node(prior=0.386139839887619, action_taken=0, visit_count=1, value_sum=-1.768183207511902, parent=..., children={0: Node(prior=0.370535284280777, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6294646859169006, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6138601899147034, action_taken=1, visit_count=1, value_sum=-1.8995561742782594, parent=..., children={0: Node(prior=0.4558551609516144, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5441448092460632, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.53786700963974, action_taken=1, visit_count=58, value_sum=185.98673591363294, parent=..., children={0: Node(prior=0.5661457180976868, action_taken=0, visit_count=54, value_sum=184.36182951181584, parent=..., children={0: Node(prior=0.45730018615722656, action_taken=0, visit_count=3, value_sum=-3.2435329908317323, parent=..., children={0: Node(prior=0.3871479332447052, action_taken=0, visit_count=1, value_sum=-1.760520350933075, parent=..., children={0: Node(prior=0.37359684705734253, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6264031529426575, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6128520369529724, action_taken=1, visit_count=1, value_sum=-1.9006603431701659, parent=..., children={0: Node(prior=0.45179301500320435, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5482069253921509, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5426997542381287, action_taken=1, visit_count=50, value_sum=182.6732736809449, parent=..., children={0: Node(prior=0.5655089616775513, action_taken=0, visit_count=47, value_sum=173.92152005111706, parent=..., children={0: Node(prior=0.45416057109832764, action_taken=0, visit_count=2, value_sum=-2.6340929181098938, parent=..., children={0: Node(prior=0.3880867660045624, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.61191326379776, action_taken=1, visit_count=1, value_sum=-1.9021841812133788, parent=..., children={0: Node(prior=0.4530058801174164, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.546994149684906, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5458393692970276, action_taken=1, visit_count=44, value_sum=165.18676130967657, parent=..., children={0: Node(prior=0.5632600784301758, action_taken=0, visit_count=41, value_sum=157.14880818497903, parent=..., children={0: Node(prior=0.4531548321247101, action_taken=0, visit_count=2, value_sum=-2.6158942710280417, parent=..., children={0: Node(prior=0.39108338952064514, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6089165806770325, action_taken=1, visit_count=1, value_sum=-1.9009556221961974, parent=..., children={0: Node(prior=0.4518287181854248, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5481712818145752, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5468451380729675, action_taken=1, visit_count=38, value_sum=149.17124474465314, parent=..., children={0: Node(prior=0.5613667368888855, action_taken=0, visit_count=36, value_sum=133.93829499396546, parent=..., children={0: Node(prior=0.45308035612106323, action_taken=0, visit_count=2, value_sum=-2.599102846837044, parent=..., children={0: Node(prior=0.39519229531288147, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6048077344894409, action_taken=1, visit_count=1, value_sum=-1.8991761589050293, parent=..., children={0: Node(prior=0.44910067319869995, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5508992671966553, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5469196438789368, action_taken=1, visit_count=33, value_sum=126.0898246103666, parent=..., children={0: Node(prior=0.5565626621246338, action_taken=0, visit_count=31, value_sum=111.68968972867955, parent=..., children={0: Node(prior=0.4505053460597992, action_taken=0, visit_count=1, value_sum=-1.8031083905696867, parent=..., children={0: Node(prior=0.3975910246372223, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6024089455604553, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5494946837425232, action_taken=1, visit_count=29, value_sum=97.61472461793771, parent=..., children={0: Node(prior=0.5513266324996948, action_taken=0, visit_count=27, value_sum=83.99711113166781, parent=..., children={0: Node(prior=0.44654417037963867, action_taken=0, visit_count=1, value_sum=-1.7907626116275788, parent=..., children={0: Node(prior=0.4006522595882416, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.599347710609436, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5534558296203613, action_taken=1, visit_count=25, value_sum=70.82040749978498, parent=..., children={0: Node(prior=0.5400351285934448, action_taken=0, visit_count=23, value_sum=58.24515871496024, parent=..., children={0: Node(prior=0.4526875615119934, action_taken=0, visit_count=1, value_sum=-1.779567972421646, parent=..., children={0: Node(prior=0.40502604842185974, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5949739813804626, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5473124980926514, action_taken=1, visit_count=21, value_sum=46.24878884289697, parent=..., children={0: Node(prior=0.5300087928771973, action_taken=0, visit_count=19, value_sum=35.037917144565064, parent=..., children={0: Node(prior=0.45844611525535583, action_taken=0, visit_count=1, value_sum=-1.7725157368183135, parent=..., children={0: Node(prior=0.4112602770328522, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5887397527694702, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.541553795337677, action_taken=1, visit_count=17, value_sum=24.56597839929125, parent=..., children={0: Node(prior=0.5183643102645874, action_taken=0, visit_count=14, value_sum=16.765017798750208, parent=..., children={0: Node(prior=0.4629034698009491, action_taken=0, visit_count=1, value_sum=-1.761679515838623, parent=..., children={0: Node(prior=0.41444775462150574, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5855522751808167, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5370965600013733, action_taken=1, visit_count=12, value_sum=8.875744273292883, parent=..., children={0: Node(prior=0.4963553845882416, action_taken=0, visit_count=9, value_sum=3.287691020903633, parent=..., children={0: Node(prior=0.46623092889785767, action_taken=0, visit_count=1, value_sum=-1.764437223672867, parent=..., children={0: Node(prior=0.4166909456253052, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5833090543746948, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5337690114974976, action_taken=1, visit_count=7, value_sum=-1.1561865229578094, parent=..., children={0: Node(prior=0.5005074143409729, action_taken=0, visit_count=3, value_sum=-2.823975, parent=..., children={}), 1: Node(prior=0.4994926452636719, action_taken=1, visit_count=3, value_sum=-2.823975, parent=..., children={})})}), 1: Node(prior=0.5036445260047913, action_taken=1, visit_count=2, value_sum=-2.5739504247188565, parent=..., children={0: Node(prior=0.5495519638061523, action_taken=0, visit_count=1, value_sum=-1.7973179948329925, parent=..., children={0: Node(prior=0.49999070167541504, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5000093579292297, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45044803619384766, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4816356599330902, action_taken=1, visit_count=2, value_sum=-2.6495756814718243, parent=..., children={0: Node(prior=0.54984050989151, action_taken=0, visit_count=1, value_sum=-1.8399526512622835, parent=..., children={0: Node(prior=0.4974624514579773, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5025375485420227, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4501595199108124, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.46999120712280273, action_taken=1, visit_count=1, value_sum=-1.9580119800567628, parent=..., children={0: Node(prior=0.5585036277770996, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4414963722229004, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.45996490120887756, action_taken=1, visit_count=1, value_sum=-1.9790839505195619, parent=..., children={0: Node(prior=0.5671595931053162, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43284040689468384, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4486733376979828, action_taken=1, visit_count=1, value_sum=-2.0023785388469695, parent=..., children={0: Node(prior=0.5653825402259827, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43461740016937256, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4434373676776886, action_taken=1, visit_count=1, value_sum=-2.0290331947803497, parent=..., children={0: Node(prior=0.5620538592338562, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4379461109638214, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4386332631111145, action_taken=1, visit_count=1, value_sum=-2.0418746459484103, parent=..., children={0: Node(prior=0.5572227835655212, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.44277718663215637, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.43673989176750183, action_taken=1, visit_count=2, value_sum=-2.789444588208198, parent=..., children={0: Node(prior=0.5513436794281006, action_taken=0, visit_count=1, value_sum=-1.8757336950302124, parent=..., children={0: Node(prior=0.5632596015930176, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43674036860466003, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4486563205718994, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4344910979270935, action_taken=1, visit_count=2, value_sum=-2.794865756523609, parent=..., children={0: Node(prior=0.5456045866012573, action_taken=0, visit_count=1, value_sum=-1.8843275892734528, parent=..., children={0: Node(prior=0.5667170286178589, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4332830011844635, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45439544320106506, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4338543117046356, action_taken=1, visit_count=3, value_sum=-3.8543451334726804, parent=..., children={0: Node(prior=0.5411300659179688, action_taken=0, visit_count=1, value_sum=-1.897102006673813, parent=..., children={0: Node(prior=0.5682271718978882, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4317728281021118, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.45886993408203125, action_taken=1, visit_count=1, value_sum=-2.2167485153675077, parent=..., children={0: Node(prior=0.47163909673690796, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.528360903263092, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.43354690074920654, action_taken=1, visit_count=3, value_sum=-3.857354231330752, parent=..., children={0: Node(prior=0.5379495024681091, action_taken=0, visit_count=1, value_sum=-1.9094592332839966, parent=..., children={0: Node(prior=0.5688297748565674, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.43117016553878784, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.46205052733421326, action_taken=1, visit_count=1, value_sum=-2.2059683525562286, parent=..., children={0: Node(prior=0.4648236930370331, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5351762771606445, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: Node(prior=0.4332282245159149, action_taken=1, visit_count=4, value_sum=-3.5416411952714912, parent=..., children={0: Node(prior=0.5364394783973694, action_taken=0, visit_count=2, value_sum=-2.7240092052936555, parent=..., children={0: Node(prior=0.5689586997032166, action_taken=0, visit_count=1, value_sum=-1.9279455649852753, parent=..., children={0: Node(prior=0.46526652574539185, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5347334742546082, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.43104124069213867, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.46356046199798584, action_taken=1, visit_count=1, value_sum=-2.191757901906967, parent=..., children={0: Node(prior=0.45953065156936646, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5404694080352783, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})})}), 1: ...}), children={0: Node(prior=0.5361707210540771, action_taken=0, visit_count=13, value_sum=11.237434869016916, parent=..., children={0: Node(prior=0.5694295763969421, action_taken=0, visit_count=11, value_sum=4.198807600505871, parent=..., children={0: Node(prior=0.47154173254966736, action_taken=0, visit_count=3, value_sum=-3.2907117472940683, parent=..., children={0: Node(prior=0.3857184052467346, action_taken=0, visit_count=1, value_sum=-1.7807540452480315, parent=..., children={0: Node(prior=0.36542534828186035, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6345745921134949, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.6142815947532654, action_taken=1, visit_count=1, value_sum=-1.9129426288604736, parent=..., children={0: Node(prior=0.4659036695957184, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5340963006019592, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.5284582376480103, action_taken=1, visit_count=7, value_sum=-0.5855545580628474, parent=..., children={0: Node(prior=0.5684077739715576, action_taken=0, visit_count=5, value_sum=-2.975070796689784, parent=..., children={0: Node(prior=0.4672994911670685, action_taken=0, visit_count=1, value_sum=-1.8786221659183502, parent=..., children={0: Node(prior=0.38593369722366333, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.6140663027763367, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.5327004790306091, action_taken=1, visit_count=3, value_sum=-3.596926851428747, parent=..., children={0: Node(prior=0.5679386854171753, action_taken=0, visit_count=1, value_sum=-1.92354434967041, parent=..., children={0: Node(prior=0.46206292510032654, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5379371047019958, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})}), 1: Node(prior=0.4320612847805023, action_taken=1, visit_count=1, value_sum=-2.051801780462265, parent=..., children={0: Node(prior=0.5399029850959778, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4600970447063446, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4315922260284424, action_taken=1, visit_count=1, value_sum=-2.0537051618099214, parent=..., children={0: Node(prior=0.538829505443573, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.4611704647541046, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})}), 1: Node(prior=0.4305703639984131, action_taken=1, visit_count=1, value_sum=-2.055764561891556, parent=..., children={0: Node(prior=0.5388778448104858, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.46112218499183655, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})}), 1: Node(prior=0.4638291895389557, action_taken=1, visit_count=1, value_sum=-2.1793695187568662, parent=..., children={0: Node(prior=0.4556121826171875, action_taken=0, visit_count=0, value_sum=0, parent=..., children={}), 1: Node(prior=0.5443878173828125, action_taken=1, visit_count=0, value_sum=0, parent=..., children={})})})\n"
          ]
        }
      ],
      "source": [
        "model = Network().to(device)\n",
        "\n",
        "mcts = MCTS(model,100,100)\n",
        "observations,_ = env.reset()\n",
        "#print(observations)\n",
        "root=mcts.search(observations)\n",
        "print('root value',root.Q_value)\n",
        "print('root visit', root.visit_count)\n",
        "print('root children left visit',root.children[0])\n",
        "print('root children right',root.children[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.Replay Buffer"
      ],
      "metadata": {
        "id": "HEfaFtDD-IuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3nVT2QLMNjC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "from collections import deque\n",
        "\n",
        "class GameHistory:\n",
        "    def __init__(self):\n",
        "      self.observations = []  # List[np.ndarray]\n",
        "      self.actions = []       # List[int]\n",
        "      self.rewards = []       # List[float]\n",
        "      self.mcts_policies = []    # List[float]\n",
        "      self.values = []\n",
        "\n",
        "\n",
        "    def store(self, observation, action, reward, action_probs,value):\n",
        "      self.observations.append(observation)\n",
        "      self.actions.append(action)\n",
        "      self.rewards.append(reward)\n",
        "      self.mcts_policies.append(action_probs)\n",
        "      self.values.append(value)\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "      self.observations.clear()\n",
        "      self.actions.clear()\n",
        "      self.rewards.clear()\n",
        "      self.mcts_policies.clear()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFo_CR6iK52U"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, batch_size, minimum_size, capacity=500):\n",
        "        self.batch_size = batch_size\n",
        "        self.minimum_size = minimum_size\n",
        "        self.capacity = capacity\n",
        "        self.game_history = GameHistory()\n",
        "\n",
        "    def store(self, observation, action, reward, action_probs, value):\n",
        "\n",
        "        if len(self.game_history.observations) >= self.capacity:\n",
        "            # del the oldest data\n",
        "            self.game_history.observations.pop(0)\n",
        "            self.game_history.actions.pop(0)\n",
        "            self.game_history.rewards.pop(0)\n",
        "            self.game_history.mcts_policies.pop(0)\n",
        "            self.game_history.values.pop(0)\n",
        "\n",
        "        self.game_history.store(observation, action, reward, action_probs,value)\n",
        "\n",
        "\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "\n",
        "\n",
        "\n",
        "      indices = np.random.choice(len(self.game_history.observations), batch_size)\n",
        "\n",
        "\n",
        "      observations = torch.tensor([self.game_history.observations[i] for i in indices], dtype=torch.float32)\n",
        "      actions = torch.tensor([self.game_history.actions[i] for i in indices], dtype=torch.long)\n",
        "      rewards = torch.tensor([self.game_history.rewards[i] for i in indices], dtype=torch.float32)\n",
        "      policies = torch.tensor([self.game_history.mcts_policies[i] for i in indices], dtype=torch.float32)\n",
        "      values = torch.tensor([self.game_history.values[i] for i in indices], dtype=torch.float32)\n",
        "\n",
        "\n",
        "      return observations.to(device), actions.to(device), rewards.to(device), policies.to(device), values.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.game_history.observations)\n",
        "\n",
        "    def clear(self):\n",
        "        self.game_history.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.Self play"
      ],
      "metadata": {
        "id": "BuyGkppkiInr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne9SroBNtRVB"
      },
      "outputs": [],
      "source": [
        "def self_play(replay_buffer, model, epoch_of_training=0):\n",
        "    env = gym.make('CartPole-v1')\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    for episode in range(1):\n",
        "\n",
        "      observation,_ = env.reset()\n",
        "      done = False\n",
        "      episode_reward = 0\n",
        "\n",
        "\n",
        "      trajectory = []\n",
        "      while not done:\n",
        "        mcts = MCTS(model,epoch_of_training=epoch_of_training)\n",
        "        root = mcts.search(observation)\n",
        "        #temperature = min(1.0, 1.0 - episode/num_episodes)\n",
        "        action_probs = mcts.get_action_probs(root, temperature=1)\n",
        "\n",
        "        action = np.argmax(action_probs)\n",
        "\n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "        trajectory.append({\n",
        "            'state': observation,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'action_probs': action_probs\n",
        "        })\n",
        "        observation = next_observation\n",
        "\n",
        "      returns = []\n",
        "      G = 0\n",
        "      gamma = 0.99\n",
        "      for t in reversed(trajectory):\n",
        "        G = t['reward'] + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "      for t, G in zip(trajectory, returns):\n",
        "        replay_buffer.store(\n",
        "            t['state'],\n",
        "            t['action'],\n",
        "            t['reward'],\n",
        "            t['action_probs'],\n",
        "            G\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "      print('Self-play')\n",
        "      #print('observation',observation)\n",
        "      #print('action',action)\n",
        "      #print('action_probs',action_probs)\n",
        "      #print('returns',returns)\n",
        "\n",
        "\n",
        "      print('Episode {}: episode_reward = {}'.format(episode, episode_reward))\n",
        "\n",
        "      #print(\"-----------------------------\")\n",
        "      #print(\"-----------------------------\")\n",
        "      return episode_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMCDn64uuGx-",
        "outputId": "b23b8ad6-8147-4b88-9f50-d9503863ae72",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-play\n",
            "Episode 0: episode_reward = 15.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model = Network().to(device)\n",
        "#model.load('best_model.pth')\n",
        "replay_buffer_test=ReplayBuffer(batch_size=32, minimum_size=100)\n",
        "self_play(replay_buffer_test, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observations, actions, rewards, policies, values = replay_buffer_test.sample_batch(10)\n",
        "policy_logits, pred_values = model(observations)\n",
        "print('values',values)\n",
        "print('pred_values',pred_values.squeeze())\n",
        "print('policies',policies)\n",
        "print('policy_logits',policy_logits)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rh1l62L7E_TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a33c3d7-6b08-404b-c25f-12004335b247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values tensor([ 8.6483,  6.7935, 10.4662,  7.7255,  4.9010, 11.3615,  5.8520,  8.6483,\n",
            "         9.5618,  8.6483])\n",
            "pred_values tensor([-0.0732, -0.0706, -0.0375, -0.0339, -0.4599, -0.0564, -0.3290, -0.0732,\n",
            "        -0.0515, -0.0732], grad_fn=<SqueezeBackward0>)\n",
            "policies tensor([[0.4949, 0.5051],\n",
            "        [0.5859, 0.4141],\n",
            "        [0.5051, 0.4949],\n",
            "        [0.6566, 0.3434],\n",
            "        [0.5657, 0.4343],\n",
            "        [0.4949, 0.5051],\n",
            "        [0.7475, 0.2525],\n",
            "        [0.4949, 0.5051],\n",
            "        [0.6364, 0.3636],\n",
            "        [0.4949, 0.5051]])\n",
            "policy_logits tensor([[ 0.1690, -0.7911],\n",
            "        [ 0.1853, -0.7749],\n",
            "        [ 0.6352,  0.2304],\n",
            "        [ 0.0768, -0.2188],\n",
            "        [ 0.3572, -1.3648],\n",
            "        [ 0.0677, -0.2579],\n",
            "        [ 0.2771, -1.2215],\n",
            "        [ 0.1690, -0.7911],\n",
            "        [ 0.0685, -0.2479],\n",
            "        [ 0.1690, -0.7911]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-2ce9a0165a96>:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  observations = torch.tensor([self.game_history.observations[i] for i in indices], dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.Train\n"
      ],
      "metadata": {
        "id": "F0sMecPp-Q9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, replay_buffer, num_epochs, save_path='best_model.pth'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    #model = model.to(device)\n",
        "    best_reward = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "      # 1. data collection\n",
        "      episode_reward = self_play(replay_buffer, model, epoch)\n",
        "      if episode_reward > 450:\n",
        "        epoch_training  = 100\n",
        "      else:\n",
        "        epoch_training  = 50\n",
        "\n",
        "      #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "      for i in range(epoch_training):\n",
        "        # 2. data from replay buffer and transform into torch\n",
        "        observations, actions, rewards, policies, values = replay_buffer.sample_batch()\n",
        "\n",
        "        # 3. clean grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3.1 Loss calculation\n",
        "        model.train()\n",
        "\n",
        "        policy_logits, pred_values = model(observations)\n",
        "\n",
        "        #policy_loss = -torch.sum(policies * torch.log_softmax(policy_logits, dim=1), dim=1).mean()\n",
        "        value_loss = F.mse_loss(pred_values.squeeze(), values)\n",
        "        probs = F.softmax(policy_logits, dim=1)\n",
        "\n",
        "        policy_loss = -torch.sum(policies * torch.log(probs))\n",
        "        #temperature = 1\n",
        "        #scaled_logits = policy_logits / temperature\n",
        "        #policy_loss = -torch.mean(torch.sum(policies * F.log_softmax(scaled_logits, dim=1), dim=1))\n",
        "        alpha = value_loss.mean().item()\n",
        "        beta = policy_loss.mean().item()\n",
        "\n",
        "        # 3.2 sum the loss\n",
        "        total_loss = value_loss + policy_loss\n",
        "        policy_entropy =  -(F.softmax(policy_logits, dim=1) * F.log_softmax(policy_logits, dim=1)).sum(1).mean()\n",
        "        total_loss += 0.01 * policy_entropy\n",
        "        best_loss  = total_loss.item()\n",
        "\n",
        "        # 4. backward and optimize\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "      #scheduler.step()\n",
        "      if epoch % 3 == 0:\n",
        "        eval_reward = evaluate(model)\n",
        "        print(f\"Epoch {epoch}: Evaluation reward = {eval_reward:.2f}\")\n",
        "        # 5. Save best model\n",
        "        if eval_reward > best_reward:\n",
        "          best_reward = eval_reward\n",
        "          model.save(save_path)\n",
        "          print(f\"Epoch {epoch}: New best model saved with loss: {best_loss:.4f}\")\n",
        "\n",
        "\n",
        "      print(f\"Epoch {epoch}: \"\n",
        "            f\"Total Loss = {total_loss:.4f}, \"\n",
        "            f\"Policy Loss = {policy_loss.mean().item():.4f}, \"\n",
        "            f\"Value Loss = {value_loss.mean().item():.4f} \"\n",
        "            )\n",
        "\n",
        "\n",
        "    print(f\"Training completed. Best model loaded with loss: {best_loss:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "pyQyPstewuWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###evaluate"
      ],
      "metadata": {
        "id": "UDrOEvKbsNWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, num_episodes=2):\n",
        "    \"\"\"\n",
        "    评估当前模型在CartPole环境的表现\n",
        "\n",
        "    Args:\n",
        "        model: 训练的网络模型\n",
        "        num_episodes: 评估的回合数\n",
        "    Returns:\n",
        "        mean_reward: 平均回合奖励\n",
        "    \"\"\"\n",
        "    model.eval()  # 设置为评估模式\n",
        "    rewards = []\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            with torch.no_grad():  # 不需要梯度\n",
        "                mcts = MCTS(model)\n",
        "                root = mcts.search(obs)  # 只使用最优动作\n",
        "                action = np.argmax(mcts.get_action_probs(root))\n",
        "\n",
        "                # 执行动作\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "                done = terminated or truncated\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        print('++++++Evaluation++++++')\n",
        "        print(f\"Evaluation episode {episode}: Reward = {episode_reward}\")\n",
        "\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "\n",
        "    return mean_reward"
      ],
      "metadata": {
        "id": "7KKFito1r1MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Let's start Training\n"
      ],
      "metadata": {
        "id": "JSXlma5jaEKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "num_epochs = 10\n",
        "lr= 0.001\n",
        "model = Network().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "\n",
        "\n",
        "replay_buffer = ReplayBuffer(batch_size=32, minimum_size=100)\n",
        "\n",
        "\n",
        "# training\n",
        "model = train(model, optimizer, replay_buffer, num_epochs)"
      ],
      "metadata": {
        "id": "Qj9IH11IyFRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f766f40a-e857-45d0-bb55-ea2d966a4759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 14.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 0: Reward = 137.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 1: Reward = 113.0\n",
            "Epoch 0: Evaluation reward = 125.00\n",
            "Model saved to best_model.pth\n",
            "Epoch 0: New best model saved with loss: 24.2939\n",
            "Epoch 0: Total Loss = 24.2939, Policy Loss = 21.7788, Value Loss = 2.5083 \n",
            "Epoch 1/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 165.0\n",
            "Epoch 1: Total Loss = 1003.1149, Policy Loss = 22.0261, Value Loss = 981.0819 \n",
            "Epoch 2/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 115.0\n",
            "Epoch 2: Total Loss = 206.3548, Policy Loss = 22.1152, Value Loss = 184.2327 \n",
            "Epoch 3/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 112.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 0: Reward = 116.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 1: Reward = 146.0\n",
            "Epoch 3: Evaluation reward = 131.00\n",
            "Model saved to best_model.pth\n",
            "Epoch 3: New best model saved with loss: 26.6919\n",
            "Epoch 3: Total Loss = 26.6919, Policy Loss = 22.0592, Value Loss = 4.6258 \n",
            "Epoch 4/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 128.0\n",
            "Epoch 4: Total Loss = 28.3358, Policy Loss = 22.0426, Value Loss = 6.2863 \n",
            "Epoch 5/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 126.0\n",
            "Epoch 5: Total Loss = 26.3833, Policy Loss = 22.1738, Value Loss = 4.2026 \n",
            "Epoch 6/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 149.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 0: Reward = 426.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 1: Reward = 346.0\n",
            "Epoch 6: Evaluation reward = 386.00\n",
            "Model saved to best_model.pth\n",
            "Epoch 6: New best model saved with loss: 24.9915\n",
            "Epoch 6: Total Loss = 24.9915, Policy Loss = 22.1201, Value Loss = 2.8645 \n",
            "Epoch 7/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 166.0\n",
            "Epoch 7: Total Loss = 33.7198, Policy Loss = 22.1296, Value Loss = 11.5833 \n",
            "Epoch 8/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 500.0\n",
            "Epoch 8: Total Loss = 568.2723, Policy Loss = 22.0399, Value Loss = 546.2256 \n",
            "Epoch 9/10\n",
            "Self-play\n",
            "Episode 0: episode_reward = 403.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 0: Reward = 500.0\n",
            "++++++Evaluation++++++\n",
            "Evaluation episode 1: Reward = 500.0\n",
            "Epoch 9: Evaluation reward = 500.00\n",
            "Model saved to best_model.pth\n",
            "Epoch 9: New best model saved with loss: 383.2965\n",
            "Epoch 9: Total Loss = 383.2965, Policy Loss = 20.8497, Value Loss = 362.4403 \n",
            "Training completed. Best model loaded with loss: 383.2965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.Test thoughts"
      ],
      "metadata": {
        "id": "ccjqlKRn09Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observations, actions, rewards, policies, values = replay_buffer.sample_batch(10)\n",
        "policy_logits, pred_values = model(observations)\n",
        "policy_logits = F.softmax(policy_logits, dim=1)\n",
        "\n",
        "print('values',values)\n",
        "print('pred_values',pred_values.squeeze())\n",
        "print('policies',policies)\n",
        "print('policy_logits',policy_logits)"
      ],
      "metadata": {
        "id": "1dNJ5kMmHEBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cdd5dcbe-ebdb-4ea2-8290-6f017d949676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values tensor([47.4403, 42.4645, 97.3435, 92.6692, 31.0551, 93.0285, 97.8705, 29.6552,\n",
            "        33.7718, 73.9915])\n",
            "pred_values tensor([49.4657, 61.3272, 71.8766, 90.1167, 30.4016, 91.6647, 86.4833, 28.7126,\n",
            "        33.6460, 73.7598], grad_fn=<SqueezeBackward0>)\n",
            "policies tensor([[0.6263, 0.3737],\n",
            "        [0.3636, 0.6364],\n",
            "        [0.7576, 0.2424],\n",
            "        [0.3535, 0.6465],\n",
            "        [0.4949, 0.5051],\n",
            "        [0.3838, 0.6162],\n",
            "        [0.7475, 0.2525],\n",
            "        [0.4848, 0.5152],\n",
            "        [0.5657, 0.4343],\n",
            "        [0.1919, 0.8081]])\n",
            "policy_logits tensor([[0.5609, 0.4391],\n",
            "        [0.5036, 0.4964],\n",
            "        [0.4927, 0.5073],\n",
            "        [0.4337, 0.5663],\n",
            "        [0.4975, 0.5025],\n",
            "        [0.4831, 0.5169],\n",
            "        [0.4655, 0.5345],\n",
            "        [0.4579, 0.5421],\n",
            "        [0.5901, 0.4099],\n",
            "        [0.3668, 0.6332]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_play(replay_buffer, model)"
      ],
      "metadata": {
        "id": "5BE64GPORjNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Net_w(nn.Module):\n",
        "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
        "        super(Net_w, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),  # 加入BN层帮助训练\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.FloatTensor(x)\n",
        "        logits = self.net(x)\n",
        "        # 用temperature参数调节softmax的平滑程度\n",
        "        temperature = 1\n",
        "        return logits / temperature"
      ],
      "metadata": {
        "id": "jODB4axH74Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#self_play(replay_buffer, model)\n",
        "observations, actions, rewards, policies, values = replay_buffer.sample_batch(10)\n",
        "\n",
        "\n",
        "net_policy = Net_w()\n",
        "\n",
        "print(net_policy(observations))\n",
        "optimizer_policy = torch.optim.Adam(net_policy.parameters(), lr = 0.0001)\n",
        "\n",
        "observations, actions, rewards, policies, values = replay_buffer.sample_batch()\n",
        "for i in range(100):\n",
        "\n",
        "  net_policy.train()\n",
        "  optimizer_policy.zero_grad()\n",
        "  logits = net_policy(observations)\n",
        "  probs = F.softmax(logits, dim=1)\n",
        "  #\n",
        "\n",
        "\n",
        "  policy_net_loss = -torch.sum(policies * torch.log(probs))\n",
        "  print('policy_net_loss',policy_net_loss.item())\n",
        "  policy_net_loss.backward()\n",
        "\n",
        "  torch.nn.utils.clip_grad_norm_(net_policy.parameters(), max_norm=1.0)\n",
        "  optimizer_policy.step()\n",
        "\n",
        "\n",
        "#policy_logits, pred_values = model(observations)\n",
        "policy_logits = F.softmax(net_policy(observations), dim=1)\n",
        "\n",
        "#print('values',values)\n",
        "#print('pred_values',pred_values.squeeze())\n",
        "print('policies',policies)\n",
        "print('policy_logits',policy_logits)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jSjt1GoeuFQJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+eJI+qbjJtm6blbfTynEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}