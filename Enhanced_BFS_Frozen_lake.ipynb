{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "OcQmY0NJfXQJ"
      ],
      "authorship_tag": "ABX9TyOdxaRiWavMiPELbM1SInSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/Enhanced_BFS_Frozen_lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTs3ymgx4JcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c438c2f2-5b7d-4779-9154-6e3e65fc7bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Network"
      ],
      "metadata": {
        "id": "jiU_4agsX-Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=128, num_actions=4):\n",
        "        super(Network, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # 共享层\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size)\n",
        "        )\n",
        "\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            state: 游戏状态的one-hot编码 (batch_size, 16)\n",
        "        返回:\n",
        "            policy_logits: 动作概率的对数 (batch_size, 4)\n",
        "            value: 状态价值估计 (batch_size, 1)\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(state, (int, np.integer)):\n",
        "            state = F.one_hot(torch.tensor(state), num_classes=64).float()\n",
        "        elif isinstance(state, np.ndarray):\n",
        "            state = torch.FloatTensor(state)\n",
        "        elif not isinstance(state, torch.Tensor):\n",
        "            raise TypeError(f\"Unsupported input type: {type(state)}\")\n",
        "\n",
        "        shared_features = self.shared_layers(state)\n",
        "\n",
        "        value = self.value_head(shared_features)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        short cut for BFS\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            value = self.forward(state)\n",
        "            return value.item()\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load(self, filepath):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=self.device))\n",
        "        print(f\"Model loaded from {filepath}\")\n"
      ],
      "metadata": {
        "id": "1gwkeHF5X9nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Tree Node"
      ],
      "metadata": {
        "id": "8FkjNZ7FYQ2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import gymnasium as gym\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "from queue import PriorityQueue\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "  action_taken: int\n",
        "  visit_count: int = 0\n",
        "  state: int = 0\n",
        "  parent: Optional['Node'] = None\n",
        "  children: Dict[int, 'Node'] = None\n",
        "  done = False\n",
        "  has_children = False\n",
        "\n",
        "  def __post_init__(self):\n",
        "      if self.children is None:\n",
        "          self.children = {}\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    # 任意返回 False，因为我们只关心优先级的比较\n",
        "    return False"
      ],
      "metadata": {
        "id": "LW1jBpWxRAOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.BFS"
      ],
      "metadata": {
        "id": "OcQmY0NJfXQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs_search(start_state, depth_limit=11):\n",
        "  \"\"\"使用真实环境进行局部BFS搜索\"\"\"\n",
        "  env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "  path_node = Node(action_taken=None,state = start_state)\n",
        "  #print('start_state',start_state)\n",
        "  counter = 10\n",
        "  queue = PriorityQueue()\n",
        "  queue.put((0, path_node))\n",
        "\n",
        "  def get_path_states(node):\n",
        "    # 获取当前路径上的所有状态\n",
        "    states = set()\n",
        "    current = node\n",
        "    while current:\n",
        "        states.add(current.state)\n",
        "        current = current.parent\n",
        "        #print('记录的',current.state)\n",
        "    return states\n",
        "  def get_action_path(node):\n",
        "    # 回溯完整路径\n",
        "    path_action = []\n",
        "    current = node\n",
        "    while current.parent:\n",
        "      path_action.append(current.action_taken)\n",
        "      #print('current.state',current.state)\n",
        "      current = current.parent\n",
        "    return list(reversed(path_action))\n",
        "  def get_path(node):\n",
        "    # 回溯完整路径\n",
        "    path = []\n",
        "    current = node\n",
        "    while current:\n",
        "      path.append(current.state)\n",
        "      current = current.parent\n",
        "    return path\n",
        "  while not queue.empty() and counter >=0:\n",
        "    counter -= 1\n",
        "    priority, current_node = queue.get()\n",
        "    visited = get_path_states(current_node)\n",
        "    # 如果达到深度限制\n",
        "    if len(get_path(current_node)) > depth_limit:\n",
        "        continue\n",
        "    #reach to gaol\n",
        "    if current_node.state == 15:\n",
        "      return get_action_path(current_node), 1\n",
        "    # 获取当前状态下的有效动作\n",
        "    row = current_node.state // 4\n",
        "    col = current_node.state % 4\n",
        "    valid_actions = []\n",
        "    if col > 0: valid_actions.append(0)    # 左\n",
        "    if row < 3: valid_actions.append(1)    # 下\n",
        "    if col < 3: valid_actions.append(2)    # 右\n",
        "    if row > 0: valid_actions.append(3)    # 上\n",
        "\n",
        "    for action in valid_actions:\n",
        "      # 使用环境模拟动作\n",
        "      env.reset()\n",
        "      env.unwrapped.s = current_node.state\n",
        "      #assert env.unwrapped.s == current_state, f\"State mismatch: expected {current_state}, got {env.unwrapped.s}\"\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      # 如果是合法的下一个状态\n",
        "      if next_state not in visited:\n",
        "\n",
        "          # 使用价值网络评估优先级\n",
        "          _, value = self.evaluate_state(next_state)\n",
        "          next_node = Node(action_taken=action,\n",
        "                    state = next_state,\n",
        "                    parent=current_node)\n",
        "          current_node.children[action] = next_node\n",
        "          #print('value',value)\n",
        "          queue.put((-value, next_node))\n",
        "\n",
        "  return None, 0"
      ],
      "metadata": {
        "id": "sGQFFUaWasoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.Enhanced BFS"
      ],
      "metadata": {
        "id": "8SXsA70wfZ8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.1 version1.0"
      ],
      "metadata": {
        "id": "k5nmY5uzFDqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedBFS:\n",
        "  def __init__(self, network, num_simulations: int = 100):\n",
        "    self.network = network\n",
        "    self.num_simulations = num_simulations\n",
        "\n",
        "  def bfs_search(self, start_state):\n",
        "    \"\"\"使用真实环境进行局部BFS搜索\"\"\"\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    path_node = Node(action_taken=None,state = start_state)\n",
        "    #print('start_state',start_state)\n",
        "    counter = 100\n",
        "    queue = PriorityQueue()\n",
        "    queue.put((0, path_node))\n",
        "\n",
        "    def get_path_states(node):\n",
        "      # 获取当前路径上的所有状态\n",
        "      states = set()\n",
        "      current = node\n",
        "      while current:\n",
        "          states.add(current.state)\n",
        "          current = current.parent\n",
        "          #print('记录的',current.state)\n",
        "      return states\n",
        "    def get_action_path(node):\n",
        "      # 回溯完整路径\n",
        "      path_action = []\n",
        "      current = node\n",
        "      while current.parent:\n",
        "        path_action.append(current.action_taken)\n",
        "        #print('current.state',current.state)\n",
        "        current = current.parent\n",
        "      return list(reversed(path_action))\n",
        "    def get_path(node):\n",
        "      # 回溯完整路径\n",
        "      path = []\n",
        "      current = node\n",
        "      while current:\n",
        "        path.append(current.state)\n",
        "        current = current.parent\n",
        "      return path\n",
        "    while not queue.empty() and counter >=0:#\n",
        "      counter -= 1\n",
        "      priority, current_node = queue.get()\n",
        "      visited = get_path_states(current_node)\n",
        "      '''# 如果达到深度限制\n",
        "      if len(get_path(current_node)) > depth_limit:\n",
        "          continue'''\n",
        "      #reach to gaol\n",
        "      if current_node.state == 63 and len(get_path(current_node))<=16:\n",
        "        return get_action_path(current_node), 1\n",
        "      # 获取当前状态下的有效动作\n",
        "      row = current_node.state // 8#4\n",
        "      col = current_node.state % 8#4\n",
        "      valid_actions = []\n",
        "      if col > 0: valid_actions.append(0)    # 左\n",
        "      if row < 7: valid_actions.append(1)    # 下\n",
        "      if col < 7: valid_actions.append(2)    # 右\n",
        "      if row > 0: valid_actions.append(3)    # 上\n",
        "\n",
        "      for action in valid_actions:\n",
        "        # 使用环境模拟动作\n",
        "        env.reset()\n",
        "        env.unwrapped.s = current_node.state\n",
        "        #assert env.unwrapped.s == current_state, f\"State mismatch: expected {current_state}, got {env.unwrapped.s}\"\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        # 如果是合法的下一个状态\n",
        "        if next_state not in visited:\n",
        "\n",
        "            # 使用价值网络评估优先级\n",
        "            value = self.network.get_value(next_state)\n",
        "            next_node = Node(\n",
        "                      action_taken=action,\n",
        "                      state = next_state,\n",
        "                      parent=current_node)\n",
        "            current_node.children[action] = next_node\n",
        "            #print('value',value)\n",
        "            queue.put((-value, next_node))\n",
        "\n",
        "    return None, 0\n",
        "\n",
        "  def predict_next_state(self, current_state, action):\n",
        "\n",
        "    # 克隆环境，以避免影响原始环境\n",
        "    env_copy = gym.make('FrozenLake-v1',map_name=\"8x8\", render_mode=None)#map_name=\"8x8\",\n",
        "    env_copy.reset(seed=42)\n",
        "\n",
        "    # 设置环境状态\n",
        "    env_copy.unwrapped.s = current_state\n",
        "\n",
        "    # 执行动作并获取下一个状态\n",
        "    next_state, reward, terminated, truncated, info = env_copy.step(action)\n",
        "    return next_state\n",
        "\n",
        "  def search(self, root_state):\n",
        "\n",
        "    root = Node(action_taken=None,state = root_state)\n",
        "\n",
        "    for times in range(self.num_simulations):\n",
        "      best_path, path_value = self.bfs_search(root.state)\n",
        "      #cant find a path,use value net\n",
        "      if best_path is None:\n",
        "        best_actions = []\n",
        "        def get_value(state):\n",
        "          # 简单的价值计算：越接近目标价值越高\n",
        "          goal_row, goal_col = 7, 7  # 目标在(3,3)\n",
        "          current_row = state // 8\n",
        "          current_col = state % 8\n",
        "          manhattan_dist = abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "          return 1.0 / (manhattan_dist + 1)  # 避免除以零\n",
        "        for a in range(4):\n",
        "          next_state = self.predict_next_state(root.state, a)\n",
        "          next_state_value = get_value(next_state)\n",
        "          best_actions.append(next_state_value)\n",
        "        print('I find nothing!!!!!!!!!!!!!!!',[np.argmax(best_actions)])\n",
        "        self.expand_path(root,[np.argmax(best_actions)])\n",
        "      #find a path,just update\n",
        "      else:\n",
        "        print('I find something!!!!!!!!!!!!!!!',best_path)\n",
        "        self.expand_path(root,best_path)\n",
        "\n",
        "\n",
        "    best_visit_count = -1\n",
        "    best_action = None\n",
        "\n",
        "    for action, child_node in root.children.items():\n",
        "        if child_node.visit_count > best_visit_count:\n",
        "            best_visit_count = child_node.visit_count\n",
        "            best_action = action\n",
        "\n",
        "\n",
        "    return best_action\n",
        "\n",
        "  def expand_path(self, start_node: Node, action_list: List[int]) -> Tuple[bool, Node]:\n",
        "    \"\"\"\n",
        "    从起始节点按动作列表展开路径\n",
        "\n",
        "    Args:\n",
        "        start_node: 起始节点\n",
        "        action_list: 动作列表\n",
        "\n",
        "    Returns:\n",
        "        success: 是否成功展开完整路径\n",
        "        final_node: 最后到达的节点\n",
        "    \"\"\"\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    env.reset()\n",
        "    current_node = start_node\n",
        "    env.unwrapped.s = current_node.state\n",
        "\n",
        "\n",
        "\n",
        "    for action in action_list:\n",
        "        # 执行动作\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # 检查是否已经有这个子节点\n",
        "        if action in current_node.children:\n",
        "            # 使用已存在的节点\n",
        "            current_node = current_node.children[action]\n",
        "        else:\n",
        "            # 创建新节点\n",
        "            new_node = Node(\n",
        "                action_taken=action,\n",
        "                state=next_state,\n",
        "                parent=current_node,\n",
        "            )\n",
        "            # 标记父节点有子节点\n",
        "            current_node.has_children = True\n",
        "            current_node.children[action] = new_node\n",
        "            current_node = new_node\n",
        "\n",
        "\n",
        "        # 更新节点访问计数\n",
        "        current_node.visit_count += 1"
      ],
      "metadata": {
        "id": "ugwCmTEUfkWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2 version2.0 (simplified)"
      ],
      "metadata": {
        "id": "JDr9xanDFLFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class EnhancedBFS:\n",
        "    def __init__(self, network, num_simulations: int = 100, env_size: int = 8):\n",
        "        self.network = network\n",
        "        self.num_simulations = num_simulations\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size * env_size - 1\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        \"\"\"创建环境的工厂方法\"\"\"\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_valid_actions(state: int, env_size: int) -> List[int]:\n",
        "        \"\"\"获取有效动作列表\"\"\"\n",
        "        row, col = state // env_size, state % env_size\n",
        "        valid_actions = []\n",
        "        if col > 0: valid_actions.append(0)    # 左\n",
        "        if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "        if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "        if row > 0: valid_actions.append(3)    # 上\n",
        "        return valid_actions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_manhattan_distance(state: int, goal_state: int, env_size: int) -> float:\n",
        "        \"\"\"计算曼哈顿距离\"\"\"\n",
        "        current_row, current_col = state // env_size, state % env_size\n",
        "        goal_row, goal_col = goal_state // env_size, goal_state % env_size\n",
        "        return abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "\n",
        "    @lru_cache(maxsize=1024)\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        \"\"\"计算启发式值\"\"\"\n",
        "        manhattan_dist = self._get_manhattan_distance(state, self.goal_state, self.env_size)\n",
        "        return 1.0 / (manhattan_dist + 1)\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int]:\n",
        "        \"\"\"使用启发式引导的BFS搜索\"\"\"\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        start_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(start_node), start_node))\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                return self._get_action_path(current_node), 1\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node\n",
        "                    )\n",
        "                    priority = -self._calculate_heuristic(next_state)\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "        return None, 0\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        \"\"\"主搜索方法\"\"\"\n",
        "        root = Node(state=root_state)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            path, success = self.bfs_search(root.state)\n",
        "\n",
        "            if path is None:\n",
        "                best_action = self._get_best_heuristic_action(root.state)\n",
        "                self._expand_path(root, [best_action])\n",
        "            else:\n",
        "                self._expand_path(root, path)\n",
        "\n",
        "        return self._select_best_action(root)\n",
        "\n",
        "    def _get_best_heuristic_action(self, state: int) -> int:\n",
        "        \"\"\"获取基于启发式的最佳动作\"\"\"\n",
        "        valid_actions = self._get_valid_actions(state, self.env_size)\n",
        "        action_values = []\n",
        "\n",
        "        for action in valid_actions:\n",
        "            next_state = self._predict_next_state(state, action)\n",
        "            value = self._calculate_heuristic(next_state)\n",
        "            action_values.append((value, action))\n",
        "\n",
        "        return max(action_values)[1]\n",
        "\n",
        "    def _predict_next_state(self, state: int, action: int) -> int:\n",
        "        \"\"\"预测下一个状态\"\"\"\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state\n",
        "\n",
        "    def _expand_path(self, start_node: Node, action_list: List[int]) -> None:\n",
        "        \"\"\"展开路径\"\"\"\n",
        "        current_node = start_node\n",
        "\n",
        "        for action in action_list:\n",
        "            if action not in current_node.children:\n",
        "                next_state = self._predict_next_state(current_node.state, action)\n",
        "                current_node.children[action] = Node(\n",
        "                    state=next_state,\n",
        "                    action_taken=action,\n",
        "                    parent=current_node\n",
        "                )\n",
        "\n",
        "            current_node = current_node.children[action]\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_action_path(node: Node) -> List[int]:\n",
        "        \"\"\"获取动作路径\"\"\"\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    @staticmethod\n",
        "    def _select_best_action(root: Node) -> int:\n",
        "        \"\"\"选择最佳动作\"\"\"\n",
        "        return max(root.children.items(),\n",
        "                  key=lambda x: x[1].visit_count)[0]"
      ],
      "metadata": {
        "id": "5RxjUDtq8QLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.3 version3.0(Find action in BFS tree)"
      ],
      "metadata": {
        "id": "4ocrG8_V_5hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "  state: int\n",
        "  action_taken: Optional[int] = None\n",
        "  parent: Optional['Node'] = None\n",
        "  children: dict = field(default_factory=dict)\n",
        "  visit_count: int = 0\n",
        "  value: float = 0.0\n",
        "\n",
        "class EnhancedBFS:\n",
        "  def __init__(self, network, num_simulations: int = 100, env_size: int = 8):\n",
        "      self.network = network\n",
        "      self.num_simulations = num_simulations\n",
        "      self.env_size = env_size\n",
        "      self.env = self._create_env()\n",
        "      self.goal_state = env_size * env_size - 1\n",
        "\n",
        "  def _create_env(self) -> gym.Env:\n",
        "      \"\"\"创建环境的工厂方法\"\"\"\n",
        "      return gym.make('FrozenLake-v1',\n",
        "                      map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                      is_slippery=False,\n",
        "                      render_mode=None)\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_valid_actions(state: int, env_size: int) -> List[int]:\n",
        "      \"\"\"获取有效动作列表\"\"\"\n",
        "      row, col = state // env_size, state % env_size\n",
        "      valid_actions = []\n",
        "      if col > 0: valid_actions.append(0)    # 左\n",
        "      if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "      if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "      if row > 0: valid_actions.append(3)    # 上\n",
        "      return valid_actions\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_manhattan_distance(state: int, goal_state: int, env_size: int) -> float:\n",
        "      \"\"\"计算曼哈顿距离\"\"\"\n",
        "      current_row, current_col = state // env_size, state % env_size\n",
        "      goal_row, goal_col = goal_state // env_size, goal_state % env_size\n",
        "      return abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "\n",
        "  @lru_cache(maxsize=1024)\n",
        "  def _calculate_heuristic(self, state: int) -> float:\n",
        "      \"\"\"计算启发式值\"\"\"\n",
        "      manhattan_dist = self._get_manhattan_distance(state, self.goal_state, self.env_size)\n",
        "      return 1.0 / (manhattan_dist + 1)\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_action_path(node: Node) -> List[int]:\n",
        "\n",
        "      \"\"\"获取动作路径\"\"\"\n",
        "      path = []\n",
        "      current = node\n",
        "      while current.parent:\n",
        "          path.append(current.action_taken)\n",
        "          current = current.parent\n",
        "      return list(reversed(path))\n",
        "\n",
        "  def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "      \"\"\"返回路径、是否成功、以及根节点\"\"\"\n",
        "      visited = set()\n",
        "      queue = PriorityQueue()\n",
        "      root_node = Node(state=start_state)\n",
        "      queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "      found_goal = False\n",
        "      goal_node = None\n",
        "      counter = 10\n",
        "\n",
        "      while not queue.empty() and counter > 0:\n",
        "        counter -= 1\n",
        "        _, _, current_node = queue.get()\n",
        "\n",
        "        if current_node.state in visited:\n",
        "            continue\n",
        "\n",
        "        visited.add(current_node.state)\n",
        "        current_node.visit_count += 1  # 记录访问次数\n",
        "\n",
        "        if current_node.state == self.goal_state:\n",
        "            found_goal = True\n",
        "            goal_node = current_node\n",
        "            break\n",
        "\n",
        "        for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "            self.env.reset()\n",
        "            self.env.unwrapped.s = current_node.state\n",
        "            next_state, _, _, _, _ = self.env.step(action)\n",
        "\n",
        "            if next_state not in visited:\n",
        "              next_node = Node(\n",
        "                  state=next_state,\n",
        "                  action_taken=action,\n",
        "                  parent=current_node\n",
        "              )\n",
        "              current_node.children[action] = next_node  # 建立树结构\n",
        "              priority = -self._calculate_heuristic(next_state)\n",
        "              queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "      if found_goal:\n",
        "          return self._get_action_path(goal_node), 1, root_node\n",
        "      return None, 0, root_node\n",
        "\n",
        "\n",
        "  def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "      \"\"\"基于搜索树选择最佳动作\"\"\"\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      for action, child in root_node.children.items():\n",
        "          # 计算每个动作的价值\n",
        "          value = self._evaluate_subtree(child)\n",
        "          if value > best_value:\n",
        "              best_value = value\n",
        "              best_action = action\n",
        "\n",
        "      return best_action #if best_action is not None else self._get_best_heuristic_action(root_node.state)\n",
        "\n",
        "  def _evaluate_subtree(self, node: Node) -> float:\n",
        "      \"\"\"评估子树的价值\"\"\"\n",
        "      # 如果找到目标\n",
        "      if node.state == self.goal_state:\n",
        "          return float('inf')\n",
        "\n",
        "      # 综合考虑多个因素\n",
        "      visit_value = node.visit_count  # 访问次数说明这个方向被多次探索\n",
        "      heuristic_value = self._calculate_heuristic(node.state)  # 启发式值\n",
        "      children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "\n",
        "      return visit_value + heuristic_value + 0.5 * children_value  # 可以调整这些因素的权重\n",
        "\n",
        "  def search(self, root_state: int) -> int:\n",
        "    \"\"\"主搜索方法\"\"\"\n",
        "    _, _, root_node = self.bfs_search(root_state)\n",
        "    return self.get_best_action_from_tree(root_node)"
      ],
      "metadata": {
        "id": "0rqpzgv2Mpa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "value_net = Network()\n",
        "bfs = EnhancedBFS(value_net, num_simulations=10)\n",
        "action = bfs.search(45)\n",
        "print(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS_SjPNlDB2s",
        "outputId": "a795a02d-9ea5-4023-9ad5-031c25c7cf04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.4 version4.0(NN with Imitation-Learning)"
      ],
      "metadata": {
        "id": "siuEHgluLc5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.state_size = env_size * env_size  # one-hot编码状态\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(env_size**2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),       # 添加归一化\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.state_to_tensor(state)\n",
        "        return self.network(x)\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        # 将状态转换为one-hot向量\n",
        "        x = torch.zeros(self.state_size)\n",
        "        x[state] = 1.0\n",
        "        return x\n",
        "class ExpertDataCollector:\n",
        "    def __init__(self, env_size):\n",
        "        self.env_size = env_size\n",
        "        self.expert_data = []\n",
        "        # 创建环境以获取地图信息\n",
        "        self.env = gym.make('FrozenLake-v1',\n",
        "                           map_name=f\"{env_size}x{env_size}\",\n",
        "                           is_slippery=False)\n",
        "\n",
        "    def _is_valid_state(self, state: int) -> bool:\n",
        "        \"\"\"检查状态是否有效\"\"\"\n",
        "        # 获取地图\n",
        "        desc = self.env.unwrapped.desc.flatten()\n",
        "        # 检查是否是洞\n",
        "        return desc[state] != b'H'\n",
        "\n",
        "    def collect_from_bfs(self):\n",
        "      \"\"\"用BFS计算每个状态到终点的最短步数\"\"\"\n",
        "      goal = self.env_size**2 - 1\n",
        "      for state in range(self.env_size**2):\n",
        "          if not self._is_valid_state(state):\n",
        "              continue\n",
        "          # 运行BFS计算最短路径\n",
        "          queue = deque([(state, 0)])\n",
        "          visited = set()\n",
        "          while queue:\n",
        "              s, steps = queue.popleft()\n",
        "              if s == goal:\n",
        "                  self.expert_data.append((state, 1.0/(steps+1)))\n",
        "                  break\n",
        "              for action in range(4):\n",
        "                  self.env.unwrapped.s = s\n",
        "                  next_state, _, _, _, _ = self.env.step(action)\n",
        "                  if next_state not in visited and self._is_valid_state(next_state):\n",
        "                      visited.add(next_state)\n",
        "                      queue.append((next_state, steps+1))\n",
        "\n",
        "\n",
        "\n",
        "    def collect_from_manhattan(self):\n",
        "        \"\"\"使用曼哈顿距离启发式生成专家数据\"\"\"\n",
        "        goal_state = self.env_size * self.env_size - 1\n",
        "        for state in range(self.env_size * self.env_size):\n",
        "            if self._is_valid_state(state):\n",
        "                steps = self._manhattan_distance(state, goal_state)\n",
        "                value = 1.0 / (steps + 1)  # 归一化的价值\n",
        "                self.expert_data.append((state, value))\n",
        "\n",
        "    def _manhattan_distance(self, state, goal_state):\n",
        "        state_row, state_col = state // self.env_size, state % self.env_size\n",
        "        goal_row, goal_col = goal_state // self.env_size, goal_state % self.env_size\n",
        "        return abs(state_row - goal_row) + abs(state_col - goal_col)\n",
        "class ImitationBFS:\n",
        "    def __init__(self, env_size: int = 8):\n",
        "        self.env_size = env_size\n",
        "        self.env = gym.make('FrozenLake-v1',\n",
        "                           map_name=f\"{env_size}x{env_size}\",\n",
        "                           is_slippery=False)  # 确定性环境\n",
        "        self.goal_state = env_size * env_size - 1\n",
        "\n",
        "        # 初始化网络和传统启发式\n",
        "        self.value_net = ValueNetwork(env_size)\n",
        "        self.expert_collector = ExpertDataCollector(env_size)\n",
        "\n",
        "        # 训练网络\n",
        "        self._train_from_expert()\n",
        "\n",
        "    def _get_valid_actions(self, state: int) -> List[int]:\n",
        "        \"\"\"获取当前状态下合法动作\"\"\"\n",
        "        valid_actions = []\n",
        "        row = state // self.env_size\n",
        "        col = state % self.env_size\n",
        "        if col > 0: valid_actions.append(0)  # 左\n",
        "        if row < self.env_size-1: valid_actions.append(1)  # 下\n",
        "        if col < self.env_size-1: valid_actions.append(2)  # 右\n",
        "        if row > 0: valid_actions.append(3)  # 上\n",
        "        return valid_actions\n",
        "\n",
        "    def _train_from_expert(self):\n",
        "        # 收集专家数据\n",
        "        self.expert_collector.collect_from_manhattan()  # 启用数据收集\n",
        "\n",
        "        # 训练网络\n",
        "        optimizer = torch.optim.Adam(self.value_net.parameters())\n",
        "        for epoch in range(40):\n",
        "            total_loss = 0\n",
        "            for state, target_value in self.expert_collector.expert_data:\n",
        "                optimizer.zero_grad()\n",
        "                predicted_value = self.value_net(state)\n",
        "                loss = nn.MSELoss()(predicted_value, torch.tensor([target_value]))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        \"\"\"结合网络输出和曼哈顿距离\"\"\"\n",
        "        with torch.no_grad():\n",
        "            success_prob = self.value_net(state).item()\n",
        "\n",
        "        # 计算曼哈顿距离\n",
        "        current_row = state // self.env_size\n",
        "        current_col = state % self.env_size\n",
        "        goal_row = self.env_size - 1\n",
        "        goal_col = self.env_size - 1\n",
        "        manhattan_dist = abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "\n",
        "        # 加权组合（可调整系数）\n",
        "        return success_prob * 0.7 #+ (1/(manhattan_dist+0.1)) * 0.3\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_action_path(node: Node) -> List[int]:\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "        counter = 50\n",
        "\n",
        "        while not queue.empty() and counter > 0:\n",
        "            counter -= 1\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "\n",
        "            visited.add(current_node.state)\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                break\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state):\n",
        "                # 正确设置环境状态\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, _, _, _ = self.env.step(action)\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -self._calculate_heuristic(next_state)\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "        if found_goal:\n",
        "            return self._get_action_path(goal_node), 1, root_node\n",
        "        return None, 0, root_node\n",
        "\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "        def _evaluate_subtree(node: Node) -> float:\n",
        "            if node.state == self.goal_state:\n",
        "                return float('inf')\n",
        "\n",
        "            visit_value = node.visit_count\n",
        "            heuristic_value = self._calculate_heuristic(node.state)\n",
        "            children_value = max([_evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "            return visit_value * 0.6 + heuristic_value * 0.3 + children_value * 0.1\n",
        "\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        for action, child in root_node.children.items():\n",
        "            value = _evaluate_subtree(child)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        # 后备启发式策略\n",
        "        if best_action is None:\n",
        "            valid_actions = self._get_valid_actions(root_node.state)\n",
        "            return self._get_best_heuristic_action(root_node.state, valid_actions)\n",
        "        return best_action\n",
        "\n",
        "    def _get_best_heuristic_action(self, state: int, valid_actions: List[int]) -> int:\n",
        "        \"\"\"纯启发式策略\"\"\"\n",
        "        best_action = None\n",
        "        best_value = -float('inf')\n",
        "        for action in valid_actions:\n",
        "            self.env.unwrapped.s = state\n",
        "            next_state, _, _, _, _ = self.env.step(action)\n",
        "            value = self._calculate_heuristic(next_state)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "        return best_action if best_action is not None else valid_actions[0]\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        return self.get_best_action_from_tree(root_node)"
      ],
      "metadata": {
        "id": "eeSHoyUFLcHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "value_net = ValueNetwork(8)\n",
        "bfs = ImitationBFS()\n",
        "action = bfs.search(0)\n",
        "print(action)"
      ],
      "metadata": {
        "id": "WlFFG743SGx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.5 version5.0(NN with double network learning)"
      ],
      "metadata": {
        "id": "GdzwgKnQiizN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "# 神经网络模型定义\n",
        "class HeuristicNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()  # 输出0-1之间的启发值\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class NeuralEnhancedBFS:\n",
        "    def __init__(self, env_size: int = 8, num_simulations: int = 100,\n",
        "                 buffer_size: int = 10000, batch_size: int = 32):\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size**2 - 1\n",
        "\n",
        "        # 神经网络相关配置\n",
        "        self.model = HeuristicNetwork(env_size)\n",
        "        self.target_model = HeuristicNetwork(env_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # 经验回放缓存\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络同步间隔\n",
        "        self.target_update_interval = 15\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    # ... (保留原有的_get_valid_actions和_get_action_path方法)\n",
        "    def _get_valid_actions(self,state: int, env_size: int) -> List[int]:\n",
        "      \"\"\"获取有效动作列表\"\"\"\n",
        "      row, col = state // env_size, state % env_size\n",
        "      valid_actions = []\n",
        "      if col > 0: valid_actions.append(0)    # 左\n",
        "      if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "      if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "      if row > 0: valid_actions.append(3)    # 上\n",
        "      return valid_actions\n",
        "\n",
        "    def _get_action_path(self,node: Node) -> List[int]:\n",
        "        \"\"\"获取动作路径\"\"\"\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        \"\"\"使用神经网络预测启发值\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.model(state_tensor).item()\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        \"\"\"训练网络\"\"\"\n",
        "        states = torch.LongTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 定期更新目标网络\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def _remember(self, state, target):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target))\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"经验回放\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "    def _get_bootstrap_target(self, state):\n",
        "        \"\"\"使用目标网络生成训练目标\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.target_model(state_tensor).item()\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "            # 收集训练数据\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                self._remember(current_node.parent.state, target)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                # 传播成功信号\n",
        "                self._remember(current_node.state, 1.0)\n",
        "                break\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0)  # 记录失败状态\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "            # 进行经验回放\n",
        "            self._replay()\n",
        "\n",
        "        if found_goal:\n",
        "          current = goal_node\n",
        "          while current.parent:\n",
        "              current.value += 1.0  # 或其他奖励值\n",
        "              current = current.parent\n",
        "\n",
        "          return self._get_action_path(goal_node), 1, root_node\n",
        "        return None, 0, root_node\n",
        "\n",
        "    # ... (保留其他辅助方法)\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "      \"\"\"基于搜索树选择最佳动作\"\"\"\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      for action, child in root_node.children.items():\n",
        "          # 计算每个动作的价值\n",
        "          value = self._evaluate_subtree(child)\n",
        "          if value > best_value:\n",
        "              best_value = value\n",
        "              best_action = action\n",
        "\n",
        "      return best_action #if best_action is not None else self._get_best_heuristic_action(root_node.state)\n",
        "\n",
        "    def _evaluate_subtree(self, node: Node) -> float:\n",
        "        \"\"\"评估子树的价值\"\"\"\n",
        "        # 如果找到目标\n",
        "        if node.state == self.goal_state:\n",
        "            return float('inf')\n",
        "\n",
        "        # 综合考虑多个因素\n",
        "        visit_value = node.visit_count  # 访问次数说明这个方向被多次探索\n",
        "        heuristic_value = node.value#self._calculate_heuristic(node.state)  # 启发式值\n",
        "        children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "        return visit_value + heuristic_value + 0.5 * children_value  # 可以调整这些因素的权重\n",
        "\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        best_action = self.get_best_action_from_tree(root_node)\n",
        "\n",
        "        '''# 使用最终结果更新网络\n",
        "        if best_action is not None:\n",
        "            next_state = self._simulate_step(root_state, best_action)\n",
        "            if next_state == self.goal_state:\n",
        "                self._remember(root_state, 1.0)\n",
        "            else:\n",
        "                self._remember(root_state, self._get_bootstrap_target(next_state))'''\n",
        "        return best_action\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state"
      ],
      "metadata": {
        "id": "w1YZ3KRMf-kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.5 TEST"
      ],
      "metadata": {
        "id": "yljnI18pFanY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def test_enhanced_bfs():\n",
        "    # 1. 创建简单的价值网络\n",
        "\n",
        "    # 2. 初始化环境和算法\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    #value_net = ValueNetwork(8)\n",
        "\n",
        "    bfs = NeuralEnhancedBFS()#EnhancedBFS(value_net, num_simulations=10)#Neural\n",
        "\n",
        "\n",
        "    # 3. 运行多个回合\n",
        "    num_episodes = 1\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\n开始测试Enhanced BFS...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        print(f\"\\n回合 {episode + 1}:\")\n",
        "        print(f\"起始状态: {state}\")\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # 使用算法选择动作\n",
        "            action = bfs.search(state)\n",
        "            print(f\"Steps {steps}: 在状态 {state} 选择动作 {action}\")\n",
        "\n",
        "            # 执行动作\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            print(f\"-> 新状态: {state}, 奖励: {reward}\")\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"成功到达目标！\")\n",
        "                else:\n",
        "                    print(\"失败（掉入陷阱或超时）\")\n",
        "\n",
        "        total_reward += episode_reward\n",
        "        print(f\"回合 {episode + 1} 结束 - 总步数: {steps}, 总奖励: {episode_reward}\")\n",
        "\n",
        "    print(f\"\\n测试完成 - 平均奖励: {total_reward/num_episodes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_bfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_UnxrzIDSyUn",
        "outputId": "d4db212b-0cc9-4d7a-dc2a-a20ac38547c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "开始测试Enhanced BFS...\n",
            "\n",
            "回合 1:\n",
            "起始状态: 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4753377139568329 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4753377139568329 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4917571544647217 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4926496744155884 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4926496744155884 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.498965859413147 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4926496744155884 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4917571544647217 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5086429119110107 0.4926496744155884\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5093880891799927 1.754967749118805\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4926496744155884 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5162313580513 2.386871963739395\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5082707405090332 2.7096673399209976\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.474857360124588 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5103021264076233 2.863104410469532\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4808589816093445 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5020257234573364 2.9418543316423893\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5091415047645569 2.972952889278531\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4808589816093445 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5017077922821045 2.9956179494038224\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4808589816093445 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4685453772544861 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4853469133377075 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4853469133377075 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4798348844051361 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4798348844051361 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5144664645195007 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5074318647384644 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5268012285232544 0.5144664645195007\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4888739585876465 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5144664645195007 0.4888739585876465\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.500645637512207 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5264954566955566 0.500645637512207\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.532108724117279 1.7768182754516602\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.510492742061615 2.420517861843109\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5106263160705566 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.51530921459198 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5077234506607056 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5146406888961792 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.514285147190094 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5077234506607056 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5223299264907837 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5077234506607056 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.504464328289032 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5195905566215515 0.5077234506607056\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5115946531295776 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.504464328289032 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5032784342765808 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5267997980117798 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5207397937774658 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5032784342765808 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5211381316184998 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5157476663589478 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5316868424415588 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5016215443611145 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5044202208518982 inf\n",
            "Steps 0: 在状态 0 选择动作 2\n",
            "-> 新状态: 1, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46646633744239807 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49878957867622375 0.46646633744239807\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46646633744239807 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48608118295669556 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4881247282028198 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4881247282028198 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49363893270492554 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4881247282028198 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48608118295669556 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5025113224983215 0.4881247282028198\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.503415584564209 1.7465736865997314\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4881247282028198 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.509476363658905 2.3767024278640747\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5021584630012512 2.6978275775909424\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47148212790489197 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5053742527961731 2.8510722517967224\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4751622676849365 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4958162009716034 2.9309103786945343\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5049788951873779 2.9612713903188705\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4751622676849365 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49303627014160156 2.985614590346813\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4751622676849365 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46189144253730774 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4806824028491974 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4806824028491974 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47476211190223694 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47476211190223694 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5020725131034851 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5052825212478638 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.509809136390686 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5031520128250122 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5088780522346497 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5077872276306152 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5031520128250122 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5162410140037537 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.5031520128250122 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4995287358760834 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5148458480834961 0.5031520128250122\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5058988928794861 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4995287358760834 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49763280153274536 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5181554555892944 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5152648687362671 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49763280153274536 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.515307605266571 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5106069445610046 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.525802731513977 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49536359310150146 inf\n",
            "Steps 1: 在状态 1 选择动作 2\n",
            "-> 新状态: 2, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4958362281322479 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46986985206604004 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45551997423171997 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4768607020378113 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4768607020378113 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4705911874771118 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4705911874771118 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4947749376296997 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5004807114601135 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5050092339515686 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49890270829200745 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5034701228141785 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.501283586025238 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49890270829200745 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5104706287384033 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49890270829200745 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4950605630874634 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5096728205680847 0.49890270829200745\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5004649758338928 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4950605630874634 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4922814667224884 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5105055570602417 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5102578997612 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4922814667224884 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.509950578212738 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5055174827575684 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5204075574874878 inf\n",
            "Steps 2: 在状态 2 选择动作 2\n",
            "-> 新状态: 3, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4843551516532898 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.44930070638656616 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47311002016067505 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47311002016067505 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46677833795547485 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46677833795547485 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4880281686782837 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4956951141357422 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5002703666687012 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4945853650569916 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4981449842453003 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4947444498538971 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4945853650569916 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5047709345817566 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4945853650569916 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49079757928848267 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5041468739509583 0.4945853650569916\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49513864517211914 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49079757928848267 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4872080683708191 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.503013014793396 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5054140686988831 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4872080683708191 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5048559308052063 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5004064440727234 inf\n",
            "Steps 3: 在状态 3 选择动作 2\n",
            "-> 新状态: 4, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.479621559381485 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4432252049446106 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5095611810684204 0.479621559381485\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46940967440605164 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46940967440605164 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4629769027233124 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4629769027233124 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4813533425331116 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.490605890750885 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49501749873161316 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49047523736953735 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4927257299423218 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48786240816116333 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49047523736953735 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4987676739692688 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.49047523736953735 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48666897416114807 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4988221228122711 0.49047523736953735\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49018222093582153 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48666897416114807 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4821837246417999 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4955102503299713 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5006147623062134 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4821837246417999 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49968940019607544 inf\n",
            "Steps 4: 在状态 4 选择动作 2\n",
            "-> 新状态: 5, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4763083755970001 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4388495683670044 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5051208138465881 0.4763083755970001\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46648502349853516 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49074870347976685 1.7432750016450882\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46648502349853516 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45952415466308594 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45952415466308594 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48222142457962036 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4871513545513153 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4937743544578552 0.4871513545513153\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48222142457962036 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48620355129241943 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49038419127464294 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4878954589366913 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4871513545513153 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48337864875793457 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49441295862197876 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4862264394760132 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48337864875793457 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47814399003982544 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4890892207622528 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4968932867050171 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47814399003982544 0\n",
            "Steps 5: 在状态 5 选择动作 1\n",
            "-> 新状态: 13, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4640994668006897 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45629245042800903 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45629245042800903 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4772662818431854 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4772662818431854 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48260360956192017 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4864741265773773 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48355111479759216 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4844476878643036 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48963281512260437 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4844476878643036 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4806826412677765 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49055641889572144 0.4844476878643036\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4830233156681061 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4806826412677765 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47472086548805237 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48332756757736206 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4736254811286926 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.43514975905418396 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.5011641383171082 0.4736254811286926\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4640994668006897 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4868941605091095 1.7379768788814545\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47472086548805237 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49200403690338135 2.3558825999498367\n",
            "Steps 6: 在状态 13 选择动作 2\n",
            "-> 新状态: 14, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4615250825881958 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4526118338108063 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4708154797554016 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.43147146701812744 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4968733787536621 0.4708154797554016\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4615250825881958 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48267191648483276 1.732281118631363\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4709740877151489 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48802000284194946 2.348812475800514\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4907451868057251 2.6624262407422066\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4526118338108063 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4723247289657593 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.48217445611953735 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48581868410110474 0.48217445611953735\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4723247289657593 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.479103684425354 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4824487268924713 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4790792763233185 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.48217445611953735 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47788894176483154 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4869476854801178 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47983938455581665 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47788894176483154 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4709740877151489 0\n",
            "Steps 7: 在状态 14 选择动作 1\n",
            "-> 新状态: 22, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4491212069988251 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4673386514186859 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4673386514186859 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4756213426589966 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4787161350250244 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47484683990478516 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4799267053604126 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4819818139076233 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4799267053604126 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4751244783401489 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.483492910861969 0.4799267053604126\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47166359424591064 0\n",
            "Steps 8: 在状态 22 选择动作 1\n",
            "-> 新状态: 30, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46291670203208923 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46291670203208923 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4729633331298828 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4754266142845154 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4711523652076721 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.446039080619812 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4663890600204468 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47417280077934265 0.4663890600204468\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4663890600204468 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46396613121032715 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4933737516403198 0.46396613121032715\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4726153016090393 1.7253568172454834\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4803674519062042 2.335293710231781\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4782206416130066 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.47417280077934265 0\n",
            "Steps 9: 在状态 30 选择动作 2\n",
            "-> 新状态: 31, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4585493206977844 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4714919924736023 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47511669993400574 0.4714919924736023\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4585493206977844 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4703810513019562 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47219422459602356 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4674265384674072 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.44275325536727905 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46114593744277954 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4714919924736023 0.46114593744277954\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46114593744277954 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4608391523361206 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.49091073870658875 0.4608391523361206\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.469753235578537 1.721330314874649\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47726237773895264 2.3304183930158615\n",
            "Steps 10: 在状态 31 选择动作 1\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4545212686061859 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4682852029800415 inf\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.46933943033218384 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4545212686061859 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4692293405532837 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47240951657295227 0.4692293405532837\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4396524727344513 0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.45619744062423706 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4692293405532837 0.45619744062423706\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.4668234884738922 0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.4745170772075653 1.6973280608654022\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.47540706396102905 2.3231811076402664\n",
            "Steps 11: 在状态 39 选择动作 1\n",
            "-> 新状态: 47, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "1 0.46637141704559326 inf\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46157535910606384 0\n",
            "Steps 12: 在状态 47 选择动作 1\n",
            "-> 新状态: 55, 奖励: 0.0\n",
            "(visit_value heuristic_value children_value\n",
            "0 0.46427324414253235 0\n",
            "Steps 13: 在状态 55 选择动作 1\n",
            "-> 新状态: 63, 奖励: 1.0\n",
            "成功到达目标！\n",
            "回合 1 结束 - 总步数: 14, 总奖励: 1.0\n",
            "\n",
            "测试完成 - 平均奖励: 1.0\n"
          ]
        }
      ]
    }
  ]
}