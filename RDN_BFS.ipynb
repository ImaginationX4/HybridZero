{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/BvfmVrO04lQvhfetQYb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/RDN_BFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JRJ54ICp0SL",
        "outputId": "ba68071b-ed69-4efc-a44e-ee36238ebd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Learn RDN"
      ],
      "metadata": {
        "id": "xnkrRrmKp8Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 超参数\n",
        "GAMMA = 0.99           # 折扣因子\n",
        "EPSILON_START = 1.0    # 初始 ε (探索率)\n",
        "EPSILON_END = 0.01     # 最终 ε\n",
        "EPSILON_DECAY = 500    # ε 衰减步数\n",
        "BATCH_SIZE = 32        # 批大小\n",
        "MEMORY_SIZE = 10000    # 经验回放缓冲区大小\n",
        "TARGET_UPDATE = 10     # 目标网络更新频率\n",
        "LEARNING_RATE = 0.001  # 学习率\n",
        "EPISODES = 500         # 训练回合数\n",
        "RND_SCALE = 1.0        # RND 内部奖励的缩放因子\n",
        "\n",
        "# 设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# DQN 网络\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# RND 网络\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, state_size, output_size=32):\n",
        "        super(RNDNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# 智能体\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "        self.epsilon = EPSILON_START\n",
        "\n",
        "        # DQN 网络\n",
        "        self.policy_net = DQN(state_size, action_size).to(device)\n",
        "        self.target_net = DQN(state_size, action_size).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        # RND 网络\n",
        "        self.random_net = RNDNetwork(state_size).to(device)  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork(state_size).to(device)  # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    def select_action(self, state, step):\n",
        "        self.epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-step / EPSILON_DECAY)\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def compute_rnd_reward(self, next_state):\n",
        "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            target = self.random_net(next_state)\n",
        "        prediction = self.predictor_net(next_state)\n",
        "        rnd_reward = (target - prediction).pow(2).mean()\n",
        "        return rnd_reward.item() * RND_SCALE\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "        # DQN 更新\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.target_net(next_states).max(1)[0]\n",
        "        targets = rewards + (1 - dones) * GAMMA * next_q_values\n",
        "\n",
        "        dqn_loss = nn.MSELoss()(q_values, targets.detach())\n",
        "        self.optimizer.zero_grad()\n",
        "        dqn_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # RND 更新\n",
        "        predictor_outputs = self.predictor_net(next_states)\n",
        "        random_outputs = self.random_net(next_states).detach()\n",
        "        rnd_loss = (predictor_outputs - random_outputs).pow(2).mean()\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IQokjAXXp1G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.RDN with BFS"
      ],
      "metadata": {
        "id": "UlQ9OAFpsVfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 FROZEN-LAKE"
      ],
      "metadata": {
        "id": "w0O2MhHawpVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "# 神经网络模型定义\n",
        "class HeuristicNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()  # 输出0-1之间的启发值\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "# RND 网络定义\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, env_size, output_size=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class NeuralEnhancedBFSwithRND:\n",
        "    def __init__(self, env_size: int = 8, num_simulations: int = 100,\n",
        "                 buffer_size: int = 10000, batch_size: int = 32, rnd_scale: float = 0.40):\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size**2 - 1\n",
        "\n",
        "        # 启发式网络配置\n",
        "        self.model = HeuristicNetwork(env_size)\n",
        "        self.target_model = HeuristicNetwork(env_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # RND 网络配置\n",
        "        self.random_net = RNDNetwork(env_size).eval()  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork(env_size)      # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=0.001)\n",
        "        self.rnd_scale = rnd_scale  # RND 内部奖励缩放因子\n",
        "\n",
        "        # 经验回放缓存\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络同步间隔\n",
        "        self.target_update_interval = 15\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "        self.rnd_scale = rnd_scale\n",
        "        self.rnd_mean = 0.0  # 移动平均\n",
        "        self.rnd_std = 1.0   # 移动标准差\n",
        "        self.rnd_count = 0   # 统计次数\n",
        "        self.rnd_m2 = 0.0    # 用于 Welford 法计算方差\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    def _get_valid_actions(self, state: int, env_size: int) -> List[int]:\n",
        "        row, col = state // env_size, state % env_size\n",
        "        valid_actions = []\n",
        "        if col > 0: valid_actions.append(0)    # 左\n",
        "        if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "        if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "        if row > 0: valid_actions.append(3)    # 上\n",
        "        return valid_actions\n",
        "\n",
        "    def _get_action_path(self, node: Node) -> List[int]:\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.model(state_tensor).item()\n",
        "\n",
        "    def _calculate_rnd_reward(self, state: int) -> float:\n",
        "        \"\"\"计算 RND 内部奖励\"\"\"\n",
        "        state_tensor = torch.LongTensor([state])\n",
        "        with torch.no_grad():\n",
        "            target = self.random_net(state_tensor)\n",
        "            prediction = self.predictor_net(state_tensor)\n",
        "        raw_rnd = (target - prediction).pow(2).mean().item()\n",
        "\n",
        "        '''# Welford 在线算法更新均值和标准差\n",
        "        self.rnd_count += 1\n",
        "        delta = raw_rnd - self.rnd_mean\n",
        "        self.rnd_mean += delta / self.rnd_count\n",
        "        delta2 = raw_rnd - self.rnd_mean\n",
        "        self.rnd_m2 += delta * delta2\n",
        "        if self.rnd_count > 1:\n",
        "            self.rnd_std = np.sqrt(self.rnd_m2 / (self.rnd_count - 1))\n",
        "\n",
        "        # 归一化并缩放\n",
        "        normalized_rnd = (raw_rnd - self.rnd_mean) / (self.rnd_std + 1e-5)  # 避免除零\n",
        "        scaled_rnd = self.rnd_scale * normalized_rnd\n",
        "\n",
        "        # 裁剪到合理范围'''\n",
        "        return raw_rnd * 0.2 #根据论文建议，限制最大值\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        states = torch.LongTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def _update_rnd_network(self, states):\n",
        "        \"\"\"更新 RND 的预测网络\"\"\"\n",
        "        states = torch.LongTensor(states)\n",
        "        predictions = self.predictor_net(states)\n",
        "        with torch.no_grad():\n",
        "            targets = self.random_net(states)\n",
        "        rnd_loss = (predictions - targets).pow(2).mean()\n",
        "\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "    def _remember(self, state, target, next_state=None):\n",
        "        \"\"\"存储经验，包括 RND 的状态\"\"\"\n",
        "        self.replay_buffer.append((state, target, next_state if next_state is not None else state))\n",
        "\n",
        "    def _replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets, next_states = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        # 更新启发式网络\n",
        "        self.train_step_counter += 1\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "        # 更新 RND 网络\n",
        "        self._update_rnd_network(next_states)\n",
        "\n",
        "    def _get_bootstrap_target(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.target_model(state_tensor).item()\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                self._remember(current_node.state, 1.0, current_node.state)\n",
        "                break\n",
        "            # 收集训练数据，包括 RND 奖励\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                rnd_reward = self._calculate_rnd_reward(current_node.state)\n",
        "                total_target = min(target + rnd_reward, 1.0)  # 限制最大值为1\n",
        "                self._remember(current_node.parent.state, total_target, current_node.state)\n",
        "\n",
        "\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0, next_state)\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state) + self._calculate_rnd_reward(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "            self._replay()\n",
        "\n",
        "        if found_goal:\n",
        "            current = goal_node\n",
        "            while current.parent:\n",
        "                current.value += 1.0\n",
        "                current = current.parent\n",
        "\n",
        "            #return self._get_action_path(goal_node), 1, root_node\n",
        "        return None, 0, root_node\n",
        "\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        for action, child in root_node.children.items():\n",
        "            value = self._evaluate_subtree(child)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        return best_action #if best_action is not None else random.choice(self._get_valid_actions(root_node.state, self.env_size))\n",
        "\n",
        "    def _evaluate_subtree(self, node: Node) -> float:\n",
        "        if node.state == self.goal_state:\n",
        "            return float('inf')\n",
        "\n",
        "        heuristic_value = self._calculate_heuristic(node.state)\n",
        "        rnd_reward = self._calculate_rnd_reward(node.state)\n",
        "        children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "        return node.value + 0.5 * children_value\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        best_action = self.get_best_action_from_tree(root_node)\n",
        "        return best_action\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state\n"
      ],
      "metadata": {
        "id": "TBPrpee3uH93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def test_enhanced_bfs():\n",
        "    # 1. 创建简单的价值网络\n",
        "\n",
        "    # 2. 初始化环境和算法\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    #value_net = ValueNetwork(8)\n",
        "\n",
        "    bfs = NeuralEnhancedBFSwithRND()#EnhancedBFS(value_net, num_simulations=10)#Neural\n",
        "\n",
        "\n",
        "    # 3. 运行多个回合\n",
        "    num_episodes = 1\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\n开始测试NeuralEnhancedBFSwithRND...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        print(f\"\\n回合 {episode + 1}:\")\n",
        "        print(f\"起始状态: {state}\")\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # 使用算法选择动作\n",
        "            action = bfs.search(state)\n",
        "            print(f\"Steps {steps}: 在状态 {state} 选择动作 {action}\")\n",
        "\n",
        "            # 执行动作\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            print(f\"-> 新状态: {state}, 奖励: {reward}\")\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"成功到达目标！\")\n",
        "                else:\n",
        "                    print(\"失败（掉入陷阱或超时）\")\n",
        "\n",
        "        total_reward += episode_reward\n",
        "        print(f\"回合 {episode + 1} 结束 - 总步数: {steps}, 总奖励: {episode_reward}\")\n",
        "\n",
        "    print(f\"\\n测试完成 - 平均奖励: {total_reward/num_episodes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_bfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N6H2r-2WvJmW",
        "outputId": "56ed3fd8-c4d2-4bf0-fa8b-484b504f0771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "开始测试NeuralEnhancedBFSwithRND...\n",
            "\n",
            "回合 1:\n",
            "起始状态: 0\n",
            "Steps 0: 在状态 0 选择动作 2\n",
            "-> 新状态: 1, 奖励: 0.0\n",
            "Steps 1: 在状态 1 选择动作 1\n",
            "-> 新状态: 9, 奖励: 0.0\n",
            "Steps 2: 在状态 9 选择动作 1\n",
            "-> 新状态: 17, 奖励: 0.0\n",
            "Steps 3: 在状态 17 选择动作 2\n",
            "-> 新状态: 18, 奖励: 0.0\n",
            "Steps 4: 在状态 18 选择动作 1\n",
            "-> 新状态: 26, 奖励: 0.0\n",
            "Steps 5: 在状态 26 选择动作 2\n",
            "-> 新状态: 27, 奖励: 0.0\n",
            "Steps 6: 在状态 27 选择动作 2\n",
            "-> 新状态: 28, 奖励: 0.0\n",
            "Steps 7: 在状态 28 选择动作 3\n",
            "-> 新状态: 20, 奖励: 0.0\n",
            "Steps 8: 在状态 20 选择动作 2\n",
            "-> 新状态: 21, 奖励: 0.0\n",
            "Steps 9: 在状态 21 选择动作 2\n",
            "-> 新状态: 22, 奖励: 0.0\n",
            "Steps 10: 在状态 22 选择动作 1\n",
            "-> 新状态: 30, 奖励: 0.0\n",
            "Steps 11: 在状态 30 选择动作 2\n",
            "-> 新状态: 31, 奖励: 0.0\n",
            "Steps 12: 在状态 31 选择动作 1\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "Steps 13: 在状态 39 选择动作 1\n",
            "-> 新状态: 47, 奖励: 0.0\n",
            "Steps 14: 在状态 47 选择动作 1\n",
            "-> 新状态: 55, 奖励: 0.0\n",
            "Steps 15: 在状态 55 选择动作 1\n",
            "-> 新状态: 63, 奖励: 1.0\n",
            "成功到达目标！\n",
            "回合 1 结束 - 总步数: 16, 总奖励: 1.0\n",
            "\n",
            "测试完成 - 平均奖励: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 CARTPOLE"
      ],
      "metadata": {
        "id": "m6MsCdcOwd55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(4, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        # 初始化最后一层权重为接近0的小值\n",
        "        nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, state_size=4, output_size=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        return self.fc(x)\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: np.ndarray\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    value: float = 0.0\n",
        "    depth: int = 0\n",
        "    done: bool = False\n",
        "    reward: float = 0.0\n",
        "    visit_count: int = 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.parent is not None:\n",
        "            self.depth = self.parent.depth + 1\n",
        "\n",
        "class BFS:\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 40,\n",
        "                 buffer_size: int = 1000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 rnd_scale: float = 0.5):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.num_simulations = num_simulations\n",
        "        self.gamma = gamma\n",
        "        self.exploration_weight = 1.5\n",
        "        self.initial_epsilon = 0.2\n",
        "\n",
        "\n",
        "        # 启发式网络配置\n",
        "        self.model = ValueNetwork()\n",
        "        self.target_model = ValueNetwork()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # RND 网络配置\n",
        "        self.random_net = RNDNetwork().eval()  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork()      # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=0.001)\n",
        "        self.rnd_scale = rnd_scale  # RND 内部奖励缩放因子\n",
        "\n",
        "        # 经验回放\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络更新参数\n",
        "        self.train_step_counter = 0\n",
        "        self.target_update_interval = 15\n",
        "        self.tau = 0.005  # 软更新参数\n",
        "\n",
        "        self.rnd_scale = rnd_scale\n",
        "        self.rnd_mean = 0.0  # 移动平均\n",
        "        self.rnd_std = 1.0   # 移动标准差\n",
        "        self.rnd_count = 0   # 统计次数\n",
        "        self.rnd_m2 = 0.0    # 用于 Welford 法计算方差\n",
        "\n",
        "    def _calculate_value(self, state: np.ndarray) -> Tuple[float, float]:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            return self.model(state_tensor).item()\n",
        "    def _calculate_rnd_reward(self, state: np.array) -> float:\n",
        "      state_tensor = torch.FloatTensor(state)\n",
        "      with torch.no_grad():\n",
        "          target = self.random_net(state_tensor)\n",
        "          prediction = self.predictor_net(state_tensor)\n",
        "      raw_rnd = (target - prediction).pow(2).mean().item()\n",
        "      return self.rnd_scale * raw_rnd\n",
        "\n",
        "    def bfs_search1(self, current_state: np.ndarray) -> int:\n",
        "      root = Node(current_state)\n",
        "      queue = PriorityQueue()\n",
        "      queue.put((0, 0, root))\n",
        "\n",
        "      temp_env = gym.make('CartPole-v1')\n",
        "      temp_env.reset()\n",
        "      temp_env.unwrapped.state = current_state.copy()\n",
        "\n",
        "      for _ in range(self.num_simulations):\n",
        "          if queue.empty():\n",
        "              break\n",
        "          _, _, current_node = queue.get()\n",
        "          for action in range(self.env.action_space.n):\n",
        "              temp_env.unwrapped.state = current_node.state.copy()\n",
        "              next_state, reward, done, _, _ = temp_env.step(action)\n",
        "              next_node = Node(state=next_state, action_taken=action, parent=current_node, reward=reward, done=done)\n",
        "              next_value = self._calculate_value(next_state)\n",
        "              rnd_reward = self._calculate_rnd_reward(next_state)\n",
        "              next_node.value = reward + (1 - done) * self.gamma * next_value + rnd_reward\n",
        "              current_node.children[action] = next_node\n",
        "              self._backpropagate(next_node, next_node.value)\n",
        "              uct = next_node.value + self.exploration_weight * rnd_reward\n",
        "              queue.put((-uct, id(next_node), next_node))\n",
        "          current_node.visit_count += 1\n",
        "          if current_node.parent is not None:\n",
        "              self._remember(current_node.parent.state, current_node.value)\n",
        "\n",
        "      best_action = max([0, 1], key=lambda a: root.children[a].value)\n",
        "      return best_action, root\n",
        "\n",
        "    def bfs_search(self, current_state: np.ndarray) -> int:\n",
        "        \"\"\"执行一次BFS搜索返回最佳动作\"\"\"\n",
        "        root = Node(current_state)\n",
        "        queue = PriorityQueue()\n",
        "\n",
        "        # 计算初始状态的价值\n",
        "\n",
        "        queue.put((0, 0, root))\n",
        "\n",
        "        temp_env = gym.make('CartPole-v1')\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            if queue.empty():\n",
        "                break\n",
        "\n",
        "            priority, _, current_node = queue.get()\n",
        "\n",
        "            # 扩展子节点\n",
        "            for action in range(self.env.action_space.n):\n",
        "                temp_env.reset()\n",
        "                temp_env.unwrapped.state = current_node.state.copy()\n",
        "                next_state, reward, done, _, _ = temp_env.step(action)\n",
        "\n",
        "                next_node = Node(\n",
        "                    state=next_state,\n",
        "                    action_taken=action,\n",
        "                    parent=current_node,\n",
        "                    reward=reward,\n",
        "                    done=done\n",
        "                )\n",
        "\n",
        "                # 使用集成网络预测价值\n",
        "                next_value = self._calculate_value(next_state) #+ self._calculate_rnd_reward(next_state)\n",
        "\n",
        "\n",
        "                # 终止状态处理\n",
        "                if done:\n",
        "                    next_node.value = reward #+ self._calculate_rnd_reward(next_state)\n",
        "                else:\n",
        "                    next_node.value = next_value\n",
        "\n",
        "                current_node.children[action] = next_node\n",
        "                self._backpropagate(next_node, next_value)\n",
        "\n",
        "                # 计算优先级（UCT + 不确定性奖励）\n",
        "\n",
        "                uct = next_node.value + self._calculate_rnd_reward(next_state)\n",
        "\n",
        "                '''# 随机探索\n",
        "                epsilon = max(0.01, self.initial_epsilon * (0.995 ** self.train_step_counter))\n",
        "                if np.random.rand() < 0.2:\n",
        "                    uct *= 2'''\n",
        "\n",
        "                queue.put((-uct, id(next_node), next_node))\n",
        "\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "            # 经验回填\n",
        "            if current_node.parent is not None:\n",
        "\n",
        "\n",
        "              with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(current_node.state).unsqueeze(0)\n",
        "                target_value=self.target_model(state_tensor).item()\n",
        "\n",
        "              total_target = self.gamma * target_value + current_node.reward  # 限制最大值为1\n",
        "              if done:\n",
        "                total_target =1\n",
        "              self._remember(current_node.parent.state, total_target)\n",
        "\n",
        "            # 训练网络\n",
        "            if len(self.replay_buffer) >= self.batch_size:\n",
        "                self._replay()\n",
        "\n",
        "        # 选择最佳动作\n",
        "        best_action = max([0, 1], key=lambda a: root.children[a].value)\n",
        "\n",
        "        return best_action, root\n",
        "\n",
        "    def _backpropagate(self, node: Node, value: float):\n",
        "        \"\"\"回溯更新节点价值\"\"\"\n",
        "        while node is not None:\n",
        "            node.value = max(node.value, value)\n",
        "            node = node.parent\n",
        "\n",
        "    def _remember(self, state: np.ndarray, target_value: float):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target_value))\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        states = torch.FloatTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "    def _update_rnd_network(self, states):\n",
        "        \"\"\"更新 RND 的预测网络\"\"\"\n",
        "        states = torch.FloatTensor(states)\n",
        "        predictions = self.predictor_net(states)\n",
        "        with torch.no_grad():\n",
        "            targets = self.random_net(states)\n",
        "        rnd_loss = (predictions - targets).pow(2).mean()\n",
        "\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"经验回放训练\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        # 更新启发式网络\n",
        "        self.train_step_counter += 1\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "        # 更新 RND 网络\n",
        "        self._update_rnd_network(states)\n",
        "\n",
        "\n",
        "def train_agent():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    agent = BFS(num_simulations=100)\n",
        "\n",
        "    for episode in range(3):\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.bfs_search(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "        print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R6Lx9D-0_Fi",
        "outputId": "aaadd7df-e6bc-452c-df95-d205d04660b0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 1.0\n",
            "Episode 1, Total Reward: 2.0\n",
            "Episode 1, Total Reward: 3.0\n",
            "Episode 1, Total Reward: 4.0\n",
            "Episode 1, Total Reward: 5.0\n",
            "Episode 1, Total Reward: 6.0\n",
            "Episode 1, Total Reward: 7.0\n",
            "Episode 1, Total Reward: 8.0\n",
            "Episode 1, Total Reward: 9.0\n",
            "Episode 1, Total Reward: 10.0\n",
            "Episode 1, Total Reward: 11.0\n",
            "Episode 1, Total Reward: 12.0\n",
            "Episode 1, Total Reward: 13.0\n",
            "Episode 1, Total Reward: 14.0\n",
            "Episode 1, Total Reward: 15.0\n",
            "Episode 1, Total Reward: 16.0\n",
            "Episode 1, Total Reward: 17.0\n",
            "Episode 1, Total Reward: 18.0\n",
            "Episode 1, Total Reward: 19.0\n",
            "Episode 1, Total Reward: 20.0\n",
            "Episode 1, Total Reward: 21.0\n",
            "Episode 1, Total Reward: 22.0\n",
            "Episode 1, Total Reward: 23.0\n",
            "Episode 1, Total Reward: 24.0\n",
            "Episode 1, Total Reward: 25.0\n",
            "Episode 1, Total Reward: 26.0\n",
            "Episode 1, Total Reward: 27.0\n",
            "Episode 1, Total Reward: 28.0\n",
            "Episode 1, Total Reward: 29.0\n",
            "Episode 1, Total Reward: 30.0\n",
            "Episode 1, Total Reward: 31.0\n",
            "Episode 1, Total Reward: 32.0\n",
            "Episode 1, Total Reward: 33.0\n",
            "Episode 1, Total Reward: 34.0\n",
            "Episode 1, Total Reward: 35.0\n",
            "Episode 1, Total Reward: 36.0\n",
            "Episode 1, Total Reward: 37.0\n",
            "Episode 1, Total Reward: 38.0\n",
            "Episode 1, Total Reward: 39.0\n",
            "Episode 1, Total Reward: 40.0\n",
            "Episode 1, Total Reward: 41.0\n",
            "Episode 1, Total Reward: 42.0\n",
            "Episode 1, Total Reward: 43.0\n",
            "Episode 1, Total Reward: 44.0\n",
            "Episode 1, Total Reward: 45.0\n",
            "Episode 1, Total Reward: 46.0\n",
            "Episode 1, Total Reward: 47.0\n",
            "Episode 1, Total Reward: 48.0\n",
            "Episode 1, Total Reward: 49.0\n",
            "Episode 1, Total Reward: 50.0\n",
            "Episode 1, Total Reward: 51.0\n",
            "Episode 1, Total Reward: 52.0\n",
            "Episode 1, Total Reward: 53.0\n",
            "Episode 1, Total Reward: 54.0\n",
            "Episode 1, Total Reward: 55.0\n",
            "Episode 1, Total Reward: 56.0\n",
            "Episode 1, Total Reward: 57.0\n",
            "Episode 1, Total Reward: 58.0\n",
            "Episode 1, Total Reward: 59.0\n",
            "Episode 1, Total Reward: 60.0\n",
            "Episode 1, Total Reward: 61.0\n",
            "Episode 1, Total Reward: 62.0\n",
            "Episode 1, Total Reward: 63.0\n",
            "Episode 1, Total Reward: 64.0\n",
            "Episode 1, Total Reward: 65.0\n",
            "Episode 1, Total Reward: 66.0\n",
            "Episode 1, Total Reward: 67.0\n",
            "Episode 1, Total Reward: 68.0\n",
            "Episode 1, Total Reward: 69.0\n",
            "Episode 1, Total Reward: 70.0\n",
            "Episode 1, Total Reward: 71.0\n",
            "Episode 1, Total Reward: 72.0\n",
            "Episode 1, Total Reward: 73.0\n",
            "Episode 1, Total Reward: 74.0\n",
            "Episode 1, Total Reward: 75.0\n",
            "Episode 1, Total Reward: 76.0\n",
            "Episode 1, Total Reward: 77.0\n",
            "Episode 1, Total Reward: 78.0\n",
            "Episode 1, Total Reward: 79.0\n",
            "Episode 1, Total Reward: 80.0\n",
            "Episode 1, Total Reward: 81.0\n",
            "Episode 1, Total Reward: 82.0\n",
            "Episode 1, Total Reward: 83.0\n",
            "Episode 1, Total Reward: 84.0\n",
            "Episode 1, Total Reward: 85.0\n",
            "Episode 1, Total Reward: 86.0\n",
            "Episode 1, Total Reward: 87.0\n",
            "Episode 1, Total Reward: 88.0\n",
            "Episode 1, Total Reward: 89.0\n",
            "Episode 1, Total Reward: 90.0\n",
            "Episode 1, Total Reward: 91.0\n",
            "Episode 1, Total Reward: 92.0\n",
            "Episode 1, Total Reward: 93.0\n",
            "Episode 1, Total Reward: 94.0\n",
            "Episode 1, Total Reward: 95.0\n",
            "Episode 1, Total Reward: 96.0\n",
            "Episode 1, Total Reward: 97.0\n",
            "Episode 1, Total Reward: 98.0\n",
            "Episode 1, Total Reward: 99.0\n",
            "Episode 1, Total Reward: 100.0\n",
            "Episode 1, Total Reward: 101.0\n",
            "Episode 1, Total Reward: 102.0\n",
            "Episode 1, Total Reward: 103.0\n",
            "Episode 1, Total Reward: 104.0\n",
            "Episode 1, Total Reward: 105.0\n",
            "Episode 1, Total Reward: 106.0\n",
            "Episode 1, Total Reward: 107.0\n",
            "Episode 1, Total Reward: 108.0\n",
            "Episode 1, Total Reward: 109.0\n",
            "Episode 1, Total Reward: 110.0\n",
            "Episode 1, Total Reward: 111.0\n",
            "Episode 1, Total Reward: 112.0\n",
            "Episode 1, Total Reward: 113.0\n",
            "Episode 1, Total Reward: 114.0\n",
            "Episode 1, Total Reward: 115.0\n",
            "Episode 1, Total Reward: 116.0\n",
            "Episode 1, Total Reward: 117.0\n",
            "Episode 1, Total Reward: 118.0\n",
            "Episode 1, Total Reward: 119.0\n",
            "Episode 1, Total Reward: 120.0\n",
            "Episode 1, Total Reward: 121.0\n",
            "Episode 1, Total Reward: 122.0\n",
            "Episode 1, Total Reward: 123.0\n",
            "Episode 1, Total Reward: 124.0\n",
            "Episode 1, Total Reward: 125.0\n",
            "Episode 1, Total Reward: 126.0\n",
            "Episode 1, Total Reward: 127.0\n",
            "Episode 1, Total Reward: 128.0\n",
            "Episode 1, Total Reward: 129.0\n",
            "Episode 1, Total Reward: 130.0\n",
            "Episode 1, Total Reward: 131.0\n",
            "Episode 1, Total Reward: 132.0\n",
            "Episode 1, Total Reward: 133.0\n",
            "Episode 1, Total Reward: 134.0\n",
            "Episode 1, Total Reward: 135.0\n",
            "Episode 1, Total Reward: 136.0\n",
            "Episode 1, Total Reward: 137.0\n",
            "Episode 1, Total Reward: 138.0\n",
            "Episode 1, Total Reward: 139.0\n",
            "Episode 1, Total Reward: 140.0\n",
            "Episode 1, Total Reward: 141.0\n",
            "Episode 1, Total Reward: 142.0\n",
            "Episode 1, Total Reward: 143.0\n",
            "Episode 1, Total Reward: 144.0\n",
            "Episode 1, Total Reward: 145.0\n",
            "Episode 1, Total Reward: 146.0\n",
            "Episode 1, Total Reward: 147.0\n",
            "Episode 1, Total Reward: 148.0\n",
            "Episode 1, Total Reward: 149.0\n",
            "Episode 1, Total Reward: 150.0\n",
            "Episode 1, Total Reward: 151.0\n",
            "Episode 1, Total Reward: 152.0\n",
            "Episode 1, Total Reward: 153.0\n",
            "Episode 1, Total Reward: 154.0\n",
            "Episode 1, Total Reward: 155.0\n",
            "Episode 1, Total Reward: 156.0\n",
            "Episode 1, Total Reward: 157.0\n",
            "Episode 1, Total Reward: 158.0\n",
            "Episode 1, Total Reward: 159.0\n",
            "Episode 1, Total Reward: 160.0\n",
            "Episode 1, Total Reward: 161.0\n",
            "Episode 1, Total Reward: 162.0\n",
            "Episode 1, Total Reward: 163.0\n",
            "Episode 1, Total Reward: 164.0\n",
            "Episode 1, Total Reward: 165.0\n",
            "Episode 1, Total Reward: 166.0\n",
            "Episode 1, Total Reward: 167.0\n",
            "Episode 1, Total Reward: 168.0\n",
            "Episode 1, Total Reward: 169.0\n",
            "Episode 1, Total Reward: 170.0\n",
            "Episode 1, Total Reward: 171.0\n",
            "Episode 1, Total Reward: 172.0\n",
            "Episode 1, Total Reward: 173.0\n",
            "Episode 1, Total Reward: 174.0\n",
            "Episode 1, Total Reward: 175.0\n",
            "Episode 1, Total Reward: 176.0\n",
            "Episode 1, Total Reward: 177.0\n",
            "Episode 1, Total Reward: 178.0\n",
            "Episode 1, Total Reward: 179.0\n",
            "Episode 1, Total Reward: 180.0\n",
            "Episode 1, Total Reward: 181.0\n",
            "Episode 1, Total Reward: 182.0\n",
            "Episode 1, Total Reward: 183.0\n",
            "Episode 1, Total Reward: 184.0\n",
            "Episode 1, Total Reward: 185.0\n",
            "Episode 1, Total Reward: 186.0\n",
            "Episode 1, Total Reward: 187.0\n",
            "Episode 1, Total Reward: 188.0\n",
            "Episode 1, Total Reward: 189.0\n",
            "Episode 1, Total Reward: 190.0\n",
            "Episode 1, Total Reward: 191.0\n",
            "Episode 1, Total Reward: 192.0\n",
            "Episode 1, Total Reward: 193.0\n",
            "Episode 1, Total Reward: 194.0\n",
            "Episode 1, Total Reward: 195.0\n",
            "Episode 1, Total Reward: 196.0\n",
            "Episode 1, Total Reward: 197.0\n",
            "Episode 1, Total Reward: 198.0\n",
            "Episode 1, Total Reward: 199.0\n",
            "Episode 1, Total Reward: 200.0\n",
            "Episode 1, Total Reward: 201.0\n",
            "Episode 1, Total Reward: 202.0\n",
            "Episode 1, Total Reward: 202.0\n",
            "Episode 2, Total Reward: 1.0\n",
            "Episode 2, Total Reward: 2.0\n",
            "Episode 2, Total Reward: 3.0\n",
            "Episode 2, Total Reward: 4.0\n",
            "Episode 2, Total Reward: 5.0\n",
            "Episode 2, Total Reward: 6.0\n",
            "Episode 2, Total Reward: 7.0\n",
            "Episode 2, Total Reward: 8.0\n",
            "Episode 2, Total Reward: 9.0\n",
            "Episode 2, Total Reward: 10.0\n",
            "Episode 2, Total Reward: 11.0\n",
            "Episode 2, Total Reward: 12.0\n",
            "Episode 2, Total Reward: 13.0\n",
            "Episode 2, Total Reward: 14.0\n",
            "Episode 2, Total Reward: 15.0\n",
            "Episode 2, Total Reward: 16.0\n",
            "Episode 2, Total Reward: 17.0\n",
            "Episode 2, Total Reward: 18.0\n",
            "Episode 2, Total Reward: 19.0\n",
            "Episode 2, Total Reward: 20.0\n",
            "Episode 2, Total Reward: 21.0\n",
            "Episode 2, Total Reward: 22.0\n",
            "Episode 2, Total Reward: 23.0\n",
            "Episode 2, Total Reward: 24.0\n",
            "Episode 2, Total Reward: 25.0\n",
            "Episode 2, Total Reward: 26.0\n",
            "Episode 2, Total Reward: 27.0\n",
            "Episode 2, Total Reward: 28.0\n",
            "Episode 2, Total Reward: 29.0\n",
            "Episode 2, Total Reward: 30.0\n",
            "Episode 2, Total Reward: 31.0\n",
            "Episode 2, Total Reward: 32.0\n",
            "Episode 2, Total Reward: 33.0\n",
            "Episode 2, Total Reward: 34.0\n",
            "Episode 2, Total Reward: 35.0\n",
            "Episode 2, Total Reward: 36.0\n",
            "Episode 2, Total Reward: 37.0\n",
            "Episode 2, Total Reward: 38.0\n",
            "Episode 2, Total Reward: 39.0\n",
            "Episode 2, Total Reward: 40.0\n",
            "Episode 2, Total Reward: 41.0\n",
            "Episode 2, Total Reward: 42.0\n",
            "Episode 2, Total Reward: 43.0\n",
            "Episode 2, Total Reward: 44.0\n",
            "Episode 2, Total Reward: 45.0\n",
            "Episode 2, Total Reward: 46.0\n",
            "Episode 2, Total Reward: 47.0\n",
            "Episode 2, Total Reward: 48.0\n",
            "Episode 2, Total Reward: 49.0\n",
            "Episode 2, Total Reward: 50.0\n",
            "Episode 2, Total Reward: 51.0\n",
            "Episode 2, Total Reward: 52.0\n",
            "Episode 2, Total Reward: 53.0\n",
            "Episode 2, Total Reward: 54.0\n",
            "Episode 2, Total Reward: 55.0\n",
            "Episode 2, Total Reward: 56.0\n",
            "Episode 2, Total Reward: 57.0\n",
            "Episode 2, Total Reward: 58.0\n",
            "Episode 2, Total Reward: 59.0\n",
            "Episode 2, Total Reward: 60.0\n",
            "Episode 2, Total Reward: 61.0\n",
            "Episode 2, Total Reward: 62.0\n",
            "Episode 2, Total Reward: 63.0\n",
            "Episode 2, Total Reward: 64.0\n",
            "Episode 2, Total Reward: 65.0\n",
            "Episode 2, Total Reward: 66.0\n",
            "Episode 2, Total Reward: 67.0\n",
            "Episode 2, Total Reward: 68.0\n",
            "Episode 2, Total Reward: 69.0\n",
            "Episode 2, Total Reward: 70.0\n",
            "Episode 2, Total Reward: 71.0\n",
            "Episode 2, Total Reward: 72.0\n",
            "Episode 2, Total Reward: 73.0\n",
            "Episode 2, Total Reward: 74.0\n",
            "Episode 2, Total Reward: 75.0\n",
            "Episode 2, Total Reward: 76.0\n",
            "Episode 2, Total Reward: 77.0\n",
            "Episode 2, Total Reward: 78.0\n",
            "Episode 2, Total Reward: 79.0\n",
            "Episode 2, Total Reward: 80.0\n",
            "Episode 2, Total Reward: 81.0\n",
            "Episode 2, Total Reward: 82.0\n",
            "Episode 2, Total Reward: 83.0\n",
            "Episode 2, Total Reward: 84.0\n",
            "Episode 2, Total Reward: 85.0\n",
            "Episode 2, Total Reward: 86.0\n",
            "Episode 2, Total Reward: 87.0\n",
            "Episode 2, Total Reward: 88.0\n",
            "Episode 2, Total Reward: 89.0\n",
            "Episode 2, Total Reward: 90.0\n",
            "Episode 2, Total Reward: 91.0\n",
            "Episode 2, Total Reward: 92.0\n",
            "Episode 2, Total Reward: 93.0\n",
            "Episode 2, Total Reward: 94.0\n",
            "Episode 2, Total Reward: 95.0\n",
            "Episode 2, Total Reward: 96.0\n",
            "Episode 2, Total Reward: 97.0\n",
            "Episode 2, Total Reward: 98.0\n",
            "Episode 2, Total Reward: 99.0\n",
            "Episode 2, Total Reward: 100.0\n",
            "Episode 2, Total Reward: 101.0\n",
            "Episode 2, Total Reward: 102.0\n",
            "Episode 2, Total Reward: 103.0\n",
            "Episode 2, Total Reward: 104.0\n",
            "Episode 2, Total Reward: 105.0\n",
            "Episode 2, Total Reward: 106.0\n",
            "Episode 2, Total Reward: 107.0\n",
            "Episode 2, Total Reward: 108.0\n",
            "Episode 2, Total Reward: 109.0\n",
            "Episode 2, Total Reward: 110.0\n",
            "Episode 2, Total Reward: 111.0\n",
            "Episode 2, Total Reward: 112.0\n",
            "Episode 2, Total Reward: 113.0\n",
            "Episode 2, Total Reward: 114.0\n",
            "Episode 2, Total Reward: 115.0\n",
            "Episode 2, Total Reward: 116.0\n",
            "Episode 2, Total Reward: 117.0\n",
            "Episode 2, Total Reward: 118.0\n",
            "Episode 2, Total Reward: 119.0\n",
            "Episode 2, Total Reward: 120.0\n",
            "Episode 2, Total Reward: 121.0\n",
            "Episode 2, Total Reward: 122.0\n",
            "Episode 2, Total Reward: 123.0\n",
            "Episode 2, Total Reward: 124.0\n",
            "Episode 2, Total Reward: 125.0\n",
            "Episode 2, Total Reward: 126.0\n",
            "Episode 2, Total Reward: 127.0\n",
            "Episode 2, Total Reward: 128.0\n",
            "Episode 2, Total Reward: 129.0\n",
            "Episode 2, Total Reward: 130.0\n",
            "Episode 2, Total Reward: 131.0\n",
            "Episode 2, Total Reward: 132.0\n",
            "Episode 2, Total Reward: 133.0\n",
            "Episode 2, Total Reward: 134.0\n",
            "Episode 2, Total Reward: 135.0\n",
            "Episode 2, Total Reward: 136.0\n",
            "Episode 2, Total Reward: 137.0\n",
            "Episode 2, Total Reward: 138.0\n",
            "Episode 2, Total Reward: 139.0\n",
            "Episode 2, Total Reward: 140.0\n",
            "Episode 2, Total Reward: 141.0\n",
            "Episode 2, Total Reward: 142.0\n",
            "Episode 2, Total Reward: 143.0\n",
            "Episode 2, Total Reward: 144.0\n",
            "Episode 2, Total Reward: 145.0\n",
            "Episode 2, Total Reward: 146.0\n",
            "Episode 2, Total Reward: 147.0\n",
            "Episode 2, Total Reward: 148.0\n",
            "Episode 2, Total Reward: 149.0\n",
            "Episode 2, Total Reward: 150.0\n",
            "Episode 2, Total Reward: 151.0\n",
            "Episode 2, Total Reward: 152.0\n",
            "Episode 2, Total Reward: 153.0\n",
            "Episode 2, Total Reward: 154.0\n",
            "Episode 2, Total Reward: 155.0\n",
            "Episode 2, Total Reward: 156.0\n",
            "Episode 2, Total Reward: 157.0\n",
            "Episode 2, Total Reward: 158.0\n",
            "Episode 2, Total Reward: 159.0\n",
            "Episode 2, Total Reward: 160.0\n",
            "Episode 2, Total Reward: 161.0\n",
            "Episode 2, Total Reward: 162.0\n",
            "Episode 2, Total Reward: 163.0\n",
            "Episode 2, Total Reward: 164.0\n",
            "Episode 2, Total Reward: 165.0\n",
            "Episode 2, Total Reward: 166.0\n",
            "Episode 2, Total Reward: 167.0\n",
            "Episode 2, Total Reward: 168.0\n",
            "Episode 2, Total Reward: 169.0\n",
            "Episode 2, Total Reward: 170.0\n",
            "Episode 2, Total Reward: 171.0\n",
            "Episode 2, Total Reward: 172.0\n",
            "Episode 2, Total Reward: 173.0\n",
            "Episode 2, Total Reward: 174.0\n",
            "Episode 2, Total Reward: 175.0\n",
            "Episode 2, Total Reward: 176.0\n",
            "Episode 2, Total Reward: 177.0\n",
            "Episode 2, Total Reward: 178.0\n",
            "Episode 2, Total Reward: 179.0\n",
            "Episode 2, Total Reward: 180.0\n",
            "Episode 2, Total Reward: 181.0\n",
            "Episode 2, Total Reward: 182.0\n",
            "Episode 2, Total Reward: 183.0\n",
            "Episode 2, Total Reward: 184.0\n",
            "Episode 2, Total Reward: 185.0\n",
            "Episode 2, Total Reward: 186.0\n",
            "Episode 2, Total Reward: 187.0\n",
            "Episode 2, Total Reward: 188.0\n",
            "Episode 2, Total Reward: 189.0\n",
            "Episode 2, Total Reward: 190.0\n",
            "Episode 2, Total Reward: 191.0\n",
            "Episode 2, Total Reward: 192.0\n",
            "Episode 2, Total Reward: 193.0\n",
            "Episode 2, Total Reward: 194.0\n",
            "Episode 2, Total Reward: 195.0\n",
            "Episode 2, Total Reward: 196.0\n",
            "Episode 2, Total Reward: 197.0\n",
            "Episode 2, Total Reward: 198.0\n",
            "Episode 2, Total Reward: 199.0\n",
            "Episode 2, Total Reward: 200.0\n",
            "Episode 2, Total Reward: 201.0\n",
            "Episode 2, Total Reward: 202.0\n",
            "Episode 2, Total Reward: 203.0\n",
            "Episode 2, Total Reward: 204.0\n",
            "Episode 2, Total Reward: 205.0\n",
            "Episode 2, Total Reward: 206.0\n",
            "Episode 2, Total Reward: 207.0\n",
            "Episode 2, Total Reward: 208.0\n",
            "Episode 2, Total Reward: 209.0\n",
            "Episode 2, Total Reward: 210.0\n",
            "Episode 2, Total Reward: 211.0\n",
            "Episode 2, Total Reward: 212.0\n",
            "Episode 2, Total Reward: 213.0\n",
            "Episode 2, Total Reward: 214.0\n",
            "Episode 2, Total Reward: 215.0\n",
            "Episode 2, Total Reward: 216.0\n",
            "Episode 2, Total Reward: 217.0\n",
            "Episode 2, Total Reward: 218.0\n",
            "Episode 2, Total Reward: 219.0\n",
            "Episode 2, Total Reward: 220.0\n",
            "Episode 2, Total Reward: 221.0\n",
            "Episode 2, Total Reward: 222.0\n",
            "Episode 2, Total Reward: 223.0\n",
            "Episode 2, Total Reward: 224.0\n",
            "Episode 2, Total Reward: 225.0\n",
            "Episode 2, Total Reward: 226.0\n",
            "Episode 2, Total Reward: 227.0\n",
            "Episode 2, Total Reward: 228.0\n",
            "Episode 2, Total Reward: 229.0\n",
            "Episode 2, Total Reward: 230.0\n",
            "Episode 2, Total Reward: 231.0\n",
            "Episode 2, Total Reward: 232.0\n",
            "Episode 2, Total Reward: 233.0\n",
            "Episode 2, Total Reward: 234.0\n",
            "Episode 2, Total Reward: 235.0\n",
            "Episode 2, Total Reward: 236.0\n",
            "Episode 2, Total Reward: 237.0\n",
            "Episode 2, Total Reward: 238.0\n",
            "Episode 2, Total Reward: 239.0\n",
            "Episode 2, Total Reward: 240.0\n",
            "Episode 2, Total Reward: 241.0\n",
            "Episode 2, Total Reward: 241.0\n",
            "Episode 3, Total Reward: 1.0\n",
            "Episode 3, Total Reward: 2.0\n",
            "Episode 3, Total Reward: 3.0\n",
            "Episode 3, Total Reward: 4.0\n",
            "Episode 3, Total Reward: 5.0\n",
            "Episode 3, Total Reward: 6.0\n",
            "Episode 3, Total Reward: 7.0\n",
            "Episode 3, Total Reward: 8.0\n",
            "Episode 3, Total Reward: 9.0\n",
            "Episode 3, Total Reward: 10.0\n",
            "Episode 3, Total Reward: 11.0\n",
            "Episode 3, Total Reward: 12.0\n",
            "Episode 3, Total Reward: 13.0\n",
            "Episode 3, Total Reward: 14.0\n",
            "Episode 3, Total Reward: 15.0\n",
            "Episode 3, Total Reward: 16.0\n",
            "Episode 3, Total Reward: 17.0\n",
            "Episode 3, Total Reward: 18.0\n",
            "Episode 3, Total Reward: 19.0\n",
            "Episode 3, Total Reward: 20.0\n",
            "Episode 3, Total Reward: 21.0\n",
            "Episode 3, Total Reward: 22.0\n",
            "Episode 3, Total Reward: 23.0\n",
            "Episode 3, Total Reward: 24.0\n",
            "Episode 3, Total Reward: 25.0\n",
            "Episode 3, Total Reward: 26.0\n",
            "Episode 3, Total Reward: 27.0\n",
            "Episode 3, Total Reward: 28.0\n",
            "Episode 3, Total Reward: 29.0\n",
            "Episode 3, Total Reward: 30.0\n",
            "Episode 3, Total Reward: 31.0\n",
            "Episode 3, Total Reward: 32.0\n",
            "Episode 3, Total Reward: 33.0\n",
            "Episode 3, Total Reward: 34.0\n",
            "Episode 3, Total Reward: 35.0\n",
            "Episode 3, Total Reward: 36.0\n",
            "Episode 3, Total Reward: 37.0\n",
            "Episode 3, Total Reward: 38.0\n",
            "Episode 3, Total Reward: 39.0\n",
            "Episode 3, Total Reward: 40.0\n",
            "Episode 3, Total Reward: 41.0\n",
            "Episode 3, Total Reward: 42.0\n",
            "Episode 3, Total Reward: 43.0\n",
            "Episode 3, Total Reward: 44.0\n",
            "Episode 3, Total Reward: 45.0\n",
            "Episode 3, Total Reward: 46.0\n",
            "Episode 3, Total Reward: 47.0\n",
            "Episode 3, Total Reward: 48.0\n",
            "Episode 3, Total Reward: 49.0\n",
            "Episode 3, Total Reward: 50.0\n",
            "Episode 3, Total Reward: 51.0\n",
            "Episode 3, Total Reward: 52.0\n",
            "Episode 3, Total Reward: 53.0\n",
            "Episode 3, Total Reward: 54.0\n",
            "Episode 3, Total Reward: 55.0\n",
            "Episode 3, Total Reward: 56.0\n",
            "Episode 3, Total Reward: 57.0\n",
            "Episode 3, Total Reward: 58.0\n",
            "Episode 3, Total Reward: 59.0\n",
            "Episode 3, Total Reward: 60.0\n",
            "Episode 3, Total Reward: 61.0\n",
            "Episode 3, Total Reward: 62.0\n",
            "Episode 3, Total Reward: 63.0\n",
            "Episode 3, Total Reward: 64.0\n",
            "Episode 3, Total Reward: 65.0\n",
            "Episode 3, Total Reward: 66.0\n",
            "Episode 3, Total Reward: 67.0\n",
            "Episode 3, Total Reward: 68.0\n",
            "Episode 3, Total Reward: 69.0\n",
            "Episode 3, Total Reward: 70.0\n",
            "Episode 3, Total Reward: 71.0\n",
            "Episode 3, Total Reward: 72.0\n",
            "Episode 3, Total Reward: 73.0\n",
            "Episode 3, Total Reward: 74.0\n",
            "Episode 3, Total Reward: 75.0\n",
            "Episode 3, Total Reward: 76.0\n",
            "Episode 3, Total Reward: 77.0\n",
            "Episode 3, Total Reward: 78.0\n",
            "Episode 3, Total Reward: 79.0\n",
            "Episode 3, Total Reward: 80.0\n",
            "Episode 3, Total Reward: 81.0\n",
            "Episode 3, Total Reward: 82.0\n",
            "Episode 3, Total Reward: 83.0\n",
            "Episode 3, Total Reward: 84.0\n",
            "Episode 3, Total Reward: 85.0\n",
            "Episode 3, Total Reward: 86.0\n",
            "Episode 3, Total Reward: 87.0\n",
            "Episode 3, Total Reward: 88.0\n",
            "Episode 3, Total Reward: 89.0\n",
            "Episode 3, Total Reward: 90.0\n",
            "Episode 3, Total Reward: 91.0\n",
            "Episode 3, Total Reward: 92.0\n",
            "Episode 3, Total Reward: 93.0\n",
            "Episode 3, Total Reward: 94.0\n",
            "Episode 3, Total Reward: 95.0\n",
            "Episode 3, Total Reward: 96.0\n",
            "Episode 3, Total Reward: 97.0\n",
            "Episode 3, Total Reward: 98.0\n",
            "Episode 3, Total Reward: 99.0\n",
            "Episode 3, Total Reward: 100.0\n",
            "Episode 3, Total Reward: 101.0\n",
            "Episode 3, Total Reward: 102.0\n",
            "Episode 3, Total Reward: 103.0\n",
            "Episode 3, Total Reward: 104.0\n",
            "Episode 3, Total Reward: 105.0\n",
            "Episode 3, Total Reward: 106.0\n",
            "Episode 3, Total Reward: 107.0\n",
            "Episode 3, Total Reward: 108.0\n",
            "Episode 3, Total Reward: 109.0\n",
            "Episode 3, Total Reward: 110.0\n",
            "Episode 3, Total Reward: 111.0\n",
            "Episode 3, Total Reward: 112.0\n",
            "Episode 3, Total Reward: 113.0\n",
            "Episode 3, Total Reward: 114.0\n",
            "Episode 3, Total Reward: 115.0\n",
            "Episode 3, Total Reward: 116.0\n",
            "Episode 3, Total Reward: 117.0\n",
            "Episode 3, Total Reward: 118.0\n",
            "Episode 3, Total Reward: 119.0\n",
            "Episode 3, Total Reward: 120.0\n",
            "Episode 3, Total Reward: 121.0\n",
            "Episode 3, Total Reward: 122.0\n",
            "Episode 3, Total Reward: 123.0\n",
            "Episode 3, Total Reward: 124.0\n",
            "Episode 3, Total Reward: 125.0\n",
            "Episode 3, Total Reward: 126.0\n",
            "Episode 3, Total Reward: 127.0\n",
            "Episode 3, Total Reward: 128.0\n",
            "Episode 3, Total Reward: 129.0\n",
            "Episode 3, Total Reward: 130.0\n",
            "Episode 3, Total Reward: 131.0\n",
            "Episode 3, Total Reward: 132.0\n",
            "Episode 3, Total Reward: 133.0\n",
            "Episode 3, Total Reward: 134.0\n",
            "Episode 3, Total Reward: 135.0\n",
            "Episode 3, Total Reward: 136.0\n",
            "Episode 3, Total Reward: 137.0\n",
            "Episode 3, Total Reward: 138.0\n",
            "Episode 3, Total Reward: 139.0\n",
            "Episode 3, Total Reward: 140.0\n",
            "Episode 3, Total Reward: 141.0\n",
            "Episode 3, Total Reward: 142.0\n",
            "Episode 3, Total Reward: 143.0\n",
            "Episode 3, Total Reward: 144.0\n",
            "Episode 3, Total Reward: 145.0\n",
            "Episode 3, Total Reward: 146.0\n",
            "Episode 3, Total Reward: 147.0\n",
            "Episode 3, Total Reward: 148.0\n",
            "Episode 3, Total Reward: 149.0\n",
            "Episode 3, Total Reward: 150.0\n",
            "Episode 3, Total Reward: 151.0\n",
            "Episode 3, Total Reward: 152.0\n",
            "Episode 3, Total Reward: 153.0\n",
            "Episode 3, Total Reward: 154.0\n",
            "Episode 3, Total Reward: 155.0\n",
            "Episode 3, Total Reward: 156.0\n",
            "Episode 3, Total Reward: 157.0\n",
            "Episode 3, Total Reward: 158.0\n",
            "Episode 3, Total Reward: 159.0\n",
            "Episode 3, Total Reward: 160.0\n",
            "Episode 3, Total Reward: 161.0\n",
            "Episode 3, Total Reward: 162.0\n",
            "Episode 3, Total Reward: 163.0\n",
            "Episode 3, Total Reward: 164.0\n",
            "Episode 3, Total Reward: 165.0\n",
            "Episode 3, Total Reward: 165.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCwHxtahS3Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Roll-**out**"
      ],
      "metadata": {
        "id": "W41jpapyWRCD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDE32tfcWXiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Frozen-lake"
      ],
      "metadata": {
        "id": "zhItRVXLWnwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "# 神经网络模型定义\n",
        "class HeuristicNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()  # 输出0-1之间的启发值\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "# RND 网络定义\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, env_size, output_size=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class NeuralEnhancedBFSwithRND:\n",
        "    def __init__(self, env_size: int = 8, num_simulations: int = 100,\n",
        "                 buffer_size: int = 10000, batch_size: int = 32, rnd_scale: float = 0.40):\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size**2 - 1\n",
        "\n",
        "        # 启发式网络配置\n",
        "        self.model = HeuristicNetwork(env_size)\n",
        "        self.target_model = HeuristicNetwork(env_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # RND 网络配置\n",
        "        self.random_net = RNDNetwork(env_size).eval()  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork(env_size)      # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=0.001)\n",
        "        self.rnd_scale = rnd_scale  # RND 内部奖励缩放因子\n",
        "\n",
        "        # 经验回放缓存\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络同步间隔\n",
        "        self.target_update_interval = 15\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "        self.rnd_scale = rnd_scale\n",
        "        self.rnd_mean = 0.0  # 移动平均\n",
        "        self.rnd_std = 1.0   # 移动标准差\n",
        "        self.rnd_count = 0   # 统计次数\n",
        "        self.rnd_m2 = 0.0    # 用于 Welford 法计算方差\n",
        "\n",
        "        self.rollout_cache = {}\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    def _get_valid_actions(self, state: int, env_size: int) -> List[int]:\n",
        "        row, col = state // env_size, state % env_size\n",
        "        valid_actions = []\n",
        "        if col > 0: valid_actions.append(0)    # 左\n",
        "        if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "        if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "        if row > 0: valid_actions.append(3)    # 上\n",
        "        return valid_actions\n",
        "\n",
        "    def _get_action_path(self, node: Node) -> List[int]:\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.model(state_tensor).item()\n",
        "\n",
        "    def _calculate_rnd_reward(self, state: int) -> float:\n",
        "        \"\"\"计算 RND 内部奖励\"\"\"\n",
        "        state_tensor = torch.LongTensor([state])\n",
        "        with torch.no_grad():\n",
        "            target = self.random_net(state_tensor)\n",
        "            prediction = self.predictor_net(state_tensor)\n",
        "        raw_rnd = (target - prediction).pow(2).mean().item()\n",
        "\n",
        "        '''# Welford 在线算法更新均值和标准差\n",
        "        self.rnd_count += 1\n",
        "        delta = raw_rnd - self.rnd_mean\n",
        "        self.rnd_mean += delta / self.rnd_count\n",
        "        delta2 = raw_rnd - self.rnd_mean\n",
        "        self.rnd_m2 += delta * delta2\n",
        "        if self.rnd_count > 1:\n",
        "            self.rnd_std = np.sqrt(self.rnd_m2 / (self.rnd_count - 1))\n",
        "\n",
        "        # 归一化并缩放\n",
        "        normalized_rnd = (raw_rnd - self.rnd_mean) / (self.rnd_std + 1e-5)  # 避免除零\n",
        "        scaled_rnd = self.rnd_scale * normalized_rnd\n",
        "\n",
        "        # 裁剪到合理范围'''\n",
        "        return raw_rnd * 0.2 #根据论文建议，限制最大值\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        states = torch.LongTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def _update_rnd_network(self, states):\n",
        "        \"\"\"更新 RND 的预测网络\"\"\"\n",
        "        states = torch.LongTensor(states)\n",
        "        predictions = self.predictor_net(states)\n",
        "        with torch.no_grad():\n",
        "            targets = self.random_net(states)\n",
        "        rnd_loss = (predictions - targets).pow(2).mean()\n",
        "\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "    def _remember(self, state, target, next_state=None):\n",
        "        \"\"\"存储经验，包括 RND 的状态\"\"\"\n",
        "        self.replay_buffer.append((state, target, next_state if next_state is not None else state))\n",
        "\n",
        "    def _replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets, next_states = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        # 更新启发式网络\n",
        "        self.train_step_counter += 1\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "        # 更新 RND 网络\n",
        "        self._update_rnd_network(next_states)\n",
        "\n",
        "    def _get_bootstrap_target(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.target_model(state_tensor).item()\n",
        "\n",
        "    def _perform_rollout(self, state: int, num_rollouts: int = 3, rollout_depth: int = 3) -> float:\n",
        "      \"\"\"\n",
        "      Perform multiple rollouts from a given state to estimate its value.\n",
        "\n",
        "      Args:\n",
        "          state: The starting state\n",
        "          num_rollouts: Number of simulation sequences to run\n",
        "          rollout_depth: Steps to look ahead in each rollout\n",
        "\n",
        "      Returns:\n",
        "          Average value from the rollouts\n",
        "      \"\"\"\n",
        "      total_value = 0.0\n",
        "\n",
        "      for _ in range(num_rollouts):\n",
        "          current_state = state\n",
        "\n",
        "          # If already at goal, return maximum value\n",
        "          if current_state == self.goal_state:\n",
        "              return 1.0\n",
        "\n",
        "          for depth in range(rollout_depth):\n",
        "              # If reached goal during simulation, add high value\n",
        "              if current_state == self.goal_state:\n",
        "                  total_value += 1.0\n",
        "                  break\n",
        "\n",
        "              # Get valid actions\n",
        "              valid_actions = self._get_valid_actions(current_state, self.env_size)\n",
        "\n",
        "              # Choose action - 80% based on heuristic, 20% random exploration\n",
        "              if np.random.random() < 0.8:\n",
        "                  # Use heuristic to choose best action\n",
        "                  best_action = None\n",
        "                  best_value = float('-inf')\n",
        "                  for action in valid_actions:\n",
        "                      next_state = self._simulate_step(current_state, action)\n",
        "                      value = self._calculate_heuristic(next_state)\n",
        "                      if value > best_value:\n",
        "                          best_value = value\n",
        "                          best_action = action\n",
        "                  action = best_action\n",
        "              else:\n",
        "                  # Random exploration\n",
        "                  action = np.random.choice(valid_actions)\n",
        "\n",
        "              # Simulate the action\n",
        "              next_state = self._simulate_step(current_state, action)\n",
        "\n",
        "              # Check if terminal state reached\n",
        "              self.env.reset()\n",
        "              self.env.unwrapped.s = current_state\n",
        "              _, reward, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "              if terminated:\n",
        "                  if next_state == self.goal_state:\n",
        "                      total_value += 1.0  # Bonus for reaching goal\n",
        "                  else:\n",
        "                      total_value -= 0.2  # Penalty for holes\n",
        "                  break\n",
        "\n",
        "              # Move to next state\n",
        "              current_state = next_state\n",
        "\n",
        "          # If didn't terminate, add final state heuristic value\n",
        "          if current_state != self.goal_state:\n",
        "              total_value += 0.5 * self._calculate_heuristic(current_state)\n",
        "\n",
        "      # Return average value across all rollouts\n",
        "      return total_value / num_rollouts\n",
        "\n",
        "    def _evaluate_state_with_rollout(self, state: int) -> float:\n",
        "      \"\"\"\n",
        "      Evaluate a state using both neural network heuristic and rollouts\n",
        "\n",
        "      Args:\n",
        "          state: The state to evaluate\n",
        "\n",
        "      Returns:\n",
        "          Combined value estimate\n",
        "      \"\"\"\n",
        "      # Neural network heuristic\n",
        "      heuristic = self._calculate_heuristic(state)\n",
        "\n",
        "      # Rollout value\n",
        "      rollout = self._perform_rollout(state, num_rollouts=3, rollout_depth=3)\n",
        "\n",
        "      # RND exploration bonus\n",
        "      rnd_bonus = self._calculate_rnd_reward(state)\n",
        "\n",
        "      # Combine values (weights can be adjusted)\n",
        "      combined_value = 0.6 * heuristic + 0.3 * rollout + 0.3 * rnd_bonus\n",
        "\n",
        "      # Limit to range [0, 1]\n",
        "      return min(max(combined_value, 0.0), 1.0)\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "      # Add a cache for rollout results to avoid redundant calculations\n",
        "      self.rollout_cache = {}\n",
        "\n",
        "      visited = set()\n",
        "      queue = PriorityQueue()\n",
        "      root_node = Node(state=start_state)\n",
        "\n",
        "      # Evaluate root with combined method\n",
        "      root_value = self._evaluate_state_with_rollout(start_state)\n",
        "      root_node.value = root_value\n",
        "      queue.put((-root_value, id(root_node), root_node))\n",
        "\n",
        "      found_goal = False\n",
        "      goal_node = None\n",
        "\n",
        "      while not queue.empty():\n",
        "          _, _, current_node = queue.get()\n",
        "\n",
        "          if current_node.state in visited:\n",
        "              continue\n",
        "          visited.add(current_node.state)\n",
        "\n",
        "          if current_node.state == self.goal_state:\n",
        "              found_goal = True\n",
        "              goal_node = current_node\n",
        "              self._remember(current_node.state, 1.0, current_node.state)\n",
        "              break\n",
        "\n",
        "          # Collect training data, now with rollout-enhanced values\n",
        "          if current_node.parent is not None:\n",
        "            target = self._get_bootstrap_target(current_node.state)\n",
        "            rnd_reward = self._calculate_rnd_reward(current_node.state)\n",
        "            total_target = min(target + rnd_reward, 1.0)  # 限制最大值为1\n",
        "            self._remember(current_node.parent.state, total_target, current_node.state)\n",
        "\n",
        "          for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "              self.env.reset()\n",
        "              self.env.unwrapped.s = current_node.state\n",
        "              next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "              if terminated and next_state != self.goal_state:\n",
        "                  self._remember(next_state, 0.0, next_state)\n",
        "                  continue\n",
        "\n",
        "              if next_state not in visited:\n",
        "                  # Evaluate with rollout\n",
        "                  next_value = self._evaluate_state_with_rollout(next_state)\n",
        "\n",
        "                  next_node = Node(\n",
        "                      state=next_state,\n",
        "                      action_taken=action,\n",
        "                      parent=current_node,\n",
        "                      value=next_value\n",
        "                  )\n",
        "                  current_node.children[action] = next_node\n",
        "                  priority = -next_node.value\n",
        "                  queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "          self._replay()\n",
        "\n",
        "      if found_goal:\n",
        "          current = goal_node\n",
        "          while current.parent:\n",
        "              current.value += 1.0\n",
        "              current = current.parent\n",
        "\n",
        "      return None, 0, root_node\n",
        "    def bfs_search1(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                self._remember(current_node.state, 1.0, current_node.state)\n",
        "                break\n",
        "            # 收集训练数据，包括 RND 奖励\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                rnd_reward = self._calculate_rnd_reward(current_node.state)\n",
        "                total_target = min(target + rnd_reward, 1.0)  # 限制最大值为1\n",
        "                self._remember(current_node.parent.state, total_target, current_node.state)\n",
        "\n",
        "\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0, next_state)\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state) + self._calculate_rnd_reward(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "            self._replay()\n",
        "\n",
        "        if found_goal:\n",
        "            current = goal_node\n",
        "            while current.parent:\n",
        "                current.value += 1.0\n",
        "                current = current.parent\n",
        "\n",
        "            #return self._get_action_path(goal_node), 1, root_node\n",
        "        return None, 0, root_node\n",
        "\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        for action, child in root_node.children.items():\n",
        "            value = self._evaluate_subtree(child)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        return best_action #if best_action is not None else random.choice(self._get_valid_actions(root_node.state, self.env_size))\n",
        "\n",
        "    def _evaluate_subtree(self, node: Node) -> float:\n",
        "      if node.state == self.goal_state:\n",
        "          return float('inf')\n",
        "\n",
        "      # Use combined evaluation with rollout\n",
        "      state_value = self._evaluate_state_with_rollout(node.state)\n",
        "\n",
        "      children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "\n",
        "      return state_value + 0.5 * children_value\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        best_action = self.get_best_action_from_tree(root_node)\n",
        "        return best_action\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state\n"
      ],
      "metadata": {
        "id": "aKkggamaWsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def test_enhanced_bfs():\n",
        "    # 1. 创建简单的价值网络\n",
        "\n",
        "    # 2. 初始化环境和算法\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    #value_net = ValueNetwork(8)\n",
        "\n",
        "    bfs = NeuralEnhancedBFSwithRND()#EnhancedBFS(value_net, num_simulations=10)#Neural\n",
        "\n",
        "\n",
        "    # 3. 运行多个回合\n",
        "    num_episodes = 1\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\n开始测试NeuralEnhancedBFSwithRND...\")\n",
        "\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "\n",
        "        print(f\"\\n回合 {episode + 1}:\")\n",
        "        print(f\"起始状态: {state}\")\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # 使用算法选择动作\n",
        "            action = bfs.search(state)\n",
        "            print(f\"Steps {steps}: 在状态 {state} 选择动作 {action}\")\n",
        "\n",
        "            # 执行动作\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            print(f\"-> 新状态: {state}, 奖励: {reward}\")\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"成功到达目标！\")\n",
        "                else:\n",
        "                    print(\"失败（掉入陷阱或超时）\")\n",
        "\n",
        "        total_reward += episode_reward\n",
        "        print(f\"回合 {episode + 1} 结束 - 总步数: {steps}, 总奖励: {episode_reward}\")\n",
        "\n",
        "    print(f\"\\n测试完成 - 平均奖励: {total_reward/num_episodes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_bfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P12bxTDZlarf",
        "outputId": "574d9e22-0ba1-485b-92aa-af06b02e672a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "开始测试NeuralEnhancedBFSwithRND...\n",
            "\n",
            "回合 1:\n",
            "起始状态: 0\n",
            "Steps 0: 在状态 0 选择动作 1\n",
            "-> 新状态: 8, 奖励: 0.0\n",
            "Steps 1: 在状态 8 选择动作 2\n",
            "-> 新状态: 9, 奖励: 0.0\n",
            "Steps 2: 在状态 9 选择动作 2\n",
            "-> 新状态: 10, 奖励: 0.0\n",
            "Steps 3: 在状态 10 选择动作 2\n",
            "-> 新状态: 11, 奖励: 0.0\n",
            "Steps 4: 在状态 11 选择动作 2\n",
            "-> 新状态: 12, 奖励: 0.0\n",
            "Steps 5: 在状态 12 选择动作 2\n",
            "-> 新状态: 13, 奖励: 0.0\n",
            "Steps 6: 在状态 13 选择动作 2\n",
            "-> 新状态: 14, 奖励: 0.0\n",
            "Steps 7: 在状态 14 选择动作 2\n",
            "-> 新状态: 15, 奖励: 0.0\n",
            "Steps 8: 在状态 15 选择动作 1\n",
            "-> 新状态: 23, 奖励: 0.0\n",
            "Steps 9: 在状态 23 选择动作 1\n",
            "-> 新状态: 31, 奖励: 0.0\n",
            "Steps 10: 在状态 31 选择动作 1\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "Steps 11: 在状态 39 选择动作 0\n",
            "-> 新状态: 38, 奖励: 0.0\n",
            "Steps 12: 在状态 38 选择动作 2\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "Steps 13: 在状态 39 选择动作 3\n",
            "-> 新状态: 31, 奖励: 0.0\n",
            "Steps 14: 在状态 31 选择动作 1\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "Steps 15: 在状态 39 选择动作 0\n",
            "-> 新状态: 38, 奖励: 0.0\n",
            "Steps 16: 在状态 38 选择动作 0\n",
            "-> 新状态: 37, 奖励: 0.0\n",
            "Steps 17: 在状态 37 选择动作 1\n",
            "-> 新状态: 45, 奖励: 0.0\n",
            "Steps 18: 在状态 45 选择动作 3\n",
            "-> 新状态: 37, 奖励: 0.0\n",
            "Steps 19: 在状态 37 选择动作 2\n",
            "-> 新状态: 38, 奖励: 0.0\n",
            "Steps 20: 在状态 38 选择动作 0\n",
            "-> 新状态: 37, 奖励: 0.0\n",
            "Steps 21: 在状态 37 选择动作 1\n",
            "-> 新状态: 45, 奖励: 0.0\n",
            "Steps 22: 在状态 45 选择动作 1\n",
            "-> 新状态: 53, 奖励: 0.0\n",
            "Steps 23: 在状态 53 选择动作 1\n",
            "-> 新状态: 61, 奖励: 0.0\n",
            "Steps 24: 在状态 61 选择动作 2\n",
            "-> 新状态: 62, 奖励: 0.0\n",
            "Steps 25: 在状态 62 选择动作 2\n",
            "-> 新状态: 63, 奖励: 1.0\n",
            "成功到达目标！\n",
            "回合 1 结束 - 总步数: 26, 总奖励: 1.0\n",
            "\n",
            "测试完成 - 平均奖励: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 Cartpole"
      ],
      "metadata": {
        "id": "ui4udyyilcU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(4, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        # 初始化最后一层权重为接近0的小值\n",
        "        nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, state_size=4, output_size=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        return self.fc(x)\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: np.ndarray\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    value: float = 0.0\n",
        "    depth: int = 0\n",
        "    done: bool = False\n",
        "    reward: float = 0.0\n",
        "    visit_count: int = 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.parent is not None:\n",
        "            self.depth = self.parent.depth + 1\n",
        "\n",
        "class BFS:\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 40,\n",
        "                 buffer_size: int = 1000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 rnd_scale: float = 0.5):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.num_simulations = num_simulations\n",
        "        self.gamma = gamma\n",
        "        self.exploration_weight = 1.5\n",
        "        self.initial_epsilon = 0.2\n",
        "\n",
        "\n",
        "        # 启发式网络配置\n",
        "        self.model = ValueNetwork()\n",
        "        self.target_model = ValueNetwork()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # RND 网络配置\n",
        "        self.random_net = RNDNetwork().eval()  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork()      # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=0.001)\n",
        "        self.rnd_scale = rnd_scale  # RND 内部奖励缩放因子\n",
        "\n",
        "        # 经验回放\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络更新参数\n",
        "        self.train_step_counter = 0\n",
        "        self.target_update_interval = 15\n",
        "        self.tau = 0.005  # 软更新参数\n",
        "\n",
        "        self.rnd_scale = rnd_scale\n",
        "        self.rnd_mean = 0.0  # 移动平均\n",
        "        self.rnd_std = 1.0   # 移动标准差\n",
        "        self.rnd_count = 0   # 统计次数\n",
        "        self.rnd_m2 = 0.0    # 用于 Welford 法计算方差\n",
        "\n",
        "    def _calculate_value(self, state: np.ndarray) -> Tuple[float, float]:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            return self.model(state_tensor).item()\n",
        "    def _calculate_rnd_reward(self, state: np.array) -> float:\n",
        "      state_tensor = torch.FloatTensor(state)\n",
        "      with torch.no_grad():\n",
        "          target = self.random_net(state_tensor)\n",
        "          prediction = self.predictor_net(state_tensor)\n",
        "      raw_rnd = (target - prediction).pow(2).mean().item()\n",
        "      return self.rnd_scale * raw_rnd\n",
        "\n",
        "    def _perform_rollout(self, state: np.ndarray, num_rollouts: int = 3, rollout_depth: int = 5) -> float:\n",
        "      \"\"\"执行多次前向模拟来估计状态价值\"\"\"\n",
        "      total_value = 0.0\n",
        "\n",
        "      # 创建临时环境\n",
        "      rollout_env = gym.make('CartPole-v1')\n",
        "\n",
        "      for _ in range(num_rollouts):\n",
        "          # 每次rollout从初始状态开始\n",
        "          rollout_env.reset()\n",
        "          rollout_env.unwrapped.state = state.copy()\n",
        "\n",
        "          cumulative_reward = 0.0\n",
        "          discount = 1.0\n",
        "          terminal = False\n",
        "\n",
        "          for step in range(rollout_depth):\n",
        "              # 动作选择策略: 70%使用价值网络, 30%随机探索\n",
        "              if np.random.random() < 0.7:\n",
        "                  action_values = []\n",
        "                  for a in range(rollout_env.action_space.n):\n",
        "                      # 克隆当前状态\n",
        "                      temp_env = gym.make('CartPole-v1')\n",
        "                      temp_env.reset()\n",
        "                      temp_env.unwrapped.state = rollout_env.unwrapped.state.copy()\n",
        "\n",
        "                      # 模拟执行动作\n",
        "                      next_state, r, done, _, _ = temp_env.step(a)\n",
        "                      value = self._calculate_value(next_state)\n",
        "                      action_values.append(value)\n",
        "\n",
        "                  action = np.argmax(action_values)\n",
        "              else:\n",
        "                  # 随机探索\n",
        "                  action = rollout_env.action_space.sample()\n",
        "\n",
        "              # 执行选择的动作\n",
        "              next_state, reward, done, _, _ = rollout_env.step(action)\n",
        "\n",
        "              # 更新累计奖励\n",
        "              cumulative_reward += discount * reward\n",
        "              discount *= self.gamma\n",
        "\n",
        "              # 如果游戏结束，提前终止rollout\n",
        "              if done:\n",
        "                  terminal = True\n",
        "                  break\n",
        "\n",
        "          # 如果rollout没有自然终止，添加自举值估计\n",
        "          if not terminal:\n",
        "              bootstrap_value = self._calculate_value(next_state)\n",
        "              cumulative_reward += discount * bootstrap_value\n",
        "\n",
        "          total_value += cumulative_reward\n",
        "\n",
        "      # 返回平均价值\n",
        "      return total_value / num_rollouts\n",
        "\n",
        "    def _evaluate_state_with_rollout(self, state: np.ndarray) -> float:\n",
        "        \"\"\"结合神经网络和rollout评估状态价值\"\"\"\n",
        "        # 神经网络预测\n",
        "        network_value = self._calculate_value(state)\n",
        "\n",
        "        # Rollout模拟价值\n",
        "        rollout_value = self._perform_rollout(state, num_rollouts=3, rollout_depth=10)\n",
        "\n",
        "        # RND探索奖励\n",
        "        rnd_value = self._calculate_rnd_reward(state)\n",
        "\n",
        "        # 组合价值（权重可以调整）\n",
        "        combined_value = 0.5 * network_value + 0.4 * rollout_value + 0.1 * rnd_value\n",
        "\n",
        "        return combined_value\n",
        "    def bfs_search(self, current_state: np.ndarray) -> int:\n",
        "        \"\"\"执行一次BFS搜索返回最佳动作\"\"\"\n",
        "        root = Node(current_state)\n",
        "        queue = PriorityQueue()\n",
        "\n",
        "        # 计算初始状态的价值\n",
        "\n",
        "        queue.put((0, 0, root))\n",
        "\n",
        "        temp_env = gym.make('CartPole-v1')\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            if queue.empty():\n",
        "                break\n",
        "\n",
        "            priority, _, current_node = queue.get()\n",
        "\n",
        "            # 扩展子节点\n",
        "            for action in range(self.env.action_space.n):\n",
        "                temp_env.reset()\n",
        "                temp_env.unwrapped.state = current_node.state.copy()\n",
        "                next_state, reward, done, _, _ = temp_env.step(action)\n",
        "\n",
        "                next_node = Node(\n",
        "                    state=next_state,\n",
        "                    action_taken=action,\n",
        "                    parent=current_node,\n",
        "                    reward=reward,\n",
        "                    done=done\n",
        "                )\n",
        "\n",
        "                # 使用集成网络预测价值\n",
        "                next_value = self._evaluate_state_with_rollout(next_state) #+ self._calculate_rnd_reward(next_state)\n",
        "\n",
        "\n",
        "                # 终止状态处理\n",
        "                if done:\n",
        "                    next_node.value = reward #+ self._calculate_rnd_reward(next_state)\n",
        "                else:\n",
        "                    next_node.value = next_value\n",
        "\n",
        "                current_node.children[action] = next_node\n",
        "                self._backpropagate(next_node, next_value)\n",
        "\n",
        "                # 计算优先级（UCT + 不确定性奖励）\n",
        "\n",
        "                uct = next_node.value #+ self._calculate_rnd_reward(next_state)\n",
        "\n",
        "                '''# 随机探索\n",
        "                epsilon = max(0.01, self.initial_epsilon * (0.995 ** self.train_step_counter))\n",
        "                if np.random.rand() < 0.2:\n",
        "                    uct *= 2'''\n",
        "\n",
        "                queue.put((-uct, id(next_node), next_node))\n",
        "\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "            # 经验回填\n",
        "            if current_node.parent is not None:\n",
        "              with torch.no_grad():\n",
        "                  # 结合rollout的价值估计\n",
        "                  target_value = self._evaluate_state_with_rollout(current_node.state)\n",
        "\n",
        "              total_target = self.gamma * target_value + current_node.reward\n",
        "              if done:\n",
        "                  total_target = 1\n",
        "              self._remember(current_node.parent.state, total_target)\n",
        "\n",
        "            # 训练网络\n",
        "            if len(self.replay_buffer) >= self.batch_size:\n",
        "                self._replay()\n",
        "\n",
        "        # 选择最佳动作\n",
        "        best_action = max([0, 1], key=lambda a: root.children[a].value)\n",
        "\n",
        "        return best_action, root\n",
        "\n",
        "    def _backpropagate(self, node: Node, value: float):\n",
        "        \"\"\"回溯更新节点价值\"\"\"\n",
        "        while node is not None:\n",
        "            node.value = max(node.value, value)\n",
        "            node = node.parent\n",
        "\n",
        "    def _remember(self, state: np.ndarray, target_value: float):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target_value))\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        states = torch.FloatTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "    def _update_rnd_network(self, states):\n",
        "        \"\"\"更新 RND 的预测网络\"\"\"\n",
        "        states = torch.FloatTensor(states)\n",
        "        predictions = self.predictor_net(states)\n",
        "        with torch.no_grad():\n",
        "            targets = self.random_net(states)\n",
        "        rnd_loss = (predictions - targets).pow(2).mean()\n",
        "\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"经验回放训练\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        # 更新启发式网络\n",
        "        self.train_step_counter += 1\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "        # 更新 RND 网络\n",
        "        self._update_rnd_network(states)\n",
        "\n",
        "\n",
        "def train_agent():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    agent = BFS(num_simulations=100)\n",
        "\n",
        "    for episode in range(3):\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.bfs_search(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "        print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "pqdQASQrlf8m",
        "outputId": "a0ea0dfe-4083-4fdc-c986-91a55777de2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 1.0\n",
            "Episode 1, Total Reward: 2.0\n",
            "Episode 1, Total Reward: 3.0\n",
            "Episode 1, Total Reward: 4.0\n",
            "Episode 1, Total Reward: 5.0\n",
            "Episode 1, Total Reward: 6.0\n",
            "Episode 1, Total Reward: 7.0\n",
            "Episode 1, Total Reward: 8.0\n",
            "Episode 1, Total Reward: 9.0\n",
            "Episode 1, Total Reward: 10.0\n",
            "Episode 1, Total Reward: 11.0\n",
            "Episode 1, Total Reward: 12.0\n",
            "Episode 1, Total Reward: 13.0\n",
            "Episode 1, Total Reward: 14.0\n",
            "Episode 1, Total Reward: 15.0\n",
            "Episode 1, Total Reward: 16.0\n",
            "Episode 1, Total Reward: 17.0\n",
            "Episode 1, Total Reward: 18.0\n",
            "Episode 1, Total Reward: 19.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b914daafaa19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-b914daafaa19>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfs_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b914daafaa19>\u001b[0m in \u001b[0;36mbfs_search\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# 使用集成网络预测价值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mnext_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_state_with_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self._calculate_rnd_reward(next_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b914daafaa19>\u001b[0m in \u001b[0;36m_evaluate_state_with_rollout\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Rollout模拟价值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mrollout_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rollouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# RND探索奖励\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b914daafaa19>\u001b[0m in \u001b[0;36m_perform_rollout\u001b[0;34m(self, state, num_rollouts, rollout_depth)\u001b[0m\n\u001b[1;32m    131\u001b[0m                   \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                       \u001b[0;31m# 克隆当前状态\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                       \u001b[0mtemp_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                       \u001b[0mtemp_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                       \u001b[0mtemp_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menv_spec_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sutton_barto_reward, render_mode)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_short_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_short_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py\u001b[0m in \u001b[0;36marray_short_repr\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array_str_implementation\u001b[0;34m(a, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_guarded_repr_or_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mnext_line_prefix\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     lst = _formatArray(a, format_function, options['linewidth'],\n\u001b[0m\u001b[1;32m    547\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edgeitems'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                        summary_insert, options['legacy'])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;31m# invoke the recursive part with an initial index and prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         return recurser(index=(),\n\u001b[0m\u001b[1;32m    890\u001b[0m                         \u001b[0mhanging_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                         curr_width=line_width)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrailing_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_hanging_indent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m                 s, line = _extendLine_pretty(\n\u001b[1;32m    847\u001b[0m                     s, line, word, elem_width, hanging_indent, legacy)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxes_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mformat_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# when recursing, add a space to align with the [ added, and reduce the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                     \u001b[0msign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'+'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-tRAb6tpuJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Backpropgrate"
      ],
      "metadata": {
        "id": "da7R4zfrQOQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 Frozen-lake"
      ],
      "metadata": {
        "id": "wJ6iBGWgvRmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "# 神经网络模型定义\n",
        "class HeuristicNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()  # 输出0-1之间的启发值\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "# RND 网络定义\n",
        "class RNDNetwork(nn.Module):\n",
        "    def __init__(self, env_size, output_size=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class NeuralEnhancedBFSwithRND:\n",
        "    def __init__(self, env_size: int = 8, num_simulations: int = 100,\n",
        "                 buffer_size: int = 10000, batch_size: int = 32, rnd_scale: float = 0.40):\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size**2 - 1\n",
        "\n",
        "        # 启发式网络配置\n",
        "        self.model = HeuristicNetwork(env_size)\n",
        "        self.target_model = HeuristicNetwork(env_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # RND 网络配置\n",
        "        self.random_net = RNDNetwork(env_size).eval()  # 固定随机网络\n",
        "        self.predictor_net = RNDNetwork(env_size)      # 可训练预测网络\n",
        "        self.rnd_optimizer = optim.Adam(self.predictor_net.parameters(), lr=0.001)\n",
        "        self.rnd_scale = rnd_scale  # RND 内部奖励缩放因子\n",
        "\n",
        "        # 经验回放缓存\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络同步间隔\n",
        "        self.target_update_interval = 15\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "        self.rnd_scale = rnd_scale\n",
        "        self.rnd_mean = 0.0  # 移动平均\n",
        "        self.rnd_std = 1.0   # 移动标准差\n",
        "        self.rnd_count = 0   # 统计次数\n",
        "        self.rnd_m2 = 0.0    # 用于 Welford 法计算方差\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    def _get_valid_actions(self, state: int, env_size: int) -> List[int]:\n",
        "        row, col = state // env_size, state % env_size\n",
        "        valid_actions = []\n",
        "        if col > 0: valid_actions.append(0)    # 左\n",
        "        if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "        if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "        if row > 0: valid_actions.append(3)    # 上\n",
        "        return valid_actions\n",
        "\n",
        "    def _get_action_path(self, node: Node) -> List[int]:\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.model(state_tensor).item()\n",
        "\n",
        "    def _calculate_rnd_reward(self, state: int) -> float:\n",
        "        \"\"\"计算 RND 内部奖励\"\"\"\n",
        "        state_tensor = torch.LongTensor([state])\n",
        "        with torch.no_grad():\n",
        "            target = self.random_net(state_tensor)\n",
        "            prediction = self.predictor_net(state_tensor)\n",
        "        raw_rnd = (target - prediction).pow(2).mean().item()\n",
        "\n",
        "        '''# Welford 在线算法更新均值和标准差\n",
        "        self.rnd_count += 1\n",
        "        delta = raw_rnd - self.rnd_mean\n",
        "        self.rnd_mean += delta / self.rnd_count\n",
        "        delta2 = raw_rnd - self.rnd_mean\n",
        "        self.rnd_m2 += delta * delta2\n",
        "        if self.rnd_count > 1:\n",
        "            self.rnd_std = np.sqrt(self.rnd_m2 / (self.rnd_count - 1))\n",
        "\n",
        "        # 归一化并缩放\n",
        "        normalized_rnd = (raw_rnd - self.rnd_mean) / (self.rnd_std + 1e-5)  # 避免除零\n",
        "        scaled_rnd = self.rnd_scale * normalized_rnd\n",
        "\n",
        "        # 裁剪到合理范围'''\n",
        "        return raw_rnd * 0.2 #根据论文建议，限制最大值\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        states = torch.LongTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        predictions = self.model(states).squeeze()\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def _update_rnd_network(self, states):\n",
        "        \"\"\"更新 RND 的预测网络\"\"\"\n",
        "        states = torch.LongTensor(states)\n",
        "        predictions = self.predictor_net(states)\n",
        "        with torch.no_grad():\n",
        "            targets = self.random_net(states)\n",
        "        rnd_loss = (predictions - targets).pow(2).mean()\n",
        "\n",
        "        self.rnd_optimizer.zero_grad()\n",
        "        rnd_loss.backward()\n",
        "        self.rnd_optimizer.step()\n",
        "\n",
        "    def _remember(self, state, target, next_state=None):\n",
        "        \"\"\"存储经验，包括 RND 的状态\"\"\"\n",
        "        self.replay_buffer.append((state, target, next_state if next_state is not None else state))\n",
        "\n",
        "    def _replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets, next_states = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        # 更新启发式网络\n",
        "        self.train_step_counter += 1\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "        # 更新 RND 网络\n",
        "        self._update_rnd_network(next_states)\n",
        "\n",
        "    def _get_bootstrap_target(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            return self.target_model(state_tensor).item()\n",
        "\n",
        "    def bfs_search2(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                self._remember(current_node.state, 1.0, current_node.state)\n",
        "                break\n",
        "            # 收集训练数据，包括 RND 奖励\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                rnd_reward = self._calculate_rnd_reward(current_node.state)\n",
        "                total_target = min(target + rnd_reward, 1.0)  # 限制最大值为1\n",
        "                self._remember(current_node.parent.state, total_target, current_node.state)\n",
        "\n",
        "\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0, next_state)\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state) + self._calculate_rnd_reward(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "            self._replay()\n",
        "\n",
        "        self._backpropagate_values(root_node, goal_node if found_goal else None)\n",
        "        return None, 0, root_node\n",
        "\n",
        "    def _backup(self, new_node, root_node):\n",
        "        \"\"\"回溯更新节点价值\"\"\"\n",
        "        discount_factor = 0.95  # γ - 未来奖励的折扣因子\n",
        "        lambda_param = 0.8      # λ - 资格迹衰减参数\n",
        "        base_learning_rate = 0.1  # α - 基础学习率\n",
        "\n",
        "        traces = {}\n",
        "        path_nodes = []\n",
        "        current = new_node\n",
        "        while current:\n",
        "          traces[current.state] = 0.0  # 先清零，稍后递增\n",
        "          path_nodes.append(current)\n",
        "          current = current.parent\n",
        "        path_nodes = list(reversed(path_nodes))\n",
        "        for i, node in enumerate(path_nodes):\n",
        "          if i == len(path_nodes) - 1:\n",
        "            continue\n",
        "          traces[node.state] = traces.get(node.state, 0.0) + 1.0\n",
        "          current_reward = 1.0 if new_node.state == self.goal_state else 0.0\n",
        "\n",
        "          # 可以加入RND奖励\n",
        "          current_reward += self._calculate_rnd_reward(path_nodes[i+1].state)\n",
        "\n",
        "          # 使用启发式估计作为未来价值的估计\n",
        "          bootstrap_value = self._calculate_heuristic(path_nodes[i+1].state)\n",
        "\n",
        "          # TD目标 = 当前奖励 + 折扣系数 * 未来价值估计\n",
        "          td_target = current_reward + discount_factor * bootstrap_value\n",
        "\n",
        "          td_error = td_target - node.value\n",
        "\n",
        "          for state in traces:\n",
        "            node = self._find_node_by_state(state, root_node)  # 假设有方法找到对应节点\n",
        "            node.value += base_learning_rate * td_error * traces[state]\n",
        "\n",
        "          # 衰减所有资格迹\n",
        "          for state in traces:\n",
        "            traces[state] *= discount_factor * lambda_param\n",
        "    def _backpropagate_values(self, root_node: Node, goal_node: Optional[Node] = None):\n",
        "      \"\"\"\n",
        "      Propagate values through the search tree using TD(λ) principles.\n",
        "\n",
        "      Args:\n",
        "          root_node: The root node of the search tree\n",
        "          goal_node: The goal node if found, None otherwise\n",
        "      \"\"\"\n",
        "      discount_factor = 0.95  # γ - 未来奖励的折扣因子\n",
        "      lambda_param = 0.8      # λ - 资格迹衰减参数\n",
        "      base_learning_rate = 0.1  # α - 基础学习率\n",
        "\n",
        "      traces = {}\n",
        "\n",
        "      if goal_node:\n",
        "          print('find goal')\n",
        "          goal_node.value = 1.0  # 目标节点奖励\n",
        "          current = goal_node\n",
        "\n",
        "          # 初始化路径上的资格迹\n",
        "          while current:\n",
        "              traces[current.state] = 0.0  # 先清零，稍后递增\n",
        "              current = current.parent\n",
        "\n",
        "          # 从目标向上传播\n",
        "          current = goal_node\n",
        "          while current:\n",
        "              # 资格迹递增（当前状态）\n",
        "              traces[current.state] = traces.get(current.state, 0.0) + 1.0\n",
        "\n",
        "              # 计算 TD 目标和误差\n",
        "              r = 1.0 if current == goal_node else 0.0  # 仅目标节点有奖励\n",
        "              next_value = current.value\n",
        "              td_target = r + discount_factor * next_value\n",
        "              td_error = td_target - current.value\n",
        "\n",
        "              # 更新所有有资格迹的状态的价值\n",
        "              for state in traces:\n",
        "                  node = self._find_node_by_state(state,root_node)  # 假设有方法找到对应节点\n",
        "                  node.value += base_learning_rate * td_error * traces[state]\n",
        "\n",
        "              # 衰减所有资格迹\n",
        "              for state in traces:\n",
        "                  traces[state] *= discount_factor * lambda_param\n",
        "\n",
        "              current = current.parent\n",
        "      else:\n",
        "          print('xxxx')  # 未找到目标，待补充逻辑\n",
        "\n",
        "\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                self._remember(current_node.state, 1.0, current_node.state)\n",
        "                break\n",
        "\n",
        "            # 训练数据收集，包括RND奖励\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                rnd_reward = self._calculate_rnd_reward(current_node.state)\n",
        "                total_target = min(target + rnd_reward, 1.0)  # 限制最大值为1\n",
        "                self._remember(current_node.parent.state, total_target, current_node.state)\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0, next_state)\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state) + self._calculate_rnd_reward(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "                    self._backup(next_node,root_node)\n",
        "\n",
        "            self._replay()\n",
        "\n",
        "        # 搜索完成后，反向传播价值\n",
        "        #self._backpropagate_values(root_node, goal_node if found_goal else None)\n",
        "\n",
        "\n",
        "\n",
        "        return None, 0, root_node\n",
        "    def _find_node_by_state(self, state, start_node):\n",
        "      \"\"\"递归搜索树以找到具有特定状态的节点\"\"\"\n",
        "      # 基本情况：当前节点具有目标状态\n",
        "      if start_node.state == state:\n",
        "          return start_node\n",
        "\n",
        "      # 递归搜索每个子节点\n",
        "      for child in start_node.children.values():\n",
        "          found = self._find_node_by_state(state, child)\n",
        "          if found:\n",
        "              return found\n",
        "\n",
        "      # 如果在当前子树中没有找到，返回None\n",
        "      return None\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        for action, child in root_node.children.items():\n",
        "            value = self._evaluate_subtree(child)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        return best_action #if best_action is not None else random.choice(self._get_valid_actions(root_node.state, self.env_size))\n",
        "\n",
        "    def _evaluate_subtree(self, node: Node) -> float:\n",
        "        if node.state == self.goal_state:\n",
        "            return float('inf')\n",
        "\n",
        "        heuristic_value = self._calculate_heuristic(node.state)\n",
        "        rnd_reward = self._calculate_rnd_reward(node.state)\n",
        "        children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "        return  node.value +0.5 * children_value\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        best_action = self.get_best_action_from_tree(root_node)\n",
        "        return best_action\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state\n"
      ],
      "metadata": {
        "id": "WyBCsEuyQUtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def test_enhanced_bfs():\n",
        "    # 1. 创建简单的价值网络\n",
        "\n",
        "    # 2. 初始化环境和算法\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    #value_net = ValueNetwork(8)\n",
        "\n",
        "    bfs = NeuralEnhancedBFSwithRND()#EnhancedBFS(value_net, num_simulations=10)#Neural\n",
        "\n",
        "\n",
        "    # 3. 运行多个回合\n",
        "    num_episodes = 1\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\n开始测试NeuralEnhancedBFSwithRND...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        print(f\"\\n回合 {episode + 1}:\")\n",
        "        print(f\"起始状态: {state}\")\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # 使用算法选择动作\n",
        "            action = bfs.search(state)\n",
        "            print(f\"Steps {steps}: 在状态 {state} 选择动作 {action}\")\n",
        "\n",
        "            # 执行动作\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            print(f\"-> 新状态: {state}, 奖励: {reward}\")\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"成功到达目标！\")\n",
        "                else:\n",
        "                    print(\"失败（掉入陷阱或超时）\")\n",
        "\n",
        "        total_reward += episode_reward\n",
        "        print(f\"回合 {episode + 1} 结束 - 总步数: {steps}, 总奖励: {episode_reward}\")\n",
        "\n",
        "    print(f\"\\n测试完成 - 平均奖励: {total_reward/num_episodes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_bfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXOvwaENQZgZ",
        "outputId": "0e090c7c-af6c-45bd-c4d6-1f7b291f871f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "开始测试NeuralEnhancedBFSwithRND...\n",
            "\n",
            "回合 1:\n",
            "起始状态: 0\n",
            "Steps 0: 在状态 0 选择动作 2\n",
            "-> 新状态: 1, 奖励: 0.0\n",
            "Steps 1: 在状态 1 选择动作 1\n",
            "-> 新状态: 9, 奖励: 0.0\n",
            "Steps 2: 在状态 9 选择动作 1\n",
            "-> 新状态: 17, 奖励: 0.0\n",
            "Steps 3: 在状态 17 选择动作 1\n",
            "-> 新状态: 25, 奖励: 0.0\n",
            "Steps 4: 在状态 25 选择动作 2\n",
            "-> 新状态: 26, 奖励: 0.0\n",
            "Steps 5: 在状态 26 选择动作 2\n",
            "-> 新状态: 27, 奖励: 0.0\n",
            "Steps 6: 在状态 27 选择动作 2\n",
            "-> 新状态: 28, 奖励: 0.0\n",
            "Steps 7: 在状态 28 选择动作 1\n",
            "-> 新状态: 36, 奖励: 0.0\n",
            "Steps 8: 在状态 36 选择动作 1\n",
            "-> 新状态: 44, 奖励: 0.0\n",
            "Steps 9: 在状态 44 选择动作 2\n",
            "-> 新状态: 45, 奖励: 0.0\n",
            "Steps 10: 在状态 45 选择动作 1\n",
            "-> 新状态: 53, 奖励: 0.0\n",
            "Steps 11: 在状态 53 选择动作 1\n",
            "-> 新状态: 61, 奖励: 0.0\n",
            "Steps 12: 在状态 61 选择动作 2\n",
            "-> 新状态: 62, 奖励: 0.0\n",
            "Steps 13: 在状态 62 选择动作 2\n",
            "-> 新状态: 63, 奖励: 1.0\n",
            "成功到达目标！\n",
            "回合 1 结束 - 总步数: 14, 总奖励: 1.0\n",
            "\n",
            "测试完成 - 平均奖励: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzQMOKdab3i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Cartpole"
      ],
      "metadata": {
        "id": "IZ3rzdJzvXuf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hh6OUHLNvbmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}