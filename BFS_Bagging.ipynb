{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoOLIXsgHPkK5IYSuV6H6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/BFS_Bagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGUYMC-_ucoR",
        "outputId": "4bb020d5-407e-4687-95ff-44aad5bbf781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Learn Bagging"
      ],
      "metadata": {
        "id": "OU69Hg9-nrVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Optional\n",
        "from queue import PriorityQueue\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import lru_cache\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # state shape: (batch_size,)\n",
        "        x = self.embedding(state)  # shape: (batch_size, 32)\n",
        "        return self.fc(x)         # shape: (batch_size, 1)\n",
        "\n",
        "class Ensemble_Network(nn.Module):\n",
        "    def __init__(self, env_size, n_networks=5):\n",
        "        super().__init__()\n",
        "        self.networks = nn.ModuleList([\n",
        "            ValueNetwork(env_size)\n",
        "            for _ in range(n_networks)\n",
        "        ])\n",
        "        # 为每个网络维护一个mask数组，表示这个经验是否用于该网络的训练\n",
        "        self.masks = np.random.binomial(\n",
        "            n=1, p=0.8,  # 每个经验有0.8的概率被选中\n",
        "            size=(32, n_networks)\n",
        "        )\n",
        "    def forward(self, state):\n",
        "        # state shape: (batch_size,)\n",
        "        values = torch.stack([net(state) for net in self.networks])\n",
        "        # shape: (n_networks, batch_size, 1) -> (batch_size, n_networks, 1)\n",
        "        return values.permute(1, 0, 2)\n",
        "\n",
        "    def train_step(self, batch_size=32):\n",
        "      # 采样经验\n",
        "      indices, (state, action, reward, next_state, done) = self.buffer.sample(batch_size)\n",
        "      total_loss = 0\n",
        "      for i, (online, target) in enumerate(zip(self.online_net.networks, self.target_net.networks)):\n",
        "          # 获取这个网络的mask\n",
        "          mask = self.masks[indices, i]\n",
        "          if not mask.any():\n",
        "              continue\n",
        "\n",
        "          # 只使用mask为1的经验进行训练\n",
        "          masked_state = state[mask]\n",
        "          masked_action = action[mask]\n",
        "          masked_reward = reward[mask]\n",
        "          masked_next_state = next_state[mask]\n",
        "          masked_done = done[mask]\n",
        "\n",
        "          # 正常的Q学习更新...\n",
        "          current_q = online(masked_state).gather(1, masked_action)\n",
        "          with torch.no_grad():\n",
        "              next_q = target(masked_next_state).max(1)[0].unsqueeze(1)\n",
        "              target_q = masked_reward + (1 - masked_done) * 0.99 * next_q\n",
        "\n",
        "          loss = F.mse_loss(current_q, target_q)\n",
        "          total_loss += loss\n",
        "def calculate_uncertainty(values):\n",
        "    # values shape: (batch_size, n_networks, 1)\n",
        "    return torch.std(values, dim=1)  # shape: (batch_size, 1)\n",
        "\n",
        "def select_action(state, ensemble_net, epsilon=0.1):\n",
        "    # 处理输入\n",
        "    with torch.no_grad():  # 不需要梯度\n",
        "        if state.dim() == 0:\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "        # 获取预测\n",
        "        q_values = ensemble_net(state)  # (1, n_networks, 1)\n",
        "        mean_q = q_values.squeeze().mean()  # 标量\n",
        "        std_q = q_values.squeeze().std()    # 标量\n",
        "\n",
        "        # 计算动作分数\n",
        "        action_score = mean_q + std_q * epsilon\n",
        "\n",
        "        return action_score\n",
        "\n"
      ],
      "metadata": {
        "id": "zhQrlAhf1MiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Bagging_BFS Frozen-lake"
      ],
      "metadata": {
        "id": "uY-o6k3fnxcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 神经网络模型定义\n",
        "class HeuristicNetwork(nn.Module):\n",
        "    def __init__(self, env_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(env_size**2, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()  # 输出0-1之间的启发值\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.embedding(state)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    value: float = 0.0\n",
        "\n",
        "class NeuralEnhancedBFS:\n",
        "    def __init__(self, env_size: int = 8, num_simulations: int = 100,\n",
        "                 buffer_size: int = 1000, batch_size: int = 32, n_networks: int = 5):\n",
        "        self.env_size = env_size\n",
        "        self.env = self._create_env()\n",
        "        self.goal_state = env_size**2 - 1\n",
        "\n",
        "        # 使用集成网络替换单个网络\n",
        "        self.online_nets = nn.ModuleList([\n",
        "            HeuristicNetwork(env_size) for _ in range(n_networks)\n",
        "        ])\n",
        "        self.target_nets = nn.ModuleList([\n",
        "            HeuristicNetwork(env_size) for _ in range(n_networks)\n",
        "        ])\n",
        "\n",
        "        # 为每个网络创建优化器\n",
        "        self.optimizers = [\n",
        "            optim.Adam(net.parameters(), lr=0.001)\n",
        "            for net in self.online_nets\n",
        "        ]\n",
        "\n",
        "        # 创建mask\n",
        "        self.masks = np.random.binomial(\n",
        "            n=1, p=0.8,\n",
        "            size=(buffer_size, n_networks)\n",
        "        )\n",
        "\n",
        "        # 经验回放缓存\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络同步参数\n",
        "        self.target_update_interval = 15\n",
        "        self.train_step_counter = 0\n",
        "        self.tau = 0.005  # 软更新参数\n",
        "\n",
        "    def _create_env(self) -> gym.Env:\n",
        "        return gym.make('FrozenLake-v1',\n",
        "                       map_name=f\"{self.env_size}x{self.env_size}\",\n",
        "                       is_slippery=False,\n",
        "                       render_mode=None)\n",
        "\n",
        "    # ... (保留原有的_get_valid_actions和_get_action_path方法)\n",
        "    def _get_valid_actions(self,state: int, env_size: int) -> List[int]:\n",
        "      \"\"\"获取有效动作列表\"\"\"\n",
        "      row, col = state // env_size, state % env_size\n",
        "      valid_actions = []\n",
        "      if col > 0: valid_actions.append(0)    # 左\n",
        "      if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "      if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "      if row > 0: valid_actions.append(3)    # 上\n",
        "      return valid_actions\n",
        "\n",
        "    def _get_action_path(self,node: Node) -> List[int]:\n",
        "        \"\"\"获取动作路径\"\"\"\n",
        "        path = []\n",
        "        current = node\n",
        "        while current.parent:\n",
        "            path.append(current.action_taken)\n",
        "            current = current.parent\n",
        "        return list(reversed(path))\n",
        "    def _calculate_heuristic(self, state: int) -> float:\n",
        "        \"\"\"使用集成网络预测启发值\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.LongTensor([state])\n",
        "            # 获取所有网络的预测\n",
        "            predictions = torch.stack([\n",
        "                net(state_tensor) for net in self.online_nets\n",
        "            ])  # shape: (n_networks, 1, 1)\n",
        "\n",
        "            # 计算均值和不确定性\n",
        "            mean_pred = predictions.mean(dim=0).item()\n",
        "            std_pred = predictions.std(dim=0).item()\n",
        "\n",
        "            # 返回均值加上不确定性奖励\n",
        "            return mean_pred + 0.2 * std_pred\n",
        "\n",
        "    def _update_network(self, states, targets):\n",
        "        \"\"\"训练集成网络\"\"\"\n",
        "        states = torch.LongTensor(states)\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        # 获取当前batch的mask\n",
        "        batch_indices = np.random.randint(0, len(self.masks), size=len(states))\n",
        "        total_loss = 0\n",
        "\n",
        "        # 训练每个网络\n",
        "        for i, (online_net, target_net, optimizer) in enumerate(zip(\n",
        "            self.online_nets, self.target_nets, self.optimizers)):\n",
        "\n",
        "            # 获取这个网络的mask\n",
        "            mask = self.masks[batch_indices, i]\n",
        "            if not mask.any():\n",
        "                continue\n",
        "\n",
        "            # 只使用mask为1的经验进行训练\n",
        "            masked_states = states[mask]\n",
        "            masked_targets = targets[mask]\n",
        "\n",
        "            # 清空梯度\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 计算预测值和损失\n",
        "            predictions = online_net(masked_states).squeeze()\n",
        "            loss = F.mse_loss(predictions, masked_targets)\n",
        "\n",
        "            # 反向传播和优化\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # 软更新目标网络\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self._soft_update_target_networks()\n",
        "\n",
        "        return total_loss / len(self.online_nets)\n",
        "\n",
        "    def _soft_update_target_networks(self):\n",
        "        \"\"\"软更新所有目标网络\"\"\"\n",
        "        for target_net, online_net in zip(self.target_nets, self.online_nets):\n",
        "            for target_param, online_param in zip(\n",
        "                target_net.parameters(), online_net.parameters()):\n",
        "                target_param.data.copy_(\n",
        "                    target_param.data * (1.0 - self.tau) +\n",
        "                    online_param.data * self.tau\n",
        "                )\n",
        "\n",
        "    def _remember(self, state, target):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target))\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"经验回放\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, targets = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        self._update_network(states, targets)\n",
        "\n",
        "    def _get_bootstrap_target(self,  state):\n",
        "      \"\"\"使用Q学习方式计算训练目标\"\"\"\n",
        "      with torch.no_grad():\n",
        "          state_tensor = torch.LongTensor([state])\n",
        "          predictions = torch.stack([\n",
        "              net(state_tensor) for net in self.target_nets\n",
        "          ])\n",
        "          return predictions.mean(dim=0).item()\n",
        "\n",
        "    def bfs_search(self, start_state: int) -> Tuple[Optional[List[int]], int, Node]:\n",
        "        visited = set()\n",
        "        queue = PriorityQueue()\n",
        "        root_node = Node(state=start_state)\n",
        "        queue.put((-self._calculate_heuristic(start_state), id(root_node), root_node))\n",
        "        found_goal = False\n",
        "        goal_node = None\n",
        "\n",
        "        while not queue.empty():\n",
        "            _, _, current_node = queue.get()\n",
        "\n",
        "            if current_node.state in visited:\n",
        "                continue\n",
        "            visited.add(current_node.state)\n",
        "\n",
        "\n",
        "            # 收集训练数据\n",
        "            if current_node.parent is not None:\n",
        "                target = self._get_bootstrap_target(current_node.state)\n",
        "                self._remember(current_node.parent.state, target)\n",
        "\n",
        "            if current_node.state == self.goal_state:\n",
        "                found_goal = True\n",
        "                goal_node = current_node\n",
        "                # 传播成功信号\n",
        "                self._remember(current_node.state, 1.0)\n",
        "                break\n",
        "\n",
        "            for action in self._get_valid_actions(current_node.state, self.env_size):\n",
        "                self.env.reset()\n",
        "                self.env.unwrapped.s = current_node.state\n",
        "                next_state, _, terminated, _, _ = self.env.step(action)\n",
        "\n",
        "                if terminated and next_state != self.goal_state:\n",
        "                    self._remember(next_state, 0.0)  # 记录失败状态\n",
        "                    continue\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    next_node = Node(\n",
        "                        state=next_state,\n",
        "                        action_taken=action,\n",
        "                        parent=current_node,\n",
        "                        value=self._calculate_heuristic(next_state)\n",
        "                    )\n",
        "                    current_node.children[action] = next_node\n",
        "                    priority = -next_node.value\n",
        "                    queue.put((priority, id(next_node), next_node))\n",
        "\n",
        "            # 进行经验回放\n",
        "            self._replay()\n",
        "\n",
        "        if found_goal:\n",
        "          ##BACKUP##BACKUP##BACKUP##BACKUP\n",
        "          current = goal_node\n",
        "          while current.parent:\n",
        "              current.value += 1.0  # 或其他奖励值\n",
        "              current = current.parent\n",
        "\n",
        "          return self._get_action_path(goal_node), 1, root_node\n",
        "        return None, 0, root_node\n",
        "\n",
        "    # ... (保留其他辅助方法)\n",
        "    def get_best_action_from_tree(self, root_node: Node) -> int:\n",
        "      \"\"\"基于搜索树选择最佳动作\"\"\"\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      for action, child in root_node.children.items():\n",
        "          # 计算每个动作的价值\n",
        "          value = self._evaluate_subtree(child)\n",
        "          if value > best_value:\n",
        "              best_value = value\n",
        "              best_action = action\n",
        "\n",
        "      return best_action #if best_action is not None else self._get_best_heuristic_action(root_node.state)\n",
        "\n",
        "    def _evaluate_subtree(self, node: Node) -> float:\n",
        "        \"\"\"评估子树的价值\"\"\"\n",
        "        # 如果找到目标\n",
        "        if node.state == self.goal_state:\n",
        "            return float('inf')\n",
        "\n",
        "        # 综合考虑多个因素\n",
        "        #visit_value = node.visit_count  # 访问次数说明这个方向被多次探索\n",
        "        heuristic_value = node.value#self._calculate_heuristic(node.state)  # 启发式值\n",
        "        children_value = max([self._evaluate_subtree(child) for child in node.children.values()]) if node.children else 0\n",
        "        return heuristic_value + 0.5 * children_value  # 可以调整这些因素的权重\n",
        "\n",
        "\n",
        "    def search(self, root_state: int) -> int:\n",
        "        _, _, root_node = self.bfs_search(root_state)\n",
        "        best_action = self.get_best_action_from_tree(root_node)\n",
        "\n",
        "        '''# 使用最终结果更新网络\n",
        "        if best_action is not None:\n",
        "            next_state = self._simulate_step(root_state, best_action)\n",
        "            if next_state == self.goal_state:\n",
        "                self._remember(root_state, 1.0)\n",
        "            else:\n",
        "                self._remember(root_state, self._get_bootstrap_target(next_state))'''\n",
        "        return best_action\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.unwrapped.s = state\n",
        "        next_state, _, _, _, _ = self.env.step(action)\n",
        "        return next_state"
      ],
      "metadata": {
        "id": "sHOVOhVk2jhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from queue import PriorityQueue\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def test_enhanced_bfs():\n",
        "    # 1. 创建简单的价值网络\n",
        "\n",
        "    # 2. 初始化环境和算法\n",
        "    env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)#map_name=\"8x8\",\n",
        "    #value_net = ValueNetwork(8)\n",
        "\n",
        "    bfs = NeuralEnhancedBFS()#EnhancedBFS(value_net, num_simulations=10)#Neural\n",
        "\n",
        "\n",
        "    # 3. 运行多个回合\n",
        "    num_episodes = 1\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\n开始测试Enhanced BFS...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        print(f\"\\n回合 {episode + 1}:\")\n",
        "        print(f\"起始状态: {state}\")\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # 使用算法选择动作\n",
        "            action = bfs.search(state)\n",
        "            print(f\"Steps {steps}: 在状态 {state} 选择动作 {action}\")\n",
        "\n",
        "            # 执行动作\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            print(f\"-> 新状态: {state}, 奖励: {reward}\")\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"成功到达目标！\")\n",
        "                else:\n",
        "                    print(\"失败（掉入陷阱或超时）\")\n",
        "\n",
        "        total_reward += episode_reward\n",
        "        print(f\"回合 {episode + 1} 结束 - 总步数: {steps}, 总奖励: {episode_reward}\")\n",
        "\n",
        "    print(f\"\\n测试完成 - 平均奖励: {total_reward/num_episodes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_bfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Sbhgfq6sWK",
        "outputId": "d9c6c2e2-6bc9-4076-cf45-3a46eddddb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "开始测试Enhanced BFS...\n",
            "\n",
            "回合 1:\n",
            "起始状态: 0\n",
            "Steps 0: 在状态 0 选择动作 1\n",
            "-> 新状态: 8, 奖励: 0.0\n",
            "Steps 1: 在状态 8 选择动作 2\n",
            "-> 新状态: 9, 奖励: 0.0\n",
            "Steps 2: 在状态 9 选择动作 2\n",
            "-> 新状态: 10, 奖励: 0.0\n",
            "Steps 3: 在状态 10 选择动作 2\n",
            "-> 新状态: 11, 奖励: 0.0\n",
            "Steps 4: 在状态 11 选择动作 2\n",
            "-> 新状态: 12, 奖励: 0.0\n",
            "Steps 5: 在状态 12 选择动作 1\n",
            "-> 新状态: 20, 奖励: 0.0\n",
            "Steps 6: 在状态 20 选择动作 1\n",
            "-> 新状态: 28, 奖励: 0.0\n",
            "Steps 7: 在状态 28 选择动作 1\n",
            "-> 新状态: 36, 奖励: 0.0\n",
            "Steps 8: 在状态 36 选择动作 2\n",
            "-> 新状态: 37, 奖励: 0.0\n",
            "Steps 9: 在状态 37 选择动作 2\n",
            "-> 新状态: 38, 奖励: 0.0\n",
            "Steps 10: 在状态 38 选择动作 2\n",
            "-> 新状态: 39, 奖励: 0.0\n",
            "Steps 11: 在状态 39 选择动作 1\n",
            "-> 新状态: 47, 奖励: 0.0\n",
            "Steps 12: 在状态 47 选择动作 1\n",
            "-> 新状态: 55, 奖励: 0.0\n",
            "Steps 13: 在状态 55 选择动作 1\n",
            "-> 新状态: 63, 奖励: 1.0\n",
            "成功到达目标！\n",
            "回合 1 结束 - 总步数: 14, 总奖励: 1.0\n",
            "\n",
            "测试完成 - 平均奖励: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Bagging_BFS\n",
        " Cartpole\n"
      ],
      "metadata": {
        "id": "2H_ta5Zyn3BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(4, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        # 初始化最后一层权重为接近0的小值\n",
        "        nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: np.ndarray\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    value: float = 0.0\n",
        "    depth: int = 0\n",
        "    done: bool = False\n",
        "    reward: float = 0.0\n",
        "    visit_count: int = 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.parent is not None:\n",
        "            self.depth = self.parent.depth + 1\n",
        "\n",
        "class BaggingBFS:\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 40,\n",
        "                 buffer_size: int = 1000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 exploration_weight: float = 1.5,\n",
        "                 n_networks: int = 5):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.num_simulations = num_simulations\n",
        "        self.gamma = gamma\n",
        "        self.exploration_weight = exploration_weight\n",
        "        self.initial_epsilon = 0.2\n",
        "\n",
        "        # 创建多个网络进行集成\n",
        "        self.online_nets = nn.ModuleList([\n",
        "            ValueNetwork() for _ in range(n_networks)\n",
        "        ])\n",
        "        self.target_nets = nn.ModuleList([\n",
        "            ValueNetwork() for _ in range(n_networks)\n",
        "        ])\n",
        "\n",
        "        # 为每个网络创建优化器\n",
        "        self.optimizers = [\n",
        "            optim.Adam(net.parameters(), lr=0.001)\n",
        "            for net in self.online_nets\n",
        "        ]\n",
        "\n",
        "        # 创建mask用于bagging\n",
        "        self.masks = np.random.binomial(\n",
        "            n=1, p=0.8,\n",
        "            size=(buffer_size, n_networks)\n",
        "        )\n",
        "\n",
        "        # 经验回放\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 目标网络更新参数\n",
        "        self.train_step_counter = 0\n",
        "        self.target_update_interval = 15\n",
        "        self.tau = 0.005  # 软更新参数\n",
        "\n",
        "    def _calculate_value(self, state: np.ndarray) -> Tuple[float, float]:\n",
        "        \"\"\"使用集成网络计算状态价值\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            # 获取所有网络的预测\n",
        "            predictions = torch.stack([\n",
        "                net(state_tensor) for net in self.online_nets\n",
        "            ])  # shape: (n_networks, 1)\n",
        "\n",
        "            # 计算均值和不确定性\n",
        "            mean_pred = predictions.mean().item()\n",
        "            std_pred = predictions.std().item()\n",
        "\n",
        "            # 返回均值和不确定性奖励\n",
        "            return mean_pred, std_pred\n",
        "\n",
        "    def bfs_search(self, current_state: np.ndarray) -> int:\n",
        "        \"\"\"执行一次BFS搜索返回最佳动作\"\"\"\n",
        "        root = Node(current_state)\n",
        "        queue = PriorityQueue()\n",
        "\n",
        "        # 计算初始状态的价值\n",
        "        initial_value, initial_std = self._calculate_value(current_state)\n",
        "        queue.put((-(initial_value + 0.2 * initial_std), 0, root))\n",
        "\n",
        "        temp_env = gym.make('CartPole-v1')\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            if queue.empty():\n",
        "                break\n",
        "\n",
        "            priority, _, current_node = queue.get()\n",
        "\n",
        "            # 扩展子节点\n",
        "            for action in range(self.env.action_space.n):\n",
        "                temp_env.reset()\n",
        "                temp_env.unwrapped.state = current_node.state.copy()\n",
        "                next_state, reward, done, _, _ = temp_env.step(action)\n",
        "\n",
        "                next_node = Node(\n",
        "                    state=next_state,\n",
        "                    action_taken=action,\n",
        "                    parent=current_node,\n",
        "                    reward=reward,\n",
        "                    done=done\n",
        "                )\n",
        "\n",
        "                # 使用集成网络预测价值\n",
        "                next_value, next_std = self._calculate_value(next_state)\n",
        "\n",
        "                # 终止状态处理\n",
        "                if done:\n",
        "                    next_node.value = reward\n",
        "                else:\n",
        "                    next_node.value = next_value\n",
        "\n",
        "                current_node.children[action] = next_node\n",
        "                self._backpropagate(next_node, next_value)\n",
        "\n",
        "                # 计算优先级（UCT + 不确定性奖励）\n",
        "                uncertainty_bonus = 0.2 * next_std\n",
        "                uct = next_node.value  + uncertainty_bonus\n",
        "\n",
        "                # 随机探索\n",
        "                '''epsilon = max(0.01, self.initial_epsilon * (0.995 ** self.train_step_counter))\n",
        "                if np.random.rand() < epsilon:\n",
        "                    uct *= 2'''\n",
        "\n",
        "                queue.put((-uct, id(next_node), next_node))\n",
        "\n",
        "            current_node.visit_count += 1\n",
        "\n",
        "            # 经验回填\n",
        "            if current_node.parent is not None:\n",
        "                target_value = current_node.reward + (1-done) * self.gamma * current_node.value\n",
        "                self._remember(current_node.parent.state, target_value)\n",
        "\n",
        "            # 训练网络\n",
        "            if len(self.replay_buffer) >= self.batch_size:\n",
        "                self._replay()\n",
        "\n",
        "        # 选择最佳动作\n",
        "        best_action = max(\n",
        "            [0, 1],\n",
        "            key=lambda a: root.children[a].value + 0.2 * self._calculate_value(root.children[a].state)[1]\n",
        "        )\n",
        "\n",
        "        return best_action, root\n",
        "\n",
        "    def _remember(self, state: np.ndarray, target_value: float):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target_value))\n",
        "\n",
        "    def _backpropagate(self, node: Node, value: float):\n",
        "        \"\"\"回溯更新节点价值\"\"\"\n",
        "        while node is not None:\n",
        "            node.value = max(node.value, value)\n",
        "            node.visit_count += 1\n",
        "            node = node.parent\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"经验回放训练\"\"\"\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, target_values = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states))\n",
        "        target_values = torch.FloatTensor(target_values)\n",
        "\n",
        "        # 获取当前batch的mask\n",
        "        batch_indices = np.random.randint(0, len(self.masks), size=len(states))\n",
        "        total_loss = 0\n",
        "\n",
        "        # 训练每个网络\n",
        "        for i, (online_net, optimizer) in enumerate(zip(\n",
        "            self.online_nets, self.optimizers)):\n",
        "\n",
        "            # 获取这个网络的mask\n",
        "            mask = self.masks[batch_indices, i]\n",
        "            if not mask.any():\n",
        "                continue\n",
        "\n",
        "            # 只使用mask为1的经验进行训练\n",
        "            masked_states = states[mask]\n",
        "            masked_targets = target_values[mask]\n",
        "\n",
        "            # 训练网络\n",
        "            optimizer.zero_grad()\n",
        "            predictions = online_net(masked_states).squeeze()\n",
        "            loss = nn.MSELoss()(predictions, masked_targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # 软更新目标网络\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self._soft_update_target_networks()\n",
        "\n",
        "    def _soft_update_target_networks(self):\n",
        "        \"\"\"软更新所有目标网络\"\"\"\n",
        "        for target_net, online_net in zip(self.target_nets, self.online_nets):\n",
        "            for target_param, online_param in zip(\n",
        "                target_net.parameters(), online_net.parameters()):\n",
        "                target_param.data.copy_(\n",
        "                    target_param.data * (1.0 - self.tau) +\n",
        "                    online_param.data * self.tau\n",
        "                )\n",
        "\n",
        "def train_agent():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    agent = BaggingBFS(num_simulations=100)\n",
        "\n",
        "    for episode in range(2):\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.bfs_search(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "        print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_agent()"
      ],
      "metadata": {
        "id": "CwI8COwul0t8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1fe6e652-0c74-4600-c6f6-64d708595fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 1.0\n",
            "Episode 1, Total Reward: 2.0\n",
            "Episode 1, Total Reward: 3.0\n",
            "Episode 1, Total Reward: 4.0\n",
            "Episode 1, Total Reward: 5.0\n",
            "Episode 1, Total Reward: 6.0\n",
            "Episode 1, Total Reward: 7.0\n",
            "Episode 1, Total Reward: 8.0\n",
            "Episode 1, Total Reward: 9.0\n",
            "Episode 1, Total Reward: 10.0\n",
            "Episode 1, Total Reward: 11.0\n",
            "Episode 1, Total Reward: 12.0\n",
            "Episode 1, Total Reward: 13.0\n",
            "Episode 1, Total Reward: 14.0\n",
            "Episode 1, Total Reward: 15.0\n",
            "Episode 1, Total Reward: 16.0\n",
            "Episode 1, Total Reward: 17.0\n",
            "Episode 1, Total Reward: 18.0\n",
            "Episode 1, Total Reward: 19.0\n",
            "Episode 1, Total Reward: 20.0\n",
            "Episode 1, Total Reward: 21.0\n",
            "Episode 1, Total Reward: 22.0\n",
            "Episode 1, Total Reward: 22.0\n",
            "Episode 2, Total Reward: 1.0\n",
            "Episode 2, Total Reward: 2.0\n",
            "Episode 2, Total Reward: 3.0\n",
            "Episode 2, Total Reward: 4.0\n",
            "Episode 2, Total Reward: 5.0\n",
            "Episode 2, Total Reward: 6.0\n",
            "Episode 2, Total Reward: 7.0\n",
            "Episode 2, Total Reward: 8.0\n",
            "Episode 2, Total Reward: 9.0\n",
            "Episode 2, Total Reward: 10.0\n",
            "Episode 2, Total Reward: 11.0\n",
            "Episode 2, Total Reward: 12.0\n",
            "Episode 2, Total Reward: 13.0\n",
            "Episode 2, Total Reward: 14.0\n",
            "Episode 2, Total Reward: 15.0\n",
            "Episode 2, Total Reward: 16.0\n",
            "Episode 2, Total Reward: 17.0\n",
            "Episode 2, Total Reward: 18.0\n",
            "Episode 2, Total Reward: 19.0\n",
            "Episode 2, Total Reward: 20.0\n",
            "Episode 2, Total Reward: 21.0\n",
            "Episode 2, Total Reward: 22.0\n",
            "Episode 2, Total Reward: 23.0\n",
            "Episode 2, Total Reward: 24.0\n",
            "Episode 2, Total Reward: 25.0\n",
            "Episode 2, Total Reward: 26.0\n",
            "Episode 2, Total Reward: 27.0\n",
            "Episode 2, Total Reward: 28.0\n",
            "Episode 2, Total Reward: 29.0\n",
            "Episode 2, Total Reward: 30.0\n",
            "Episode 2, Total Reward: 31.0\n",
            "Episode 2, Total Reward: 32.0\n",
            "Episode 2, Total Reward: 33.0\n",
            "Episode 2, Total Reward: 34.0\n",
            "Episode 2, Total Reward: 35.0\n",
            "Episode 2, Total Reward: 36.0\n",
            "Episode 2, Total Reward: 37.0\n",
            "Episode 2, Total Reward: 38.0\n",
            "Episode 2, Total Reward: 39.0\n",
            "Episode 2, Total Reward: 40.0\n",
            "Episode 2, Total Reward: 41.0\n",
            "Episode 2, Total Reward: 42.0\n",
            "Episode 2, Total Reward: 43.0\n",
            "Episode 2, Total Reward: 44.0\n",
            "Episode 2, Total Reward: 45.0\n",
            "Episode 2, Total Reward: 46.0\n",
            "Episode 2, Total Reward: 47.0\n",
            "Episode 2, Total Reward: 48.0\n",
            "Episode 2, Total Reward: 49.0\n",
            "Episode 2, Total Reward: 50.0\n",
            "Episode 2, Total Reward: 51.0\n",
            "Episode 2, Total Reward: 52.0\n",
            "Episode 2, Total Reward: 53.0\n",
            "Episode 2, Total Reward: 54.0\n",
            "Episode 2, Total Reward: 55.0\n",
            "Episode 2, Total Reward: 56.0\n",
            "Episode 2, Total Reward: 57.0\n",
            "Episode 2, Total Reward: 58.0\n",
            "Episode 2, Total Reward: 59.0\n",
            "Episode 2, Total Reward: 60.0\n",
            "Episode 2, Total Reward: 61.0\n",
            "Episode 2, Total Reward: 62.0\n",
            "Episode 2, Total Reward: 63.0\n",
            "Episode 2, Total Reward: 64.0\n",
            "Episode 2, Total Reward: 65.0\n",
            "Episode 2, Total Reward: 66.0\n",
            "Episode 2, Total Reward: 67.0\n",
            "Episode 2, Total Reward: 68.0\n",
            "Episode 2, Total Reward: 69.0\n",
            "Episode 2, Total Reward: 70.0\n",
            "Episode 2, Total Reward: 71.0\n",
            "Episode 2, Total Reward: 72.0\n",
            "Episode 2, Total Reward: 73.0\n",
            "Episode 2, Total Reward: 74.0\n",
            "Episode 2, Total Reward: 75.0\n",
            "Episode 2, Total Reward: 76.0\n",
            "Episode 2, Total Reward: 77.0\n",
            "Episode 2, Total Reward: 78.0\n",
            "Episode 2, Total Reward: 79.0\n",
            "Episode 2, Total Reward: 80.0\n",
            "Episode 2, Total Reward: 81.0\n",
            "Episode 2, Total Reward: 82.0\n",
            "Episode 2, Total Reward: 83.0\n",
            "Episode 2, Total Reward: 84.0\n",
            "Episode 2, Total Reward: 85.0\n",
            "Episode 2, Total Reward: 86.0\n",
            "Episode 2, Total Reward: 87.0\n",
            "Episode 2, Total Reward: 88.0\n",
            "Episode 2, Total Reward: 89.0\n",
            "Episode 2, Total Reward: 90.0\n",
            "Episode 2, Total Reward: 91.0\n",
            "Episode 2, Total Reward: 92.0\n",
            "Episode 2, Total Reward: 93.0\n",
            "Episode 2, Total Reward: 94.0\n",
            "Episode 2, Total Reward: 95.0\n",
            "Episode 2, Total Reward: 96.0\n",
            "Episode 2, Total Reward: 97.0\n",
            "Episode 2, Total Reward: 98.0\n",
            "Episode 2, Total Reward: 99.0\n",
            "Episode 2, Total Reward: 100.0\n",
            "Episode 2, Total Reward: 101.0\n",
            "Episode 2, Total Reward: 102.0\n",
            "Episode 2, Total Reward: 103.0\n",
            "Episode 2, Total Reward: 104.0\n",
            "Episode 2, Total Reward: 105.0\n",
            "Episode 2, Total Reward: 106.0\n",
            "Episode 2, Total Reward: 107.0\n",
            "Episode 2, Total Reward: 108.0\n",
            "Episode 2, Total Reward: 109.0\n",
            "Episode 2, Total Reward: 110.0\n",
            "Episode 2, Total Reward: 111.0\n",
            "Episode 2, Total Reward: 112.0\n",
            "Episode 2, Total Reward: 113.0\n",
            "Episode 2, Total Reward: 114.0\n",
            "Episode 2, Total Reward: 115.0\n",
            "Episode 2, Total Reward: 116.0\n",
            "Episode 2, Total Reward: 117.0\n",
            "Episode 2, Total Reward: 118.0\n",
            "Episode 2, Total Reward: 119.0\n",
            "Episode 2, Total Reward: 120.0\n",
            "Episode 2, Total Reward: 121.0\n",
            "Episode 2, Total Reward: 122.0\n",
            "Episode 2, Total Reward: 123.0\n",
            "Episode 2, Total Reward: 124.0\n",
            "Episode 2, Total Reward: 125.0\n",
            "Episode 2, Total Reward: 126.0\n",
            "Episode 2, Total Reward: 127.0\n",
            "Episode 2, Total Reward: 128.0\n",
            "Episode 2, Total Reward: 129.0\n",
            "Episode 2, Total Reward: 130.0\n",
            "Episode 2, Total Reward: 131.0\n",
            "Episode 2, Total Reward: 132.0\n",
            "Episode 2, Total Reward: 133.0\n",
            "Episode 2, Total Reward: 134.0\n",
            "Episode 2, Total Reward: 135.0\n",
            "Episode 2, Total Reward: 136.0\n",
            "Episode 2, Total Reward: 137.0\n",
            "Episode 2, Total Reward: 138.0\n",
            "Episode 2, Total Reward: 139.0\n",
            "Episode 2, Total Reward: 140.0\n",
            "Episode 2, Total Reward: 141.0\n",
            "Episode 2, Total Reward: 142.0\n",
            "Episode 2, Total Reward: 143.0\n",
            "Episode 2, Total Reward: 144.0\n",
            "Episode 2, Total Reward: 145.0\n",
            "Episode 2, Total Reward: 146.0\n",
            "Episode 2, Total Reward: 147.0\n",
            "Episode 2, Total Reward: 148.0\n",
            "Episode 2, Total Reward: 149.0\n",
            "Episode 2, Total Reward: 150.0\n",
            "Episode 2, Total Reward: 151.0\n",
            "Episode 2, Total Reward: 152.0\n",
            "Episode 2, Total Reward: 153.0\n",
            "Episode 2, Total Reward: 154.0\n",
            "Episode 2, Total Reward: 155.0\n",
            "Episode 2, Total Reward: 156.0\n",
            "Episode 2, Total Reward: 157.0\n",
            "Episode 2, Total Reward: 158.0\n",
            "Episode 2, Total Reward: 159.0\n",
            "Episode 2, Total Reward: 160.0\n",
            "Episode 2, Total Reward: 161.0\n",
            "Episode 2, Total Reward: 162.0\n",
            "Episode 2, Total Reward: 163.0\n",
            "Episode 2, Total Reward: 164.0\n",
            "Episode 2, Total Reward: 165.0\n",
            "Episode 2, Total Reward: 166.0\n",
            "Episode 2, Total Reward: 167.0\n",
            "Episode 2, Total Reward: 168.0\n",
            "Episode 2, Total Reward: 169.0\n",
            "Episode 2, Total Reward: 170.0\n",
            "Episode 2, Total Reward: 171.0\n",
            "Episode 2, Total Reward: 172.0\n",
            "Episode 2, Total Reward: 173.0\n",
            "Episode 2, Total Reward: 174.0\n",
            "Episode 2, Total Reward: 175.0\n",
            "Episode 2, Total Reward: 176.0\n",
            "Episode 2, Total Reward: 177.0\n",
            "Episode 2, Total Reward: 178.0\n",
            "Episode 2, Total Reward: 179.0\n",
            "Episode 2, Total Reward: 180.0\n",
            "Episode 2, Total Reward: 181.0\n",
            "Episode 2, Total Reward: 182.0\n",
            "Episode 2, Total Reward: 183.0\n",
            "Episode 2, Total Reward: 184.0\n",
            "Episode 2, Total Reward: 185.0\n",
            "Episode 2, Total Reward: 186.0\n",
            "Episode 2, Total Reward: 187.0\n",
            "Episode 2, Total Reward: 188.0\n",
            "Episode 2, Total Reward: 189.0\n",
            "Episode 2, Total Reward: 190.0\n",
            "Episode 2, Total Reward: 191.0\n",
            "Episode 2, Total Reward: 192.0\n",
            "Episode 2, Total Reward: 193.0\n",
            "Episode 2, Total Reward: 194.0\n",
            "Episode 2, Total Reward: 195.0\n",
            "Episode 2, Total Reward: 196.0\n",
            "Episode 2, Total Reward: 197.0\n",
            "Episode 2, Total Reward: 198.0\n",
            "Episode 2, Total Reward: 199.0\n",
            "Episode 2, Total Reward: 200.0\n",
            "Episode 2, Total Reward: 201.0\n",
            "Episode 2, Total Reward: 202.0\n",
            "Episode 2, Total Reward: 203.0\n",
            "Episode 2, Total Reward: 204.0\n",
            "Episode 2, Total Reward: 205.0\n",
            "Episode 2, Total Reward: 206.0\n",
            "Episode 2, Total Reward: 207.0\n",
            "Episode 2, Total Reward: 208.0\n",
            "Episode 2, Total Reward: 209.0\n",
            "Episode 2, Total Reward: 210.0\n",
            "Episode 2, Total Reward: 211.0\n",
            "Episode 2, Total Reward: 212.0\n",
            "Episode 2, Total Reward: 213.0\n",
            "Episode 2, Total Reward: 214.0\n",
            "Episode 2, Total Reward: 215.0\n",
            "Episode 2, Total Reward: 216.0\n",
            "Episode 2, Total Reward: 217.0\n",
            "Episode 2, Total Reward: 218.0\n",
            "Episode 2, Total Reward: 219.0\n",
            "Episode 2, Total Reward: 220.0\n",
            "Episode 2, Total Reward: 221.0\n",
            "Episode 2, Total Reward: 222.0\n",
            "Episode 2, Total Reward: 223.0\n",
            "Episode 2, Total Reward: 224.0\n",
            "Episode 2, Total Reward: 225.0\n",
            "Episode 2, Total Reward: 226.0\n",
            "Episode 2, Total Reward: 227.0\n",
            "Episode 2, Total Reward: 228.0\n",
            "Episode 2, Total Reward: 229.0\n",
            "Episode 2, Total Reward: 230.0\n",
            "Episode 2, Total Reward: 231.0\n",
            "Episode 2, Total Reward: 232.0\n",
            "Episode 2, Total Reward: 233.0\n",
            "Episode 2, Total Reward: 234.0\n",
            "Episode 2, Total Reward: 235.0\n",
            "Episode 2, Total Reward: 236.0\n",
            "Episode 2, Total Reward: 237.0\n",
            "Episode 2, Total Reward: 238.0\n",
            "Episode 2, Total Reward: 239.0\n",
            "Episode 2, Total Reward: 240.0\n",
            "Episode 2, Total Reward: 241.0\n",
            "Episode 2, Total Reward: 242.0\n",
            "Episode 2, Total Reward: 243.0\n",
            "Episode 2, Total Reward: 244.0\n",
            "Episode 2, Total Reward: 245.0\n",
            "Episode 2, Total Reward: 246.0\n",
            "Episode 2, Total Reward: 247.0\n",
            "Episode 2, Total Reward: 248.0\n",
            "Episode 2, Total Reward: 249.0\n",
            "Episode 2, Total Reward: 250.0\n",
            "Episode 2, Total Reward: 251.0\n",
            "Episode 2, Total Reward: 252.0\n",
            "Episode 2, Total Reward: 253.0\n",
            "Episode 2, Total Reward: 254.0\n",
            "Episode 2, Total Reward: 255.0\n",
            "Episode 2, Total Reward: 256.0\n",
            "Episode 2, Total Reward: 257.0\n",
            "Episode 2, Total Reward: 258.0\n",
            "Episode 2, Total Reward: 259.0\n",
            "Episode 2, Total Reward: 260.0\n",
            "Episode 2, Total Reward: 261.0\n",
            "Episode 2, Total Reward: 262.0\n",
            "Episode 2, Total Reward: 263.0\n",
            "Episode 2, Total Reward: 264.0\n",
            "Episode 2, Total Reward: 265.0\n",
            "Episode 2, Total Reward: 266.0\n",
            "Episode 2, Total Reward: 267.0\n",
            "Episode 2, Total Reward: 268.0\n",
            "Episode 2, Total Reward: 269.0\n",
            "Episode 2, Total Reward: 270.0\n",
            "Episode 2, Total Reward: 271.0\n",
            "Episode 2, Total Reward: 272.0\n",
            "Episode 2, Total Reward: 273.0\n",
            "Episode 2, Total Reward: 274.0\n",
            "Episode 2, Total Reward: 275.0\n",
            "Episode 2, Total Reward: 276.0\n",
            "Episode 2, Total Reward: 277.0\n",
            "Episode 2, Total Reward: 278.0\n",
            "Episode 2, Total Reward: 279.0\n",
            "Episode 2, Total Reward: 279.0\n"
          ]
        }
      ]
    }
  ]
}