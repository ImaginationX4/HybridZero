{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCdU6BKNVZGgZZ9edFQf57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/THTS_BFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15b_unHiBLW",
        "outputId": "0aa7a849-435a-4bff-d4bf-4d65c64f3e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from queue import PriorityQueue\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "# 定义价值网络（保持不变）\n",
        "class ValueNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4, 256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 1)\n",
        "        )\n",
        "        torch.nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x).squeeze(-1)\n",
        "\n",
        "# 定义搜索树节点（增加唯一标识符）\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: np.ndarray\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    value: float = 0.0\n",
        "    depth: int = 0\n",
        "    done: bool = False\n",
        "    reward: float = 0.0\n",
        "    visit_count: int = 1\n",
        "    _id: int = field(init=False)  # 添加唯一ID用于哈希\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._id = id(self)\n",
        "        if self.parent is not None:\n",
        "            self.depth = self.parent.depth + 1\n",
        "\n",
        "class BFS:\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 10,\n",
        "                 buffer_size: int = 10000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 exploration_weight: float = 1.5):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.num_simulations = num_simulations\n",
        "        self.gamma = gamma\n",
        "        self.exploration_weight = exploration_weight\n",
        "\n",
        "        # 维护持久化搜索树\n",
        "        self.root: Optional[Node] = None\n",
        "        self.current_tree: Optional[Node] = None\n",
        "\n",
        "        # 神经网络与训练相关（保持不变）\n",
        "        self.model = ValueNetwork()\n",
        "        self.target_model = ValueNetwork()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.train_step_counter = 0\n",
        "        self.target_update_interval = 20\n",
        "\n",
        "    def bfs_search(self, current_state: np.ndarray) -> int:\n",
        "        \"\"\"基于持久化搜索树的UCT算法\"\"\"\n",
        "        # 初始化或更新搜索树\n",
        "        if self.root is None or not self._is_state_equal(self.root.state, current_state):\n",
        "            self.root = Node(current_state)\n",
        "        self.current_tree = self.root\n",
        "\n",
        "        # 创建临时环境用于模拟\n",
        "        temp_env = gym.make('CartPole-v1', render_mode='no_render')\n",
        "\n",
        "        # 执行多次蒙特卡洛模拟\n",
        "        for _ in range(self.num_simulations):\n",
        "            self._simulate(temp_env)\n",
        "\n",
        "        # 选择访问次数最多的动作\n",
        "        best_action = max(self.root.children.keys(), key=lambda a: self.root.children[a].visit_count)#max([0, 1], key=lambda a: self.root.children[a].visit_count)\n",
        "\n",
        "        return best_action, self.root\n",
        "\n",
        "    def _simulate(self, temp_env: gym.Env):\n",
        "        \"\"\"单次蒙特卡洛模拟（选择->扩展->评估->反向传播）\"\"\"\n",
        "        node = self.current_tree\n",
        "        path = []\n",
        "\n",
        "        # 阶段1: 选择（Selection）\n",
        "        while not node.done and len(node.children) == self.env.action_space.n:\n",
        "            # 使用UCT选择子节点\n",
        "            action = self._select_child(node)\n",
        "            node = node.children[action]\n",
        "            path.append(node)\n",
        "\n",
        "        # 阶段2: 扩展（Expansion）\n",
        "        if not node.done and len(node.children) < self.env.action_space.n:\n",
        "            # 选择未扩展的动作\n",
        "            available_actions = set(range(self.env.action_space.n)) - set(node.children.keys())\n",
        "            action = np.random.choice(list(available_actions))\n",
        "\n",
        "            # 执行动作得到新状态\n",
        "            temp_env.reset()\n",
        "            temp_env.unwrapped.state = node.state\n",
        "            next_state, reward, done, _, _ = temp_env.step(action)\n",
        "            # 预测节点价值\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(next_state)\n",
        "                next_value = self.model(state_tensor).item()\n",
        "\n",
        "            # 创建新节点\n",
        "            child = Node(\n",
        "                state=next_state,\n",
        "                action_taken=action,\n",
        "                parent=node,\n",
        "                value=next_value,\n",
        "                reward=reward,\n",
        "                done=done\n",
        "            )\n",
        "            node.children[action] = child\n",
        "            node = child\n",
        "            #path.append(node)\n",
        "\n",
        "        # 阶段3: 收集\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(node.state)\n",
        "            targe_value = self.target_model(state_tensor).item()\n",
        "        target = node.reward + (1 - node.done) * self.gamma * targe_value\n",
        "        self._remember(node.parent.state, target)\n",
        "\n",
        "        # 阶段4: 反向传播（Backpropagation）\n",
        "\n",
        "        self._backpropagate(path, node.value)\n",
        "\n",
        "        # 经验回放\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            self._replay()\n",
        "\n",
        "    def _select_child(self, node: Node) -> int:\n",
        "        \"\"\"基于UCT公式选择子节点\"\"\"\n",
        "        total_visits = node.visit_count\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "\n",
        "        for action in node.children:\n",
        "            child = node.children[action]\n",
        "            exploit = child.value\n",
        "            explore = self.exploration_weight * math.sqrt(math.log(total_visits + 1e-7) / (child.visit_count + 1e-7))\n",
        "            score = exploit + explore\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def _backpropagate1(self, path: list[Node], value: float):\n",
        "        \"\"\"反向传播更新路径上的节点\"\"\"\n",
        "        for node in reversed(path):\n",
        "            node.visit_count += 1\n",
        "            node.value += (value - node.value) / node.visit_count  # 增量平均\n",
        "            value = node.reward + self.gamma * value * (1 - node.done)\n",
        "\n",
        "            '''# 存储经验\n",
        "            if node.parent is not None:\n",
        "                target = node.reward + (1 - node.done) * self.gamma * node.value\n",
        "                self._remember(node.parent.state, target)'''\n",
        "    def _backpropagate(self, path: list[Node], value: float):\n",
        "      \"\"\"反向传播更新路径上的节点，使用加权平均更新节点值\"\"\"\n",
        "      for node in reversed(path):\n",
        "          node.visit_count += 1\n",
        "\n",
        "          if node.children:\n",
        "            total_visits = sum(child.visit_count for child in node.children.values())\n",
        "\n",
        "            weighted_value = sum(child.value * child.visit_count for child in node.children.values()) / total_visits\n",
        "            node.value = weighted_value\n",
        "\n",
        "\n",
        "    def _is_state_equal(self, state1: np.ndarray, state2: np.ndarray) -> bool:\n",
        "        \"\"\"状态比较（CartPole状态为连续值，需设定阈值）\"\"\"\n",
        "        return np.allclose(state1, state2, atol=1e-3)\n",
        "\n",
        "    # 以下方法与原始代码保持一致\n",
        "    def _remember(self, state: np.ndarray, target_value: float):\n",
        "        self.replay_buffer.append((state, target_value))\n",
        "\n",
        "    def _replay(self):\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, target_values = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states))\n",
        "        target_values = torch.FloatTensor(target_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        pred_values = self.model(states)\n",
        "        loss = self.loss_fn(pred_values, target_values)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "# 训练函数（保持不变）\n",
        "def train_agent():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    agent = BFS(num_simulations=100)\n",
        "\n",
        "    for episode in range(20):\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, node = agent.bfs_search(state)\n",
        "            next_state, reward, done, _,_ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, Reward: {total_reward}\")\n",
        "\n",
        "        print(f\"Episode {episode+1}, Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ERWGEH-2mcNf",
        "outputId": "b3ff8fc0-598d-4bb6-d557-dfd26716f514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='no_render' that is not in the possible render_modes (['human', 'rgb_array']).\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Reward: 1.0\n",
            "Episode 1, Reward: 2.0\n",
            "Episode 1, Reward: 3.0\n",
            "Episode 1, Reward: 4.0\n",
            "Episode 1, Reward: 5.0\n",
            "Episode 1, Reward: 6.0\n",
            "Episode 1, Reward: 7.0\n",
            "Episode 1, Reward: 8.0\n",
            "Episode 1, Reward: 9.0\n",
            "Episode 1, Reward: 10.0\n",
            "Episode 1, Reward: 11.0\n",
            "Episode 1, Reward: 12.0\n",
            "Episode 1, Reward: 13.0\n",
            "Episode 1, Reward: 14.0\n",
            "Episode 1, Reward: 15.0\n",
            "Episode 1, Reward: 16.0\n",
            "Episode 1, Reward: 17.0\n",
            "Episode 1, Reward: 18.0\n",
            "Episode 1, Reward: 19.0\n",
            "Episode 1, Reward: 20.0\n",
            "Episode 1, Reward: 21.0\n",
            "Episode 1, Reward: 22.0\n",
            "Episode 1, Reward: 23.0\n",
            "Episode 1, Reward: 24.0\n",
            "Episode 1, Reward: 25.0\n",
            "Episode 1, Reward: 26.0\n",
            "Episode 1, Reward: 27.0\n",
            "Episode 1, Reward: 28.0\n",
            "Episode 1, Reward: 29.0\n",
            "Episode 1, Reward: 30.0\n",
            "Episode 1, Reward: 31.0\n",
            "Episode 1, Reward: 32.0\n",
            "Episode 1, Reward: 33.0\n",
            "Episode 1, Reward: 34.0\n",
            "Episode 1, Reward: 35.0\n",
            "Episode 1, Reward: 36.0\n",
            "Episode 1, Reward: 37.0\n",
            "Episode 1, Reward: 38.0\n",
            "Episode 1, Reward: 39.0\n",
            "Episode 1, Reward: 40.0\n",
            "Episode 1, Reward: 41.0\n",
            "Episode 1, Reward: 42.0\n",
            "Episode 1, Reward: 43.0\n",
            "Episode 1, Reward: 44.0\n",
            "Episode 1, Reward: 45.0\n",
            "Episode 1, Reward: 46.0\n",
            "Episode 1, Reward: 47.0\n",
            "Episode 1, Reward: 48.0\n",
            "Episode 1, Reward: 49.0\n",
            "Episode 1, Reward: 50.0\n",
            "Episode 1, Reward: 51.0\n",
            "Episode 1, Reward: 52.0\n",
            "Episode 1, Reward: 53.0\n",
            "Episode 1, Reward: 54.0\n",
            "Episode 1, Reward: 55.0\n",
            "Episode 1, Reward: 56.0\n",
            "Episode 1, Reward: 57.0\n",
            "Episode 1, Reward: 58.0\n",
            "Episode 1, Reward: 59.0\n",
            "Episode 1, Reward: 60.0\n",
            "Episode 1, Reward: 61.0\n",
            "Episode 1, Reward: 62.0\n",
            "Episode 1, Reward: 63.0\n",
            "Episode 1, Reward: 64.0\n",
            "Episode 1, Reward: 65.0\n",
            "Episode 1, Reward: 66.0\n",
            "Episode 1, Reward: 67.0\n",
            "Episode 1, Reward: 68.0\n",
            "Episode 1, Reward: 69.0\n",
            "Episode 1, Reward: 70.0\n",
            "Episode 1, Reward: 71.0\n",
            "Episode 1, Reward: 72.0\n",
            "Episode 1, Reward: 73.0\n",
            "Episode 1, Reward: 74.0\n",
            "Episode 1, Reward: 75.0\n",
            "Episode 1, Reward: 76.0\n",
            "Episode 1, Reward: 77.0\n",
            "Episode 1, Reward: 78.0\n",
            "Episode 1, Reward: 79.0\n",
            "Episode 1, Reward: 80.0\n",
            "Episode 1, Reward: 81.0\n",
            "Episode 1, Reward: 82.0\n",
            "Episode 1, Reward: 83.0\n",
            "Episode 1, Reward: 84.0\n",
            "Episode 1, Reward: 85.0\n",
            "Episode 1, Reward: 86.0\n",
            "Episode 1, Reward: 87.0\n",
            "Episode 1, Reward: 88.0\n",
            "Episode 1, Reward: 89.0\n",
            "Episode 1, Reward: 90.0\n",
            "Episode 1, Reward: 91.0\n",
            "Episode 1, Reward: 92.0\n",
            "Episode 1, Reward: 93.0\n",
            "Episode 1, Reward: 94.0\n",
            "Episode 1, Reward: 95.0\n",
            "Episode 1, Reward: 96.0\n",
            "Episode 1, Reward: 97.0\n",
            "Episode 1, Reward: 98.0\n",
            "Episode 1, Reward: 99.0\n",
            "Episode 1, Reward: 100.0\n",
            "Episode 1, Reward: 101.0\n",
            "Episode 1, Reward: 102.0\n",
            "Episode 1, Reward: 103.0\n",
            "Episode 1, Reward: 104.0\n",
            "Episode 1, Reward: 105.0\n",
            "Episode 1, Reward: 106.0\n",
            "Episode 1, Reward: 107.0\n",
            "Episode 1, Reward: 108.0\n",
            "Episode 1, Reward: 109.0\n",
            "Episode 1, Reward: 110.0\n",
            "Episode 1, Reward: 111.0\n",
            "Episode 1, Reward: 112.0\n",
            "Episode 1, Reward: 113.0\n",
            "Episode 1, Reward: 114.0\n",
            "Episode 1, Reward: 115.0\n",
            "Episode 1, Reward: 116.0\n",
            "Episode 1, Reward: 117.0\n",
            "Episode 1, Reward: 118.0\n",
            "Episode 1, Reward: 119.0\n",
            "Episode 1, Reward: 120.0\n",
            "Episode 1, Reward: 121.0\n",
            "Episode 1, Reward: 122.0\n",
            "Episode 1, Reward: 123.0\n",
            "Episode 1, Reward: 124.0\n",
            "Episode 1, Reward: 125.0\n",
            "Episode 1, Reward: 126.0\n",
            "Episode 1, Reward: 127.0\n",
            "Episode 1, Reward: 128.0\n",
            "Episode 1, Reward: 129.0\n",
            "Episode 1, Reward: 130.0\n",
            "Episode 1, Reward: 131.0\n",
            "Episode 1, Reward: 132.0\n",
            "Episode 1, Reward: 133.0\n",
            "Episode 1, Reward: 134.0\n",
            "Episode 1, Reward: 135.0\n",
            "Episode 1, Reward: 136.0\n",
            "Episode 1, Reward: 137.0\n",
            "Episode 1, Reward: 138.0\n",
            "Episode 1, Reward: 139.0\n",
            "Episode 1, Reward: 140.0\n",
            "Episode 1, Reward: 141.0\n",
            "Episode 1, Reward: 142.0\n",
            "Episode 1, Reward: 143.0\n",
            "Episode 1, Reward: 144.0\n",
            "Episode 1, Reward: 145.0\n",
            "Episode 1, Reward: 146.0\n",
            "Episode 1, Reward: 147.0\n",
            "Episode 1, Reward: 148.0\n",
            "Episode 1, Reward: 149.0\n",
            "Episode 1, Reward: 150.0\n",
            "Episode 1, Reward: 151.0\n",
            "Episode 1, Reward: 152.0\n",
            "Episode 1, Reward: 153.0\n",
            "Episode 1, Reward: 154.0\n",
            "Episode 1, Reward: 155.0\n",
            "Episode 1, Reward: 156.0\n",
            "Episode 1, Reward: 157.0\n",
            "Episode 1, Reward: 158.0\n",
            "Episode 1, Reward: 159.0\n",
            "Episode 1, Reward: 160.0\n",
            "Episode 1, Reward: 161.0\n",
            "Episode 1, Reward: 162.0\n",
            "Episode 1, Reward: 163.0\n",
            "Episode 1, Reward: 164.0\n",
            "Episode 1, Reward: 165.0\n",
            "Episode 1, Reward: 166.0\n",
            "Episode 1, Reward: 167.0\n",
            "Episode 1, Reward: 168.0\n",
            "Episode 1, Reward: 169.0\n",
            "Episode 1, Reward: 170.0\n",
            "Episode 1, Reward: 171.0\n",
            "Episode 1, Reward: 172.0\n",
            "Episode 1, Reward: 173.0\n",
            "Episode 1, Reward: 174.0\n",
            "Episode 1, Reward: 175.0\n",
            "Episode 1, Reward: 176.0\n",
            "Episode 1, Reward: 177.0\n",
            "Episode 1, Reward: 178.0\n",
            "Episode 1, Reward: 179.0\n",
            "Episode 1, Reward: 180.0\n",
            "Episode 1, Reward: 181.0\n",
            "Episode 1, Reward: 182.0\n",
            "Episode 1, Reward: 183.0\n",
            "Episode 1, Reward: 184.0\n",
            "Episode 1, Reward: 185.0\n",
            "Episode 1, Reward: 186.0\n",
            "Episode 1, Reward: 187.0\n",
            "Episode 1, Reward: 188.0\n",
            "Episode 1, Reward: 189.0\n",
            "Episode 1, Reward: 190.0\n",
            "Episode 1, Reward: 191.0\n",
            "Episode 1, Reward: 192.0\n",
            "Episode 1, Reward: 193.0\n",
            "Episode 1, Reward: 194.0\n",
            "Episode 1, Reward: 195.0\n",
            "Episode 1, Reward: 196.0\n",
            "Episode 1, Reward: 197.0\n",
            "Episode 1, Reward: 198.0\n",
            "Episode 1, Reward: 199.0\n",
            "Episode 1, Reward: 200.0\n",
            "Episode 1, Reward: 201.0\n",
            "Episode 1, Reward: 202.0\n",
            "Episode 1, Reward: 203.0\n",
            "Episode 1, Reward: 204.0\n",
            "Episode 1, Reward: 205.0\n",
            "Episode 1, Reward: 206.0\n",
            "Episode 1, Reward: 207.0\n",
            "Episode 1, Reward: 208.0\n",
            "Episode 1, Reward: 209.0\n",
            "Episode 1, Reward: 210.0\n",
            "Episode 1, Reward: 211.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-da0df2328061>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-da0df2328061>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfs_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-da0df2328061>\u001b[0m in \u001b[0;36mbfs_search\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# 执行多次蒙特卡洛模拟\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# 选择访问次数最多的动作\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-da0df2328061>\u001b[0m in \u001b[0;36m_simulate\u001b[0;34m(self, temp_env)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# 经验回放\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_select_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-da0df2328061>\u001b[0m in \u001b[0;36m_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.THIS-BFS FROZEN LAKE\n"
      ],
      "metadata": {
        "id": "Mbswx14pR7xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_size=16, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.state_size = state_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # 输入是状态索引（无需one-hot编码）\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(1, hidden_size),  # 输入是状态索引（标量）\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)  # 输出是状态值\n",
        "        )\n",
        "\n",
        "        # 初始化最后一层的权重\n",
        "        nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 输入x是状态索引（标量），转换为浮点数张量\n",
        "        x = x.float().unsqueeze(-1)  # 形状: (batch_size, 1)\n",
        "        return self.layers(x).squeeze(-1)  # 输出形状: (batch_size,)\n",
        "class ValueNetwork1(torch.nn.Module):\n",
        "    def __init__(self, state_size=16):\n",
        "        super().__init__() # 嵌入层处理离散状态\n",
        "        self.state_size=state_size\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(state_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1)\n",
        "        )\n",
        "        torch.nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''a = torch.zeros(self.state_size)\n",
        "        a[x]=1'''\n",
        "        x = x.long()\n",
        "        x = F.one_hot(x, num_classes=self.state_size).float()   # 输入为状态索引\n",
        "        return self.layers(x).squeeze(-1)\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int  # FrozenLake 状态是离散的 (0-15)\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    cost: float = 0.0      # 改为成本（越小越好）\n",
        "    done: bool = False\n",
        "    visit_count: int = 1\n",
        "\n",
        "    shortest_path_length: float = float('inf')\n",
        "    best_action: Optional[int] = None\n",
        "    is_safe_path: bool = False\n",
        "    reached_goal: bool = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._id = id(self)\n",
        "\n",
        "class UCTFrozenLake:\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 1,\n",
        "                 buffer_size: int = 1000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 exploration_weight: float = 1.5):\n",
        "      self.num_simulations = num_simulations\n",
        "      self.gamma = gamma\n",
        "      self.exploration_weight = exploration_weight\n",
        "      self.env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "      # 神经网络\n",
        "      self.model = ValueNetwork(state_size=16)\n",
        "      self.target_model = ValueNetwork(state_size=16)\n",
        "      self.target_model.load_state_dict(self.model.state_dict())\n",
        "      self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "      self.loss_fn = torch.nn.MSELoss()\n",
        "      self.replay_buffer = deque(maxlen=buffer_size)\n",
        "      self.batch_size = batch_size\n",
        "      self.train_step_counter = 0\n",
        "      self.target_update_interval = 20\n",
        "\n",
        "      # 搜索树\n",
        "      self.root: Optional[Node] = None\n",
        "    def search(self, current_state: int):\n",
        "      self.observe_state(current_state)\n",
        "      self.root = Node(state=current_state)\n",
        "      temp_env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "      for _ in range(self.num_simulations):\n",
        "        self._simulate(temp_env, self.root)\n",
        "       # 选择访问次数最多的动作（最小化成本）\n",
        "      best_action = max(self.root.children.keys(), key=lambda a: self.root.children[a].visit_count)\n",
        "      #print('self.root.children.keys()',self.root.children.keys())\n",
        "      return best_action, self.root\n",
        "    def _simulate(self, temp_env: gym.Env, root: Node):\n",
        "      path = []\n",
        "      node = root\n",
        "      done = root.done\n",
        "\n",
        "      # Selection\n",
        "      while not done and node.children:\n",
        "          action = self._select_child(node)\n",
        "          node = node.children[action]\n",
        "          path.append(node)\n",
        "          done = node.done\n",
        "\n",
        "      # Expansion\n",
        "      if not done:\n",
        "          for action in range(self.env.action_space.n):\n",
        "              if action not in node.children:\n",
        "                  temp_env.reset()\n",
        "                  temp_env.unwrapped.s = node.state\n",
        "                  next_state, reward, terminated, truncated, _ = temp_env.step(action)\n",
        "                  done = terminated or truncated\n",
        "\n",
        "                  # 使用model预测初始成本\n",
        "                  state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
        "                  initial_cost = self.model(state_tensor).item()\n",
        "\n",
        "                  child = Node(\n",
        "                      state=next_state,\n",
        "                      action_taken=action,\n",
        "                      parent=node,\n",
        "                      cost=initial_cost,\n",
        "                      done=done\n",
        "                  )\n",
        "                  node.children[action] = child\n",
        "\n",
        "      # Evaluation\n",
        "      if node.done and node.state == 15:\n",
        "          final_cost = 0.0\n",
        "      elif node.done and node.state != 15:\n",
        "          final_cost = 1000000.0\n",
        "      else:\n",
        "          with torch.no_grad():\n",
        "              state_tensor = torch.tensor([node.state], dtype=torch.float32)\n",
        "              final_cost = 1 + self.gamma * self.target_model(state_tensor).item()\n",
        "\n",
        "      # Backup\n",
        "      self._backpropagate(path, final_cost)\n",
        "\n",
        "      # 存储经验\n",
        "      if node.parent is not None:\n",
        "          self._remember(node.parent.state, final_cost)\n",
        "\n",
        "      # 经验回放\n",
        "      if len(self.replay_buffer) >= self.batch_size:\n",
        "          self._replay()\n",
        "    def _simulate1(self, temp_env: gym.Env, root: Node):\n",
        "      path = [root]\n",
        "      node = root\n",
        "      done = root.done\n",
        "      current_path_states = set([root.state])\n",
        "      # 阶段1: Selection\n",
        "      while not done and node.children:\n",
        "          action = self._select_child(node)\n",
        "          if node.children[action] in path:\n",
        "            continue\n",
        "\n",
        "          node = node.children[action]\n",
        "          '''if node.state in current_path_states:\n",
        "            # 发现循环，给予惩罚\n",
        "            return self._backpropagate(path, -1.0)\n",
        "          current_path_states.add(node.state) '''\n",
        "          path.append(node)\n",
        "          done = node.done\n",
        "\n",
        "      '''print('叶子节点',node.state)\n",
        "      print('叶子节点孩子有么？',node.children)\n",
        "      print('叶子节点结束没',node.done)  '''\n",
        "     # 阶段2: Expansion\n",
        "      while not node.done and len(node.children) < self.env.action_space.n:\n",
        "          #print('叶子节点扩张没',True)\n",
        "          action = np.random.choice([a for a in range(4) if a not in node.children])\n",
        "          #print('action',[a for a in range(4) if a not in node.children])\n",
        "          temp_env.reset()\n",
        "          temp_env.unwrapped.s = node.state  # 直接设置状态\n",
        "          next_state, reward, terminated, truncated, _ = temp_env.step(action)\n",
        "          done = terminated or truncated\n",
        "\n",
        "          '''# FrozenLake奖励调整：到达目标+1，其他情况0\n",
        "          cost = 1.0  # 每步成本为1（最小化步数）\n",
        "          if terminated and next_state == 15:  # 到达目标\n",
        "              cost = 0.0'''\n",
        "\n",
        "          # 使用model预测初始成本\n",
        "\n",
        "          state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
        "          initial_cost = self.model(state_tensor).item()\n",
        "\n",
        "          child = Node(\n",
        "              state=next_state,\n",
        "              action_taken=action,\n",
        "              parent=node,\n",
        "              cost=initial_cost,\n",
        "              done=done\n",
        "          )\n",
        "          node.children[action] = child\n",
        "\n",
        "          #path.append(child)\n",
        "          #node = child\n",
        "      # 阶段3: Evaluation & Backup\n",
        "      if node.done and node.state == 15:\n",
        "          final_cost = 0.0\n",
        "      elif node.done and node.state != 15:\n",
        "        final_cost = 10.0\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "\n",
        "          state_tensor = torch.tensor([node.state], dtype=torch.float32)\n",
        "          final_cost = 1 + self.gamma * self.target_model(state_tensor).item()\n",
        "\n",
        "      # 反向传播累积成本\n",
        "\n",
        "      self._backpropagate(path, final_cost)\n",
        "      #self._backpropagate(path, node.done, node.state==15)\n",
        "\n",
        "      # 存储经验\n",
        "      if node.parent is not None:\n",
        "        self._remember(node.parent.state,  final_cost)\n",
        "\n",
        "      # 经验回放\n",
        "      if len(self.replay_buffer) >= self.batch_size:\n",
        "          for i in range(4):\n",
        "            self._replay()\n",
        "    def _get_valid_actions(self,state, env_size=4):\n",
        "      \"\"\"获取有效动作列表\"\"\"\n",
        "      row, col = state // env_size, state % env_size\n",
        "      valid_actions = []\n",
        "      if col > 0: valid_actions.append(0)    # 左\n",
        "      if row < env_size - 1: valid_actions.append(1)    # 下\n",
        "      if col < env_size - 1: valid_actions.append(2)    # 右\n",
        "      if row > 0: valid_actions.append(3)    # 上\n",
        "      return valid_actions\n",
        "    def _select_child(self, node: Node) -> int:\n",
        "        \"\"\"基于UCT公式选择子节点（最小化成本）\"\"\"\n",
        "        total_visits = node.visit_count\n",
        "        best_score = float('inf')  # 找最小值\n",
        "        best_action = -1\n",
        "\n",
        "        for action in self._get_valid_actions(node.state):\n",
        "            child = node.children[action]\n",
        "            #print('child.state',child.state)\n",
        "            #print('child.cost',child.cost)\n",
        "            exploit = child.cost\n",
        "\n",
        "            explore = self.exploration_weight * math.sqrt(math.log(total_visits + 1e-7) / (child.visit_count + 1e-7))\n",
        "            score = exploit - explore  # 成本越小越好，因此减去探索项\n",
        "\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "        if total_visits < 25:\n",
        "          if np.random.random() < 0.2:\n",
        "              return np.random.choice(list(node.children.keys()))\n",
        "        return best_action\n",
        "    def _backpropagate(self, path: list[Node], value: float):\n",
        "      \"\"\"反向传播更新路径上的节点，使用加权平均更新节点值\"\"\"\n",
        "\n",
        "      for node in reversed(path):\n",
        "\n",
        "          node.visit_count += 1\n",
        "\n",
        "          if node.children:\n",
        "            total_visits = sum(child.visit_count for child in node.children.values())\n",
        "\n",
        "            weighted_cost = sum(child.cost * child.visit_count for child in node.children.values()) / total_visits\n",
        "            node.cost = 0.8*weighted_cost\n",
        "\n",
        "\n",
        "\n",
        "    def _backpropagate1(self, path: list[Node], terminated: bool, reached_goal: bool):\n",
        "      \"\"\"\n",
        "      Backpropagate path information\n",
        "      \"\"\"\n",
        "      path_length = len(path)\n",
        "      if not path:  # 如果path为空\n",
        "        return\n",
        "      # Case 1: Found goal state\n",
        "      if reached_goal:\n",
        "          # Update all nodes in path\n",
        "          for i, node in enumerate(reversed(path)):\n",
        "              current_length = path_length - i\n",
        "\n",
        "              # Only update if we found a shorter path\n",
        "              if current_length < node.shortest_path_length:\n",
        "                  node.shortest_path_length = current_length\n",
        "                  node.is_safe_path = True\n",
        "                  node.reached_goal = True\n",
        "\n",
        "                  # Record best action (except for last node)\n",
        "                  if i < len(path) - 1:\n",
        "                      next_node = path[-(i+1)]\n",
        "                      node.best_action = next_node.action_taken\n",
        "\n",
        "      # Case 2: Hit a hole\n",
        "      elif terminated:\n",
        "          # Mark path as unsafe\n",
        "          for node in path:\n",
        "              node.is_safe_path = False\n",
        "\n",
        "      # Case 3: Regular state\n",
        "      else:\n",
        "          # Get best child info\n",
        "          node = path[-1]\n",
        "          if node.children:\n",
        "              best_child = min(node.children.values(),\n",
        "                            key=lambda n: n.shortest_path_length)\n",
        "\n",
        "              # Update if child has path to goal\n",
        "              if best_child.reached_goal:\n",
        "                  new_length = best_child.shortest_path_length + 1\n",
        "                  if new_length < node.shortest_path_length:\n",
        "                      node.shortest_path_length = new_length\n",
        "                      node.best_action = best_child.action_taken\n",
        "                      node.is_safe_path = best_child.is_safe_path\n",
        "                      node.reached_goal = True\n",
        "    def observe_uct_decision(self, node: Node, action: int, total_visits: int):\n",
        "        \"\"\"观察UCT决策过程\"\"\"\n",
        "        child = node.children[action]\n",
        "        exploit = child.cost\n",
        "        explore = self.exploration_weight * math.sqrt(math.log(total_visits + 1e-7) / (child.visit_count + 1e-7))\n",
        "        score = exploit - explore\n",
        "\n",
        "        print(f\"\\n=== UCT Decision Analysis for State {node.state} Action {action} ===\")\n",
        "        print(f\"Child State: {child.state}\")\n",
        "        print(f\"Visit Count: {child.visit_count}\")\n",
        "        print(f\"Cost: {child.cost:.4f}\")\n",
        "        print(f\"Exploit Term: {exploit:.4f}\")\n",
        "        print(f\"Explore Term: {explore:.4f}\")\n",
        "        print(f\"Final UCT Score: {score:.4f}\")\n",
        "        return score\n",
        "\n",
        "    def observe_state(self, state: int):\n",
        "        \"\"\"观察状态信息\"\"\"\n",
        "        print(f\"\\n=== State Analysis ===\")\n",
        "        print(f\"Current State: {state}\")\n",
        "        # 计算到目标的曼哈顿距离\n",
        "        row, col = state // 4, state % 4\n",
        "        manhattan_dist = abs(row - 3) + abs(col - 3)\n",
        "        print(f\"Manhattan Distance to Goal: {manhattan_dist}\")\n",
        "        # 显示可能的动作\n",
        "        possible_actions = []\n",
        "        for action in range(4):\n",
        "            if action == 0:  # left\n",
        "                next_col = max(0, col - 1)\n",
        "                next_state = row * 4 + next_col\n",
        "            elif action == 1:  # down\n",
        "                next_row = min(3, row + 1)\n",
        "                next_state = next_row * 4 + col\n",
        "            elif action == 2:  # right\n",
        "                next_col = min(3, col + 1)\n",
        "                next_state = row * 4 + next_col\n",
        "            elif action == 3:  # up\n",
        "                next_row = max(0, row - 1)\n",
        "                next_state = next_row * 4 + col\n",
        "            possible_actions.append((action, next_state))\n",
        "        print(\"Possible Actions:\")\n",
        "        for action, next_state in possible_actions:\n",
        "            print(f\"Action {action} -> State {next_state}\")\n",
        "    def _remember(self, state: int, target_cost: float):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target_cost))\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"训练网络\"\"\"\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, target_costs = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).unsqueeze(-1)\n",
        "        target_costs = torch.tensor(target_costs, dtype=torch.float32)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        pred_costs = self.model(states)\n",
        "        loss = self.loss_fn(pred_costs, target_costs)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "def train_frozenlake():\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    agent = UCTFrozenLake(num_simulations=50)\n",
        "\n",
        "    for episode in range(1):\n",
        "        state = env.reset()[0]\n",
        "        total_cost = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, node = agent.search(state)\n",
        "            #print(f\"Episode {episode+1}, state: {state}\")\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_cost += 1  # 每步成本+1\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, state: {next_state}\")\n",
        "            print('children',[children.visit_count for children in node.children.values()])\n",
        "\n",
        "            if terminated and next_state == 15:\n",
        "                total_cost = 0  # 到达目标时总成本为0\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_frozenlake()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b063W3cr00r7",
        "outputId": "7298604a-961b-4197-d810-3ac0e49627cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== State Analysis ===\n",
            "Current State: 0\n",
            "Manhattan Distance to Goal: 6\n",
            "Possible Actions:\n",
            "Action 0 -> State 0\n",
            "Action 1 -> State 4\n",
            "Action 2 -> State 1\n",
            "Action 3 -> State 0\n",
            "Episode 1, state: 1\n",
            "children [2, 3, 40, 8]\n",
            "\n",
            "=== State Analysis ===\n",
            "Current State: 1\n",
            "Manhattan Distance to Goal: 5\n",
            "Possible Actions:\n",
            "Action 0 -> State 0\n",
            "Action 1 -> State 5\n",
            "Action 2 -> State 2\n",
            "Action 3 -> State 1\n",
            "Episode 1, state: 5\n",
            "children [17, 31, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def _backpropagate(self, path: list[Node], value: float):\n",
        "    for node in reversed(path):\n",
        "        node.visit_count += 1\n",
        "        node.cost += value  # 直接累加成本"
      ],
      "metadata": {
        "id": "fcacoFg3WoVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "class ValueNetwork(torch.nn.Module):\n",
        "    def __init__(self, state_size=16):\n",
        "        super().__init__() # 嵌入层处理离散状态\n",
        "        self.state_size=state_size\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(state_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1)\n",
        "        )\n",
        "        torch.nn.init.uniform_(self.layers[-1].weight, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''a = torch.zeros(self.state_size)\n",
        "        a[x]=1'''\n",
        "        x = x.long()\n",
        "        x = F.one_hot(x, num_classes=self.state_size).float()   # 输入为状态索引\n",
        "        return self.layers(x).squeeze(-1)\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: int  # FrozenLake 状态是离散的 (0-15)\n",
        "    action_taken: Optional[int] = None\n",
        "    parent: Optional['Node'] = None\n",
        "    children: dict = field(default_factory=dict)\n",
        "    value: float = 0.0\n",
        "    done: bool = False\n",
        "    visit_count: int = 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self._id = id(self)\n",
        "\n",
        "class UCTFrozenLake:\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_simulations: int = 100,\n",
        "                 buffer_size: int = 10000,\n",
        "                 batch_size: int = 64,\n",
        "                 gamma: float = 0.99,\n",
        "                 exploration_weight: float = 1.414):\n",
        "      self.num_simulations = num_simulations\n",
        "      self.gamma = gamma\n",
        "      self.exploration_weight = exploration_weight\n",
        "      self.env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "      # 神经网络\n",
        "      self.model = ValueNetwork(state_size=16)\n",
        "      self.target_model = ValueNetwork(state_size=16)\n",
        "      self.target_model.load_state_dict(self.model.state_dict())\n",
        "      self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "      self.loss_fn = torch.nn.MSELoss()\n",
        "      self.replay_buffer = deque(maxlen=buffer_size)\n",
        "      self.batch_size = batch_size\n",
        "      self.train_step_counter = 0\n",
        "      self.target_update_interval = 10\n",
        "\n",
        "      # 搜索树\n",
        "      self.root: Optional[Node] = None\n",
        "    def search(self, current_state: int):\n",
        "      self.root = Node(state=current_state)\n",
        "      temp_env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "      for _ in range(self.num_simulations):\n",
        "        self._simulate(temp_env, self.root)\n",
        "       # 选择访问次数最多的动作（最小化成本）\n",
        "      best_action = max(self.root.children.keys(), key=lambda a: self.root.children[a].visit_count)\n",
        "      #print('self.root.children.keys()',self.root.children.keys())\n",
        "      return best_action, self.root\n",
        "    def _simulate(self, temp_env: gym.Env, root: Node):\n",
        "      path = []\n",
        "      node = root\n",
        "      done = root.done\n",
        "      # 阶段1: Selection\n",
        "      while not done and node.children:\n",
        "          action = self._select_child(node)\n",
        "          node = node.children[action]\n",
        "\n",
        "          path.append(node)\n",
        "          done = node.done\n",
        "\n",
        "\n",
        "     # 阶段2: Expansion\n",
        "      while not node.done and len(node.children) < self.env.action_space.n:\n",
        "          action = np.random.choice([a for a in range(4) if a not in node.children])\n",
        "          #print('action',[a for a in range(4) if a not in node.children])\n",
        "          temp_env.reset()\n",
        "          temp_env.unwrapped.s = node.state  # 直接设置状态\n",
        "          next_state, reward, terminated, truncated, _ = temp_env.step(action)\n",
        "          done = terminated or truncated\n",
        "\n",
        "          '''# FrozenLake奖励调整：到达目标+1，其他情况0\n",
        "          cost = 1.0  # 每步成本为1（最小化步数）\n",
        "          if terminated and next_state == 15:  # 到达目标\n",
        "              cost = 0.0'''\n",
        "\n",
        "          # 使用model预测初始成本\n",
        "\n",
        "          state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
        "          initial_value = self.model(state_tensor).item()\n",
        "\n",
        "          child = Node(\n",
        "              state=next_state,\n",
        "              action_taken=action,\n",
        "              parent=node,\n",
        "              value=initial_value,\n",
        "              done=done\n",
        "          )\n",
        "          node.children[action] = child\n",
        "\n",
        "      # 阶段3: Evaluation & Backup\n",
        "      if node.done and node.state == 15:\n",
        "        final_value = 1.0\n",
        "      elif node.done and node.state != 15:\n",
        "        final_cost = -1\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          state_tensor = torch.tensor([node.state], dtype=torch.float32)\n",
        "          final_cost = self.gamma * self.target_model(state_tensor).item()\n",
        "\n",
        "      # 反向传播累积成本\n",
        "      self._backpropagate(path, final_cost)\n",
        "\n",
        "      # 存储经验\n",
        "      if node.parent is not None:\n",
        "\n",
        "        self._remember(node.parent.state,  final_cost)\n",
        "\n",
        "      # 经验回放\n",
        "      if len(self.replay_buffer) >= self.batch_size:\n",
        "          self._replay()\n",
        "    def _select_child(self, node: Node) -> int:\n",
        "        \"\"\"基于UCT公式选择子节点（最小化成本）\"\"\"\n",
        "        total_visits = node.visit_count\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "\n",
        "        for action in node.children:\n",
        "            child = node.children[action]\n",
        "\n",
        "            exploit = child.value\n",
        "\n",
        "            explore = self.exploration_weight * math.sqrt(math.log(total_visits + 1e-7) / (child.visit_count + 1e-7))\n",
        "            score = exploit + explore\n",
        "\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "\n",
        "        return best_action\n",
        "    def _backpropagate(self, path: list[Node], value: float):\n",
        "      \"\"\"反向传播更新路径上的节点，使用加权平均更新节点值\"\"\"\n",
        "      for node in reversed(path):\n",
        "          node.visit_count += 1\n",
        "\n",
        "          if node.children:\n",
        "            total_visits = sum(child.visit_count for child in node.children.values())\n",
        "\n",
        "            weighted_value = sum(child.value * child.visit_count for child in node.children.values()) / total_visits\n",
        "            node.value = weighted_value\n",
        "    def _remember(self, state: int, target_cost: float):\n",
        "        \"\"\"存储经验\"\"\"\n",
        "        self.replay_buffer.append((state, target_cost))\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"训练网络\"\"\"\n",
        "        batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
        "        states, target_costs = zip(*[self.replay_buffer[i] for i in batch])\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).unsqueeze(-1)\n",
        "        target_costs = torch.tensor(target_costs, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        pred_costs = self.model(states)\n",
        "        loss = self.loss_fn(pred_costs, target_costs)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_interval == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "def train_frozenlake():\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    agent = UCTFrozenLake(num_simulations=50)\n",
        "\n",
        "    for episode in range(10):\n",
        "        state = env.reset()[0]\n",
        "        total_cost = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, node = agent.search(state)\n",
        "            print(f\"Episode {episode+1}, state: {state}\")\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_cost += 1  # 每步成本+1\n",
        "            state = next_state\n",
        "            print(f\"Episode {episode+1}, state: {next_state}\")\n",
        "            #print('children',[children.visit_count for children in node.children.values()])\n",
        "\n",
        "            if terminated and next_state == 15:\n",
        "                total_cost = 0  # 到达目标时总成本为0\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_frozenlake()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mT62lDs7yPMp",
        "outputId": "6efcfd30-642e-445e-c2ce-64bd634eb10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, state: 0\n",
            "Episode 1, state: 1\n",
            "Episode 1, state: 1\n",
            "Episode 1, state: 5\n",
            "Episode 2, state: 0\n",
            "Episode 2, state: 4\n",
            "Episode 2, state: 4\n",
            "Episode 2, state: 5\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 0\n",
            "Episode 3, state: 1\n",
            "Episode 3, state: 1\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 2\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 3\n",
            "Episode 3, state: 7\n",
            "Episode 4, state: 0\n",
            "Episode 4, state: 1\n",
            "Episode 4, state: 1\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 2\n",
            "Episode 4, state: 1\n",
            "Episode 4, state: 1\n",
            "Episode 4, state: 5\n",
            "Episode 5, state: 0\n",
            "Episode 5, state: 0\n",
            "Episode 5, state: 0\n",
            "Episode 5, state: 0\n",
            "Episode 5, state: 0\n",
            "Episode 5, state: 1\n",
            "Episode 5, state: 1\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 2\n",
            "Episode 5, state: 3\n",
            "Episode 5, state: 3\n",
            "Episode 5, state: 7\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n",
            "Episode 6, state: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0e16ec4081f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mtrain_frozenlake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-0e16ec4081f9>\u001b[0m in \u001b[0;36mtrain_frozenlake\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Episode {episode+1}, state: {state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0e16ec4081f9>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mtemp_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FrozenLake-v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m        \u001b[0;31m# 选择访问次数最多的动作（最小化成本）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0e16ec4081f9>\u001b[0m in \u001b[0;36m_simulate\u001b[0;34m(self, temp_env, root)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;31m# 经验回放\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_select_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;34m\"\"\"基于UCT公式选择子节点（最小化成本）\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0e16ec4081f9>\u001b[0m in \u001b[0;36m_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_costs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mexp_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ndz9dvIrQlWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}