{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS5meqgrmhO4rxtMs8aQ/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/HybridZero/blob/main/A0_Greddy_BFS_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eel82h7hdgji",
        "outputId": "e9546d72-213d-420e-c418-9e7c20e6ede3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import pi\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from IPython import display\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim=16, action_dim=4, hidden_dim=64):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.value_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        # one-hot\n",
        "        if isinstance(state, int):\n",
        "            one_hot = torch.zeros(16)\n",
        "            one_hot[state] = 1.0\n",
        "            state = one_hot\n",
        "\n",
        "        x = self.shared_layers(state)\n",
        "        value = self.value_head(x)\n",
        "\n",
        "        return value\n",
        "\n",
        "class AlphaZeroGreedyBFS:\n",
        "\n",
        "  def __init__(self, learning_rate=0.001, gamma=0.99, tau=0.005):\n",
        "      self.rows = 4\n",
        "      self.cols = 4\n",
        "      self.actions = 4\n",
        "      self.network = ValueNetwork()\n",
        "      self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
        "\n",
        "      self.value_net = ValueNetwork()\n",
        "      self.target_net = ValueNetwork()\n",
        "      # 初始化目标网络\n",
        "      self.target_net.load_state_dict(self.value_net.state_dict())\n",
        "      self.optimizer = torch.optim.Adam(self.value_net.parameters(), lr=learning_rate)\n",
        "      self.gamma = gamma\n",
        "      self.tau = tau\n",
        "\n",
        "\n",
        "      self.replay_buffer = []\n",
        "      self.buffer_size = 1000\n",
        "      self.batch_size = 32\n",
        "\n",
        "  def state_to_tensor(self, state):\n",
        "\n",
        "    state_tensor = torch.zeros(16)\n",
        "    state_tensor[state] = 1.0\n",
        "    return state_tensor\n",
        "\n",
        "  def get_next_state(self, state, action):\n",
        "      row = state // self.cols\n",
        "      col = state % self.cols\n",
        "\n",
        "      if action == 0:  # LEFT\n",
        "          col = max(0, col - 1)\n",
        "      elif action == 1:  # DOWN\n",
        "          row = min(self.rows - 1, row + 1)\n",
        "      elif action == 2:  # RIGHT\n",
        "          col = min(self.cols - 1, col + 1)\n",
        "      elif action == 3:  # UP\n",
        "          row = max(0, row - 1)\n",
        "\n",
        "      return row * self.cols + col\n",
        "\n",
        "  def manhattan_distance(self, state, goal_state):\n",
        "\n",
        "      current_row = state // self.cols\n",
        "      current_col = state % self.cols\n",
        "      goal_row = goal_state // self.cols\n",
        "      goal_col = goal_state % self.cols\n",
        "      return abs(current_row - goal_row) + abs(current_col - goal_col)\n",
        "  def calculate_alpha(self,v_network, v_simulation, beta=1.0):\n",
        "    #print(v_network, v_simulation)\n",
        "    difference = abs(v_network - v_simulation)\n",
        "    alpha = 1 / (1 + np.exp(-beta * difference))\n",
        "    return alpha\n",
        "\n",
        "  def alphazero_heuristic(self, state, action):\n",
        "      next_state = self.get_next_state(state, action)\n",
        "      #base_h = self.manhattan_distance(next_state, 15)\n",
        "\n",
        "\n",
        "      state_tensor = torch.zeros(16)\n",
        "      state_tensor[next_state] = 1.0\n",
        "      value = self.value_net(state_tensor).item()\n",
        "      if next_state == 15:\n",
        "        return value + 5\n",
        "\n",
        "      #simulation!!!!!\n",
        "      env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "      env.reset()\n",
        "      env.unwrapped.s = next_state\n",
        "      simulated_values = []\n",
        "\n",
        "      for _ in range(4):\n",
        "        done = False\n",
        "        curr_state = next_state\n",
        "        gamma = 0.95\n",
        "        step = 0\n",
        "        cumulative_reward = 0\n",
        "\n",
        "        while not done and step<10:\n",
        "          # 加入exploration\n",
        "\n",
        "          with torch.no_grad():\n",
        "            action_values = [self.value_net(self.state_to_tensor(\n",
        "                self.get_next_state(curr_state, a))).detach().item()\n",
        "                for a in range(4)]\n",
        "          action = np.argmax(action_values)\n",
        "\n",
        "          next_state, reward, done, truncated, _ = env.step(action)\n",
        "          curr_state = next_state\n",
        "\n",
        "          if done and reward == 0:\n",
        "              reward = -1\n",
        "          elif reward == 1:\n",
        "              reward = 10\n",
        "          else:\n",
        "              reward = -0.1\n",
        "\n",
        "          cumulative_reward += gamma ** step * reward\n",
        "          step += 1\n",
        "          done = done or truncated\n",
        "\n",
        "        simulated_values.append(cumulative_reward)\n",
        "\n",
        "      simulation_value = np.mean(simulated_values)\n",
        "      #print(value, simulation_value)\n",
        "\n",
        "      return 0.65 * value + 0.75 * simulation_value\n",
        "  def collect_episode(self, episode, episodes):\n",
        "    trajectory = {\n",
        "        'states': [],\n",
        "        'actions': [],\n",
        "        'values': [],  # Will store discounted final rewards\n",
        "        'steps_to_goal': []\n",
        "    }\n",
        "\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    max_steps = 20  # Add step limit\n",
        "    steps = 0\n",
        "    #actions_test=[2,2,1,1,1,2]\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        trajectory['states'].append(state)\n",
        "        trajectory['steps_to_goal'].append(len(trajectory['states']))\n",
        "\n",
        "        epsilon = max(0.2, 1 - episode/episodes)  # Slightly higher exploration\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action_values = [self.alphazero_heuristic(state, a)\n",
        "                           for a in range(4)]\n",
        "            action = np.argmax(action_values)\n",
        "        #action = actions_test[steps]\n",
        "        trajectory['actions'].append(action)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        done = done or truncated\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    # Determine final reward based on outcome\n",
        "    if done and reward == 1:\n",
        "        final_reward = 10  # Success\n",
        "        print(\"Success\")\n",
        "    elif steps >= max_steps:\n",
        "        final_reward = -3  # Timeout penalty\n",
        "    else:\n",
        "        final_reward = -5  # Failure\n",
        "\n",
        "    # Calculate values by propagating final reward backwards\n",
        "    gamma = 0.95\n",
        "    values = []\n",
        "    current_value = final_reward\n",
        "    for _ in range(len(trajectory['states'])):\n",
        "        values.insert(0, current_value)\n",
        "        current_value *= gamma\n",
        "\n",
        "    trajectory['values'] = values\n",
        "\n",
        "    return trajectory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def update_network(self, trajectory):\n",
        "\n",
        "\n",
        "    # 将所有状态转成batch\n",
        "    states = torch.stack([self.state_to_tensor(s) for s in trajectory['states']])\n",
        "    targets = torch.tensor(trajectory['values']).float().unsqueeze(1)\n",
        "\n",
        "\n",
        "\n",
        "    # 一次性计算所有预测\n",
        "    values = self.value_net(states)\n",
        "\n",
        "    # 计算损失\n",
        "    value_loss = nn.MSELoss()(values, targets)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    self.optimizer.step()\n",
        "    '''if trajectory['values'][0] > 0:  # 如果是成功的轨迹\n",
        "        # 多训练几次\n",
        "        for _ in range(3):\n",
        "            values = self.network(states)\n",
        "            value_loss = nn.MSELoss()(values, targets)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            value_loss.backward()\n",
        "            self.optimizer.step()\n",
        "    else:\n",
        "        values = self.network(states)\n",
        "        value_loss = nn.MSELoss()(values, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.optimizer.step()'''\n",
        "    return value_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, episodes=1000):\n",
        "    #self.TD_Training(episodes)\n",
        "    for episode in range(episodes):\n",
        "        trajectory = self.collect_episode(episode, episodes)\n",
        "        loss = self.update_network(trajectory)\n",
        "        print(f\"Episode {episode + 1}/{episodes}, Loss: {loss}\")\n",
        "        display.clear_output(wait=True)\n"
      ],
      "metadata": {
        "id": "E-x8nYdIdo_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = AlphaZeroGreedyBFS()\n",
        "a.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akuuVYVhcqmY",
        "outputId": "53e5bbe3-a52a-48cf-ba60-bde2eb2d308f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n",
            "Episode 1000/1000, Loss: 6.190967559814453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aaa(solver):\n",
        "  for i in range(16):\n",
        "    b=  [solver.value_net(solver.state_to_tensor(\n",
        "                  solver.get_next_state(i, a))).detach().item()\n",
        "                  for a in range(4)]\n",
        "    print(f'{i}',b)\n",
        "aaa(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADtlsd2Si-JS",
        "outputId": "29ffbd15-7f6d-4ff2-c9d9-1249d8165c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [2.9760804176330566, 1.3963580131530762, 3.871591329574585, 2.9760804176330566]\n",
            "1 [2.9760804176330566, 4.170629501342773, 5.4293718338012695, 3.871591329574585]\n",
            "2 [3.871591329574585, 5.931889057159424, 3.3932957649230957, 5.4293718338012695]\n",
            "3 [5.4293718338012695, 3.9126687049865723, 3.3932957649230957, 3.3932957649230957]\n",
            "4 [1.3963580131530762, 3.911417007446289, 4.170629501342773, 2.9760804176330566]\n",
            "5 [1.3963580131530762, 5.825281143188477, 5.931889057159424, 3.871591329574585]\n",
            "6 [4.170629501342773, 8.195137023925781, 3.9126687049865723, 5.4293718338012695]\n",
            "7 [5.931889057159424, 3.7212297916412354, 3.9126687049865723, 3.3932957649230957]\n",
            "8 [3.911417007446289, 4.479738235473633, 5.825281143188477, 1.3963580131530762]\n",
            "9 [3.911417007446289, 6.550695419311523, 8.195137023925781, 4.170629501342773]\n",
            "10 [5.825281143188477, 9.750393867492676, 3.7212297916412354, 5.931889057159424]\n",
            "11 [8.195137023925781, 4.119339942932129, 3.7212297916412354, 3.9126687049865723]\n",
            "12 [4.479738235473633, 4.479738235473633, 6.550695419311523, 3.911417007446289]\n",
            "13 [4.479738235473633, 6.550695419311523, 9.750393867492676, 5.825281143188477]\n",
            "14 [6.550695419311523, 9.750393867492676, 4.119339942932129, 8.195137023925781]\n",
            "15 [9.750393867492676, 4.119339942932129, 4.119339942932129, 3.7212297916412354]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(solver):\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "\n",
        "\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action_values = [solver.alphazero_heuristic(state, a)\n",
        "                        for a in range(4)]\n",
        "      b=  [solver.value_net(solver.state_to_tensor(\n",
        "                solver.get_next_state(state, a))).detach().item()\n",
        "                for a in range(4)]\n",
        "      print('action_values',b)\n",
        "      action = np.argmax(action_values)\n",
        "      next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "      print('state, action, reward, next_state',(state, action, reward, next_state))\n",
        "      if done and reward == 0:\n",
        "        print('fail')\n",
        "      if reward == 1:\n",
        "        print('success')\n",
        "\n",
        "\n",
        "      state = next_state\n",
        "      done = done or truncated\n",
        "\n",
        "\n",
        "test(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OziRpWpcwba",
        "outputId": "fed53a66-7de5-46ea-de81-b494bcdef9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_values [2.9760804176330566, 1.3963580131530762, 3.871591329574585, 2.9760804176330566]\n",
            "state, action, reward, next_state (0, 2, 0.0, 1)\n",
            "action_values [2.9760804176330566, 4.170629501342773, 5.4293718338012695, 3.871591329574585]\n",
            "state, action, reward, next_state (1, 2, 0.0, 2)\n",
            "action_values [3.871591329574585, 5.931889057159424, 3.3932957649230957, 5.4293718338012695]\n",
            "state, action, reward, next_state (2, 1, 0.0, 6)\n",
            "action_values [4.170629501342773, 8.195137023925781, 3.9126687049865723, 5.4293718338012695]\n",
            "state, action, reward, next_state (6, 1, 0.0, 10)\n",
            "action_values [5.825281143188477, 9.750393867492676, 3.7212297916412354, 5.931889057159424]\n",
            "state, action, reward, next_state (10, 1, 0.0, 14)\n",
            "action_values [6.550695419311523, 9.750393867492676, 4.119339942932129, 8.195137023925781]\n",
            "state, action, reward, next_state (14, 2, 1.0, 15)\n",
            "success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### thoughts about training"
      ],
      "metadata": {
        "id": "97jRcSbf5FKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_episode(self, episode, episodes):\n",
        "    trajectory = {\n",
        "        'states': [],\n",
        "        'actions': [],\n",
        "        'final_reward': None,  # 只记录最终reward\n",
        "        'steps_to_goal': []    # 记录每个状态距离目标的步数\n",
        "    }\n",
        "\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    with torch.no_grad():\n",
        "      while not done:\n",
        "          trajectory['states'].append(state)\n",
        "          # 记录当前状态到目标的剩余步数\n",
        "          trajectory['steps_to_goal'].append(len(trajectory['states']))\n",
        "\n",
        "          epsilon = max(0.1, 1 - episode/episodes)\n",
        "          if len(trajectory['states']) > 10:\n",
        "              trajectory['final_reward'] = -1\n",
        "              break\n",
        "\n",
        "          if random.random() < epsilon:\n",
        "              action = env.action_space.sample()\n",
        "          else:\n",
        "              action_values = [self.alphazero_heuristic(state, a)\n",
        "                            for a in range(4)]\n",
        "              action = np.argmax(action_values)\n",
        "\n",
        "          trajectory['actions'].append(action)\n",
        "\n",
        "          next_state, reward, done, truncated, _ = env.step(action)\n",
        "          state = next_state\n",
        "          done = done or truncated\n",
        "\n",
        "          if done and reward == 0:\n",
        "              reward = -1\n",
        "          if reward == 1:\n",
        "              reward = 10\n",
        "\n",
        "    trajectory['final_reward'] = reward\n",
        "    return trajectory\n",
        "\n",
        "  def update_network(self, trajectory):\n",
        "      gamma = 0.95  # 折扣因子\n",
        "      final_reward = trajectory['final_reward']\n",
        "\n",
        "      # 将所有状态转成batch\n",
        "      states = torch.stack([self.state_to_tensor(s) for s in trajectory['states']])\n",
        "\n",
        "      # 根据步数计算每个状态的target value\n",
        "      steps_to_goal = trajectory['steps_to_goal']\n",
        "      discounted_rewards = [final_reward * (gamma ** (len(steps_to_goal) - step))\n",
        "                          for step in steps_to_goal]\n",
        "      targets = torch.tensor(discounted_rewards).float().unsqueeze(1)\n",
        "\n",
        "      # 一次性计算所有预测\n",
        "      values = self.network(states)\n",
        "\n",
        "      # 计算损失\n",
        "      value_loss = nn.MSELoss()(values, targets)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      value_loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      return value_loss.item()"
      ],
      "metadata": {
        "id": "gSW9m4b95LLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KPPqKmOAgHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}